{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Opossum Search, an intelligent search system combining natural language processing with opossum-themed interactions. Overview \u00b6 Opossum Search provides a unique blend of advanced AI capabilities and playful interactions, all while maintaining high availability and reliability. Core Features \u00b6 Conversation System \u00b6 Natural language understanding Topic-aware responses Sentiment tracking Multi-model backend support Service Reliability \u00b6 Continuous monitoring Automatic failover Rate limiting protection Hybrid model architecture Special Features \u00b6 Image processing capabilities Background ambiance generation Opossum National Day celebration (October 18th) SVG visualization system Technical Stack \u00b6 AI Components \u00b6 Multiple LLM backends (Gemini, Ollama, Transformers) Sentence transformers for topic detection Markov chains for ambient text Sentiment analysis system Infrastructure \u00b6 Distributed caching Health monitoring Error tracking Performance metrics Processing \u00b6 Image handling SVG generation Real-time analytics Background processing Documentation Sections \u00b6 Service Availability \u00b6 Learn about our robust service architecture, including monitoring, failover mechanisms, and recovery procedures. Conversation Management \u00b6 Explore how we handle natural conversations, from topic detection to response generation. Image Processing \u00b6 Discover our image processing capabilities and SVG generation system. Model Integration \u00b6 Understand how we combine multiple AI models for optimal performance. Infrastructure \u00b6 Deep dive into our caching, monitoring, and configuration systems. API Documentation \u00b6 Explore detailed API documentation: Routes : Overview of available API routes. Request and Response : Details on request formats and response structures. Error Codes : Comprehensive list of API error codes. Rate Limits : Information on API rate limiting policies. Webhooks : Guide to setting up and using webhooks. Health Endpoints : Details on health check endpoints. Development Guide \u00b6 Everything you need to start contributing to Opossum Search. Getting Started \u00b6 Review the development guide Set up your local environment Explore the API documentation Run the test suite For questions or support, check our development guide or open an issue in our repository.","title":"Home"},{"location":"#overview","text":"Opossum Search provides a unique blend of advanced AI capabilities and playful interactions, all while maintaining high availability and reliability.","title":"Overview"},{"location":"#core-features","text":"","title":"Core Features"},{"location":"#conversation-system","text":"Natural language understanding Topic-aware responses Sentiment tracking Multi-model backend support","title":"Conversation System"},{"location":"#service-reliability","text":"Continuous monitoring Automatic failover Rate limiting protection Hybrid model architecture","title":"Service Reliability"},{"location":"#special-features","text":"Image processing capabilities Background ambiance generation Opossum National Day celebration (October 18th) SVG visualization system","title":"Special Features"},{"location":"#technical-stack","text":"","title":"Technical Stack"},{"location":"#ai-components","text":"Multiple LLM backends (Gemini, Ollama, Transformers) Sentence transformers for topic detection Markov chains for ambient text Sentiment analysis system","title":"AI Components"},{"location":"#infrastructure","text":"Distributed caching Health monitoring Error tracking Performance metrics","title":"Infrastructure"},{"location":"#processing","text":"Image handling SVG generation Real-time analytics Background processing","title":"Processing"},{"location":"#documentation-sections","text":"","title":"Documentation Sections"},{"location":"#service-availability","text":"Learn about our robust service architecture, including monitoring, failover mechanisms, and recovery procedures.","title":"Service Availability"},{"location":"#conversation-management","text":"Explore how we handle natural conversations, from topic detection to response generation.","title":"Conversation Management"},{"location":"#image-processing","text":"Discover our image processing capabilities and SVG generation system.","title":"Image Processing"},{"location":"#model-integration","text":"Understand how we combine multiple AI models for optimal performance.","title":"Model Integration"},{"location":"#infrastructure_1","text":"Deep dive into our caching, monitoring, and configuration systems.","title":"Infrastructure"},{"location":"#api-documentation","text":"Explore detailed API documentation: Routes : Overview of available API routes. Request and Response : Details on request formats and response structures. Error Codes : Comprehensive list of API error codes. Rate Limits : Information on API rate limiting policies. Webhooks : Guide to setting up and using webhooks. Health Endpoints : Details on health check endpoints.","title":"API Documentation"},{"location":"#development-guide","text":"Everything you need to start contributing to Opossum Search.","title":"Development Guide"},{"location":"#getting-started","text":"Review the development guide Set up your local environment Explore the API documentation Run the test suite For questions or support, check our development guide or open an issue in our repository.","title":"Getting Started"},{"location":"about/history/","text":"Project History \u00b6 Origins \u00b6 Opossum Search began with a simple idea: create a preset chatbot that could help with specific, recurring tasks. What started as a straightforward utility quickly evolved into something much more ambitious as we discovered the potential for a more comprehensive system. Evolution \u00b6 The development journey can be broken down into several key phases: Phase 1: Concept Development (Early 2023) \u00b6 The initial concept was just a basic chatbot with preset responses. However, as we explored the capabilities of modern AI systems, we realized we could build something far more powerful by creating a hybrid system that leveraged multiple AI backends. Phase 2: Architecture Design (Mid 2023) \u00b6 This is when the core architectural principles took shape: The hybrid model selection system Redis-based caching infrastructure Resilience patterns for service availability Initial GraphQL API design Phase 3: Feature Expansion (Late 2023) \u00b6 As the core architecture stabilized, we added more sophisticated capabilities: Multimodal processing with image analysis Advanced conversation management SVG generation capabilities Comprehensive telemetry and observability Phase 4: Production Refinement (Early 2024) \u00b6 The focus shifted to hardening the system for production use: Enhanced security model implementation Comprehensive error handling and resilience patterns Performance optimization Documentation and development workflows Inspiration \u00b6 The name \"Opossum\" was chosen for its metaphorical significance - opossums are adaptable creatures known for their resilience and resourcefulness. Similarly, our system adapts to changing conditions and leverages multiple strategies to achieve its goals. Contributors \u00b6 Core Team \u00b6 bebo : Project lead, architecture design, and core implementation Special Thanks \u00b6 Majd : Contributed to the workflow design and initial concept discussions Leila : Provided feedback on user experience and process flows While their direct contributions were limited, both Majd and Leila played important roles in the conceptual development of the system, particularly in the early stages when we were defining the scope and direction of the project. From Chatbot to Comprehensive System \u00b6 What began as a desire for a simple preset chatbot transformed into Opossum Search - a sophisticated, multifaceted system featuring: Multi-model AI integration Advanced caching strategies Comprehensive error handling Multimodal processing capabilities Production-grade observability Enterprise-level security controls The journey from that initial concept to the current architecture has been one of continuous discovery and refinement, always guided by the principle that AI systems should be resilient, adaptable, and designed for real-world use cases. Looking Forward \u00b6 Opossum Search continues to evolve, with ongoing development focused on expanding its capabilities while maintaining the core principles of resilience and adaptability that have defined the project from the beginning.","title":"Project History"},{"location":"about/history/#project-history","text":"","title":"Project History"},{"location":"about/history/#origins","text":"Opossum Search began with a simple idea: create a preset chatbot that could help with specific, recurring tasks. What started as a straightforward utility quickly evolved into something much more ambitious as we discovered the potential for a more comprehensive system.","title":"Origins"},{"location":"about/history/#evolution","text":"The development journey can be broken down into several key phases:","title":"Evolution"},{"location":"about/history/#phase-1-concept-development-early-2023","text":"The initial concept was just a basic chatbot with preset responses. However, as we explored the capabilities of modern AI systems, we realized we could build something far more powerful by creating a hybrid system that leveraged multiple AI backends.","title":"Phase 1: Concept Development (Early 2023)"},{"location":"about/history/#phase-2-architecture-design-mid-2023","text":"This is when the core architectural principles took shape: The hybrid model selection system Redis-based caching infrastructure Resilience patterns for service availability Initial GraphQL API design","title":"Phase 2: Architecture Design (Mid 2023)"},{"location":"about/history/#phase-3-feature-expansion-late-2023","text":"As the core architecture stabilized, we added more sophisticated capabilities: Multimodal processing with image analysis Advanced conversation management SVG generation capabilities Comprehensive telemetry and observability","title":"Phase 3: Feature Expansion (Late 2023)"},{"location":"about/history/#phase-4-production-refinement-early-2024","text":"The focus shifted to hardening the system for production use: Enhanced security model implementation Comprehensive error handling and resilience patterns Performance optimization Documentation and development workflows","title":"Phase 4: Production Refinement (Early 2024)"},{"location":"about/history/#inspiration","text":"The name \"Opossum\" was chosen for its metaphorical significance - opossums are adaptable creatures known for their resilience and resourcefulness. Similarly, our system adapts to changing conditions and leverages multiple strategies to achieve its goals.","title":"Inspiration"},{"location":"about/history/#contributors","text":"","title":"Contributors"},{"location":"about/history/#core-team","text":"bebo : Project lead, architecture design, and core implementation","title":"Core Team"},{"location":"about/history/#special-thanks","text":"Majd : Contributed to the workflow design and initial concept discussions Leila : Provided feedback on user experience and process flows While their direct contributions were limited, both Majd and Leila played important roles in the conceptual development of the system, particularly in the early stages when we were defining the scope and direction of the project.","title":"Special Thanks"},{"location":"about/history/#from-chatbot-to-comprehensive-system","text":"What began as a desire for a simple preset chatbot transformed into Opossum Search - a sophisticated, multifaceted system featuring: Multi-model AI integration Advanced caching strategies Comprehensive error handling Multimodal processing capabilities Production-grade observability Enterprise-level security controls The journey from that initial concept to the current architecture has been one of continuous discovery and refinement, always guided by the principle that AI systems should be resilient, adaptable, and designed for real-world use cases.","title":"From Chatbot to Comprehensive System"},{"location":"about/history/#looking-forward","text":"Opossum Search continues to evolve, with ongoing development focused on expanding its capabilities while maintaining the core principles of resilience and adaptability that have defined the project from the beginning.","title":"Looking Forward"},{"location":"about/roadmap/","text":"Roadmap \u00b6 Current Status (Q1 2025) \u00b6 Opossum Search has established its core architecture with robust foundations for: Multi-model AI integration Resilient service design Advanced caching strategies Comprehensive observability Security controls Short-Term Goals (Q2-Q3 2025) \u00b6 Performance Optimization \u00b6 Optimize Redis caching patterns for higher throughput Implement streaming responses for long-running operations Profile and reduce memory consumption in model loading Feature Enhancements \u00b6 Expand multimodal capabilities to handle video input Add support for PDF document analysis and querying Implement conversation summarization for long sessions Developer Experience \u00b6 Create comprehensive SDK for common languages (Python, JavaScript, Go) Improve documentation with interactive examples Build development sandbox for API experimentation Medium-Term Goals (Q4 2025 - Q2 2026) \u00b6 System Scaling \u00b6 Implement horizontal scaling for core components Introduce sharded caching for high-volume deployments Develop multi-region deployment architecture Advanced Intelligence \u00b6 Add agent-based workflows for complex task execution Implement retrieval-augmented generation (RAG) capabilities Develop domain-specific fine-tuning processes Ecosystem Expansion \u00b6 Create plugin architecture for extensibility Build integration adapters for popular platforms Establish community contribution framework Long-Term Vision (Q3 2026+) \u00b6 Autonomous Operation \u00b6 Self-optimizing model selection based on performance metrics Automated fallback configuration and resilience tuning Dynamic resource allocation based on workload patterns Advanced Capabilities \u00b6 Multi-step reasoning for complex problem-solving Cross-modal translation (text-to-image, image-to-text, etc.) Contextual understanding across multi-session interactions Ecosystem Maturity \u00b6 Public model marketplace for community extensions Enterprise-grade compliance and governance features Comprehensive analytics and business intelligence tools Development Principles \u00b6 Throughout all phases of development, we maintain commitment to these core principles: Resilience First : All features must maintain or improve system resilience Progressive Enhancement : Core functionality works without advanced features Observability by Design : Every component includes comprehensive telemetry Secure by Default : Security considerations are built-in, not added later Performance Conscious : Performance impact is evaluated for all changes Feature Request Process \u00b6 Have a feature idea not on our roadmap? We welcome community input on future development priorities: Check existing roadmap items to avoid duplication Submit detailed feature proposal with use cases Participate in discussion to refine the concept Help prioritize through community feedback The Opossum Search roadmap is a living document that evolves based on user needs, technological advances, and development resources.","title":"Roadmap"},{"location":"about/roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"about/roadmap/#current-status-q1-2025","text":"Opossum Search has established its core architecture with robust foundations for: Multi-model AI integration Resilient service design Advanced caching strategies Comprehensive observability Security controls","title":"Current Status (Q1 2025)"},{"location":"about/roadmap/#short-term-goals-q2-q3-2025","text":"","title":"Short-Term Goals (Q2-Q3 2025)"},{"location":"about/roadmap/#performance-optimization","text":"Optimize Redis caching patterns for higher throughput Implement streaming responses for long-running operations Profile and reduce memory consumption in model loading","title":"Performance Optimization"},{"location":"about/roadmap/#feature-enhancements","text":"Expand multimodal capabilities to handle video input Add support for PDF document analysis and querying Implement conversation summarization for long sessions","title":"Feature Enhancements"},{"location":"about/roadmap/#developer-experience","text":"Create comprehensive SDK for common languages (Python, JavaScript, Go) Improve documentation with interactive examples Build development sandbox for API experimentation","title":"Developer Experience"},{"location":"about/roadmap/#medium-term-goals-q4-2025-q2-2026","text":"","title":"Medium-Term Goals (Q4 2025 - Q2 2026)"},{"location":"about/roadmap/#system-scaling","text":"Implement horizontal scaling for core components Introduce sharded caching for high-volume deployments Develop multi-region deployment architecture","title":"System Scaling"},{"location":"about/roadmap/#advanced-intelligence","text":"Add agent-based workflows for complex task execution Implement retrieval-augmented generation (RAG) capabilities Develop domain-specific fine-tuning processes","title":"Advanced Intelligence"},{"location":"about/roadmap/#ecosystem-expansion","text":"Create plugin architecture for extensibility Build integration adapters for popular platforms Establish community contribution framework","title":"Ecosystem Expansion"},{"location":"about/roadmap/#long-term-vision-q3-2026","text":"","title":"Long-Term Vision (Q3 2026+)"},{"location":"about/roadmap/#autonomous-operation","text":"Self-optimizing model selection based on performance metrics Automated fallback configuration and resilience tuning Dynamic resource allocation based on workload patterns","title":"Autonomous Operation"},{"location":"about/roadmap/#advanced-capabilities","text":"Multi-step reasoning for complex problem-solving Cross-modal translation (text-to-image, image-to-text, etc.) Contextual understanding across multi-session interactions","title":"Advanced Capabilities"},{"location":"about/roadmap/#ecosystem-maturity","text":"Public model marketplace for community extensions Enterprise-grade compliance and governance features Comprehensive analytics and business intelligence tools","title":"Ecosystem Maturity"},{"location":"about/roadmap/#development-principles","text":"Throughout all phases of development, we maintain commitment to these core principles: Resilience First : All features must maintain or improve system resilience Progressive Enhancement : Core functionality works without advanced features Observability by Design : Every component includes comprehensive telemetry Secure by Default : Security considerations are built-in, not added later Performance Conscious : Performance impact is evaluated for all changes","title":"Development Principles"},{"location":"about/roadmap/#feature-request-process","text":"Have a feature idea not on our roadmap? We welcome community input on future development priorities: Check existing roadmap items to avoid duplication Submit detailed feature proposal with use cases Participate in discussion to refine the concept Help prioritize through community feedback The Opossum Search roadmap is a living document that evolves based on user needs, technological advances, and development resources.","title":"Feature Request Process"},{"location":"about/team/","text":"Team \u00b6 Core Development \u00b6 bebo \u00b6 Project Lead & Principal Developer As the driving force behind Opossum Search, bebo has been responsible for: System architecture design Core implementation Documentation Testing and quality assurance bebo brings expertise in distributed systems, AI integration, and resilience engineering to the project, establishing the foundational principles that guide Opossum's development. Contributors \u00b6 Majd \u00b6 Workflow Consultant Contributed to early discussions about workflow design and helped shape the conceptual framework of how users would interact with the system. Leila \u00b6 User Experience Advisor Provided valuable feedback on user experience considerations and assisted with reviewing process flows during the initial conceptual phase. Acknowledgments \u00b6 Opossum Search also benefits from the broader open-source community and the following technologies: Redis - For our advanced caching infrastructure FastAPI - Powering our API endpoints GraphQL - Enabling flexible data querying OpenTelemetry - Supporting our observability framework Gemini API - Providing advanced AI capabilities Ollama - Enabling local model deployment Transformers - Supporting our fallback model capabilities DSPy - For our advanced data science capabilities Join the Team \u00b6 Opossum Search is an evolving project, and we welcome contributions from developers, designers, documentation writers, and testers. If you're interested in contributing, please see our Contributing Guide for more information.","title":"Team"},{"location":"about/team/#team","text":"","title":"Team"},{"location":"about/team/#core-development","text":"","title":"Core Development"},{"location":"about/team/#bebo","text":"Project Lead & Principal Developer As the driving force behind Opossum Search, bebo has been responsible for: System architecture design Core implementation Documentation Testing and quality assurance bebo brings expertise in distributed systems, AI integration, and resilience engineering to the project, establishing the foundational principles that guide Opossum's development.","title":"bebo"},{"location":"about/team/#contributors","text":"","title":"Contributors"},{"location":"about/team/#majd","text":"Workflow Consultant Contributed to early discussions about workflow design and helped shape the conceptual framework of how users would interact with the system.","title":"Majd"},{"location":"about/team/#leila","text":"User Experience Advisor Provided valuable feedback on user experience considerations and assisted with reviewing process flows during the initial conceptual phase.","title":"Leila"},{"location":"about/team/#acknowledgments","text":"Opossum Search also benefits from the broader open-source community and the following technologies: Redis - For our advanced caching infrastructure FastAPI - Powering our API endpoints GraphQL - Enabling flexible data querying OpenTelemetry - Supporting our observability framework Gemini API - Providing advanced AI capabilities Ollama - Enabling local model deployment Transformers - Supporting our fallback model capabilities DSPy - For our advanced data science capabilities","title":"Acknowledgments"},{"location":"about/team/#join-the-team","text":"Opossum Search is an evolving project, and we welcome contributions from developers, designers, documentation writers, and testers. If you're interested in contributing, please see our Contributing Guide for more information.","title":"Join the Team"},{"location":"api/compliance/","text":"Compliance and Usage Terms \u00b6 This document outlines how Opossum Search complies with relevant service usage terms, licensing requirements, and development guidelines. API Usage Compliance \u00b6 Google Gemini API \u00b6 Opossum Search maintains compliance with Google's AI API terms of service through: Rate limit adherence via the ServiceAvailability component Proper API key management through secure environment variables Clear attribution of Google services where appropriate Automated throttling to prevent quota exceedance Model Usage Terms \u00b6 Model Provider Compliance Mechanism Implementation Google Gemini Rate limiting, quota tracking record_gemini_usage() Ollama Local resource management Local service monitoring Transformers License compliance (Apache 2.0) Package attribution Privacy and Data Handling \u00b6 Opossum Search follows best practices for user data: No persistent storage of conversation content beyond session duration Clear documentation of data handling in the UI Session timeout with automatic cleanup after 30 minutes No collection of personal identifiable information Open Source Compliance \u00b6 The project maintains license compliance: MIT License for the Opossum Search codebase All dependencies properly attributed in requirements.txt Documentation includes appropriate citations and references Clear contribution guidelines that preserve licensing Monitoring and Telemetry \u00b6 Our use of OpenTelemetry and monitoring tools complies with: Transparent data collection limited to operational metrics No personally identifiable user data in telemetry Configurable monitoring that can be disabled if needed Clear logging practices with appropriate retention policies Third-Party Content Usage \u00b6 SVG visualizations and animations are original or properly licensed Emoji usage follows Unicode Consortium guidelines Opossum facts and scientific information are from public domain sources All generated content is appropriately labeled as AI-assisted Compliance Verification \u00b6 The project includes: Automated rate limit checking in the code CI/CD pipeline validation of licenses Regular review of terms of service for used APIs Testing specifically for compliance with rate limits Contact for Compliance Issues \u00b6 If you identify any compliance concerns, please contact the project maintainers immediately through: Opening an issue in the repository Emailing the project team directly Using the contact form on the project website This document was last updated on: March 23, 2025","title":"Compliance"},{"location":"api/compliance/#compliance-and-usage-terms","text":"This document outlines how Opossum Search complies with relevant service usage terms, licensing requirements, and development guidelines.","title":"Compliance and Usage Terms"},{"location":"api/compliance/#api-usage-compliance","text":"","title":"API Usage Compliance"},{"location":"api/compliance/#google-gemini-api","text":"Opossum Search maintains compliance with Google's AI API terms of service through: Rate limit adherence via the ServiceAvailability component Proper API key management through secure environment variables Clear attribution of Google services where appropriate Automated throttling to prevent quota exceedance","title":"Google Gemini API"},{"location":"api/compliance/#model-usage-terms","text":"Model Provider Compliance Mechanism Implementation Google Gemini Rate limiting, quota tracking record_gemini_usage() Ollama Local resource management Local service monitoring Transformers License compliance (Apache 2.0) Package attribution","title":"Model Usage Terms"},{"location":"api/compliance/#privacy-and-data-handling","text":"Opossum Search follows best practices for user data: No persistent storage of conversation content beyond session duration Clear documentation of data handling in the UI Session timeout with automatic cleanup after 30 minutes No collection of personal identifiable information","title":"Privacy and Data Handling"},{"location":"api/compliance/#open-source-compliance","text":"The project maintains license compliance: MIT License for the Opossum Search codebase All dependencies properly attributed in requirements.txt Documentation includes appropriate citations and references Clear contribution guidelines that preserve licensing","title":"Open Source Compliance"},{"location":"api/compliance/#monitoring-and-telemetry","text":"Our use of OpenTelemetry and monitoring tools complies with: Transparent data collection limited to operational metrics No personally identifiable user data in telemetry Configurable monitoring that can be disabled if needed Clear logging practices with appropriate retention policies","title":"Monitoring and Telemetry"},{"location":"api/compliance/#third-party-content-usage","text":"SVG visualizations and animations are original or properly licensed Emoji usage follows Unicode Consortium guidelines Opossum facts and scientific information are from public domain sources All generated content is appropriately labeled as AI-assisted","title":"Third-Party Content Usage"},{"location":"api/compliance/#compliance-verification","text":"The project includes: Automated rate limit checking in the code CI/CD pipeline validation of licenses Regular review of terms of service for used APIs Testing specifically for compliance with rate limits","title":"Compliance Verification"},{"location":"api/compliance/#contact-for-compliance-issues","text":"If you identify any compliance concerns, please contact the project maintainers immediately through: Opening an issue in the repository Emailing the project team directly Using the contact form on the project website This document was last updated on: March 23, 2025","title":"Contact for Compliance Issues"},{"location":"api/error-codes/","text":"API Error Codes and Responses \u00b6 Opossum Search uses standard HTTP status codes to indicate the success or failure of an API request. In case of an error, the response body will contain a JSON object with a standardized structure. Standard Error Response Format \u00b6 When an error occurs (HTTP status code 4xx or 5xx), the response body follows this structure: { \"success\" : false , \"error\" : { \"code\" : \"error_category_code\" , \"message\" : \"A human-readable description of the error.\" , \"details\" : [ { \"field\" : \"optional_field_name\" , \"message\" : \"Specific detail about the error, often related to validation.\" } // ... more details if applicable ] }, \"meta\" : { \"request_id\" : \"unique-request-identifier\" , \"timestamp\" : \"YYYY-MM-DDTHH:MM:SS.ffffffZ\" , // UTC timestamp \"processing_time\" : 0.123 // Optional: Time taken in seconds } } Authentication Errors \u00b6 Code HTTP Status Description Resolution invalid_api_key 401 The API key provided is invalid or doesn't exist Verify your API key and ensure it's properly included in the request header expired_api_key 401 The API key has expired Renew your API key through the dashboard insufficient_scope 403 The API key doesn't have permission for the requested operation Request additional permissions or use a different API key with the required scope invalid_token 401 The authentication token is invalid or malformed Verify the token or request a new one expired_token 401 The authentication token has expired Refresh your token using the refresh token endpoint missing_authentication 401 No authentication was provided Include an API key or Bearer token in your request Request Validation Errors \u00b6 Code HTTP Status Description Resolution invalid_request 400 The request body is malformed or contains invalid JSON Check the request syntax and ensure valid JSON missing_required_parameter 400 A required parameter is missing Add the missing parameter to your request invalid_parameter 400 A parameter has an invalid value Check the parameter value against the documentation unsupported_parameter 400 A parameter is not supported for this endpoint Remove the unsupported parameter invalid_query 400 The search query is invalid or empty Provide a valid search query invalid_filter 400 One or more filters are invalid Check the filter parameters against the documentation malformed_url 400 A URL parameter is malformed Ensure URLs are properly formatted and encoded invalid_file_format 400 The uploaded file format is not supported Use a supported file format file_too_large 400 The uploaded file exceeds the size limit Reduce the file size or use a different file Resource Errors \u00b6 Code HTTP Status Description Resolution resource_not_found 404 The requested resource doesn't exist Check the resource identifier resource_already_exists 409 The resource you're trying to create already exists Use a different identifier or update the existing resource resource_conflict 409 The request conflicts with the current state of the resource Resolve the conflict or try a different approach resource_locked 423 The resource is locked and can't be modified Wait until the resource is unlocked resource_gone 410 The resource was permanently removed The resource is no longer available Rate Limiting Errors \u00b6 Code HTTP Status Description Resolution rate_limit_exceeded 429 You've exceeded the rate limit for this endpoint Reduce request frequency or upgrade your plan quota_exceeded 429 You've exceeded your monthly quota Wait until your quota resets or upgrade your plan concurrent_requests_limit 429 Too many concurrent requests Reduce the number of parallel requests Processing Errors \u00b6 Code HTTP Status Description Resolution processing_error 422 An error occurred while processing the request Check the error details for specific information image_processing_error 422 Failed to process the provided image Ensure the image is valid and in a supported format svg_generation_error 422 Failed to generate the requested SVG Check the input data and template model_error 422 The AI model encountered an issue processing the request Try a different model or simplify your request parsing_error 422 Failed to parse the request or response Check the format of your data Service Errors \u00b6 Code HTTP Status Description Resolution internal_server_error 500 An unexpected error occurred on the server Contact support if the issue persists service_unavailable 503 The service is temporarily unavailable Try again later gateway_timeout 504 A dependent service timed out Try again later or simplify your request database_error 500 A database error occurred Contact support if the issue persists storage_error 500 A storage system error occurred Contact support if the issue persists cache_error 500 A caching system error occurred Try again later Model Service Errors \u00b6 Code HTTP Status Description Resolution model_unavailable 503 The requested AI model is currently unavailable Try a different model or try again later model_timeout 504 The AI model took too long to respond Simplify your query or try a different model model_overloaded 503 The AI model is currently overloaded Try again later or use a different model unsupported_model 400 The specified model is not supported Use one of the supported models model_response_error 422 The model returned an invalid or inappropriate response Try rephrasing your query content_filtered 422 The content was filtered due to safety concerns Modify your query to comply with content policies System-Specific Errors \u00b6 Code HTTP Status Description Resolution maintenance_mode 503 The system is in maintenance mode Check the status page for updates feature_disabled 403 This feature is currently disabled Check the documentation or contact support deprecated_endpoint 410 This endpoint has been deprecated Migrate to the recommended alternative version_mismatch 400 API version mismatch Update your client to use the correct API version region_not_supported 403 This operation is not available in your region Check the regional availability documentation Error Fields Reference \u00b6 The details Array \u00b6 The details array provides more specific information about what went wrong: \"details\" : [ { \"field\" : \"name of the problematic field\" , \"message\" : \"specific issue with this field\" , \"code\" : \"specific_error_code\" , \"location\" : \"body|query|path|header\" , \"value\" : \"the invalid value (if applicable)\" } ] Error Details Properties \u00b6 Property Description Example field The specific field that caused the error \"query\" , \"model\" , \"image\" message Human-readable explanation of the issue \"Must be at least 3 characters\" code Field-specific error code \"min_length\" , \"pattern\" , \"required\" location Where in the request the error occurred \"body\" , \"query\" , \"header\" value The invalid value (may be omitted for security) \"abc123\" Handling Errors \u00b6 Best Practices \u00b6 Always check the success field to determine if a request succeeded Log the request_id for troubleshooting and support Implement exponential backoff for rate limiting errors Watch for service unavailability and implement appropriate fallbacks Parse the details array for field-specific validation errors Retry Strategy \u00b6 For transient errors (5xx, rate limits), we recommend: Implement exponential backoff with jitter Set maximum retry attempts (3-5 is recommended) Only retry for error codes that indicate transient issues Respect the Retry-After header if present Example Retry Implementation \u00b6 async function makeRequestWithRetry ( endpoint , options , maxRetries = 3 ) { let retries = 0 ; while ( retries < maxRetries ) { try { const response = await fetch ( endpoint , options ); if ( response . ok ) { return await response . json (); } const errorData = await response . json (); // Don't retry for client errors (except rate limiting) if ( response . status < 500 && response . status !== 429 ) { throw errorData ; } // For rate limiting, respect the Retry-After header if ( response . status === 429 ) { const retryAfter = response . headers . get ( \"Retry-After\" ); const waitTime = retryAfter ? parseInt ( retryAfter , 10 ) * 1000 : ( 2 ** retries ) * 1000 ; await new Promise ( resolve => setTimeout ( resolve , waitTime )); } else { // Exponential backoff with jitter const waitTime = ( 2 ** retries ) * 1000 + Math . random () * 1000 ; await new Promise ( resolve => setTimeout ( resolve , waitTime )); } retries ++ ; } catch ( error ) { if ( retries === maxRetries - 1 ) { throw error ; } retries ++ ; } } } Error Reporting \u00b6 If you encounter an error that seems incorrect or requires assistance, please include: The complete error response (including the request_id ) The request that triggered the error Timestamp of the request Any relevant context about the conditions Submit error reports through the developer dashboard or contact support.","title":"Error Codes"},{"location":"api/error-codes/#api-error-codes-and-responses","text":"Opossum Search uses standard HTTP status codes to indicate the success or failure of an API request. In case of an error, the response body will contain a JSON object with a standardized structure.","title":"API Error Codes and Responses"},{"location":"api/error-codes/#standard-error-response-format","text":"When an error occurs (HTTP status code 4xx or 5xx), the response body follows this structure: { \"success\" : false , \"error\" : { \"code\" : \"error_category_code\" , \"message\" : \"A human-readable description of the error.\" , \"details\" : [ { \"field\" : \"optional_field_name\" , \"message\" : \"Specific detail about the error, often related to validation.\" } // ... more details if applicable ] }, \"meta\" : { \"request_id\" : \"unique-request-identifier\" , \"timestamp\" : \"YYYY-MM-DDTHH:MM:SS.ffffffZ\" , // UTC timestamp \"processing_time\" : 0.123 // Optional: Time taken in seconds } }","title":"Standard Error Response Format"},{"location":"api/error-codes/#authentication-errors","text":"Code HTTP Status Description Resolution invalid_api_key 401 The API key provided is invalid or doesn't exist Verify your API key and ensure it's properly included in the request header expired_api_key 401 The API key has expired Renew your API key through the dashboard insufficient_scope 403 The API key doesn't have permission for the requested operation Request additional permissions or use a different API key with the required scope invalid_token 401 The authentication token is invalid or malformed Verify the token or request a new one expired_token 401 The authentication token has expired Refresh your token using the refresh token endpoint missing_authentication 401 No authentication was provided Include an API key or Bearer token in your request","title":"Authentication Errors"},{"location":"api/error-codes/#request-validation-errors","text":"Code HTTP Status Description Resolution invalid_request 400 The request body is malformed or contains invalid JSON Check the request syntax and ensure valid JSON missing_required_parameter 400 A required parameter is missing Add the missing parameter to your request invalid_parameter 400 A parameter has an invalid value Check the parameter value against the documentation unsupported_parameter 400 A parameter is not supported for this endpoint Remove the unsupported parameter invalid_query 400 The search query is invalid or empty Provide a valid search query invalid_filter 400 One or more filters are invalid Check the filter parameters against the documentation malformed_url 400 A URL parameter is malformed Ensure URLs are properly formatted and encoded invalid_file_format 400 The uploaded file format is not supported Use a supported file format file_too_large 400 The uploaded file exceeds the size limit Reduce the file size or use a different file","title":"Request Validation Errors"},{"location":"api/error-codes/#resource-errors","text":"Code HTTP Status Description Resolution resource_not_found 404 The requested resource doesn't exist Check the resource identifier resource_already_exists 409 The resource you're trying to create already exists Use a different identifier or update the existing resource resource_conflict 409 The request conflicts with the current state of the resource Resolve the conflict or try a different approach resource_locked 423 The resource is locked and can't be modified Wait until the resource is unlocked resource_gone 410 The resource was permanently removed The resource is no longer available","title":"Resource Errors"},{"location":"api/error-codes/#rate-limiting-errors","text":"Code HTTP Status Description Resolution rate_limit_exceeded 429 You've exceeded the rate limit for this endpoint Reduce request frequency or upgrade your plan quota_exceeded 429 You've exceeded your monthly quota Wait until your quota resets or upgrade your plan concurrent_requests_limit 429 Too many concurrent requests Reduce the number of parallel requests","title":"Rate Limiting Errors"},{"location":"api/error-codes/#processing-errors","text":"Code HTTP Status Description Resolution processing_error 422 An error occurred while processing the request Check the error details for specific information image_processing_error 422 Failed to process the provided image Ensure the image is valid and in a supported format svg_generation_error 422 Failed to generate the requested SVG Check the input data and template model_error 422 The AI model encountered an issue processing the request Try a different model or simplify your request parsing_error 422 Failed to parse the request or response Check the format of your data","title":"Processing Errors"},{"location":"api/error-codes/#service-errors","text":"Code HTTP Status Description Resolution internal_server_error 500 An unexpected error occurred on the server Contact support if the issue persists service_unavailable 503 The service is temporarily unavailable Try again later gateway_timeout 504 A dependent service timed out Try again later or simplify your request database_error 500 A database error occurred Contact support if the issue persists storage_error 500 A storage system error occurred Contact support if the issue persists cache_error 500 A caching system error occurred Try again later","title":"Service Errors"},{"location":"api/error-codes/#model-service-errors","text":"Code HTTP Status Description Resolution model_unavailable 503 The requested AI model is currently unavailable Try a different model or try again later model_timeout 504 The AI model took too long to respond Simplify your query or try a different model model_overloaded 503 The AI model is currently overloaded Try again later or use a different model unsupported_model 400 The specified model is not supported Use one of the supported models model_response_error 422 The model returned an invalid or inappropriate response Try rephrasing your query content_filtered 422 The content was filtered due to safety concerns Modify your query to comply with content policies","title":"Model Service Errors"},{"location":"api/error-codes/#system-specific-errors","text":"Code HTTP Status Description Resolution maintenance_mode 503 The system is in maintenance mode Check the status page for updates feature_disabled 403 This feature is currently disabled Check the documentation or contact support deprecated_endpoint 410 This endpoint has been deprecated Migrate to the recommended alternative version_mismatch 400 API version mismatch Update your client to use the correct API version region_not_supported 403 This operation is not available in your region Check the regional availability documentation","title":"System-Specific Errors"},{"location":"api/error-codes/#error-fields-reference","text":"","title":"Error Fields Reference"},{"location":"api/error-codes/#the-details-array","text":"The details array provides more specific information about what went wrong: \"details\" : [ { \"field\" : \"name of the problematic field\" , \"message\" : \"specific issue with this field\" , \"code\" : \"specific_error_code\" , \"location\" : \"body|query|path|header\" , \"value\" : \"the invalid value (if applicable)\" } ]","title":"The details Array"},{"location":"api/error-codes/#error-details-properties","text":"Property Description Example field The specific field that caused the error \"query\" , \"model\" , \"image\" message Human-readable explanation of the issue \"Must be at least 3 characters\" code Field-specific error code \"min_length\" , \"pattern\" , \"required\" location Where in the request the error occurred \"body\" , \"query\" , \"header\" value The invalid value (may be omitted for security) \"abc123\"","title":"Error Details Properties"},{"location":"api/error-codes/#handling-errors","text":"","title":"Handling Errors"},{"location":"api/error-codes/#best-practices","text":"Always check the success field to determine if a request succeeded Log the request_id for troubleshooting and support Implement exponential backoff for rate limiting errors Watch for service unavailability and implement appropriate fallbacks Parse the details array for field-specific validation errors","title":"Best Practices"},{"location":"api/error-codes/#retry-strategy","text":"For transient errors (5xx, rate limits), we recommend: Implement exponential backoff with jitter Set maximum retry attempts (3-5 is recommended) Only retry for error codes that indicate transient issues Respect the Retry-After header if present","title":"Retry Strategy"},{"location":"api/error-codes/#example-retry-implementation","text":"async function makeRequestWithRetry ( endpoint , options , maxRetries = 3 ) { let retries = 0 ; while ( retries < maxRetries ) { try { const response = await fetch ( endpoint , options ); if ( response . ok ) { return await response . json (); } const errorData = await response . json (); // Don't retry for client errors (except rate limiting) if ( response . status < 500 && response . status !== 429 ) { throw errorData ; } // For rate limiting, respect the Retry-After header if ( response . status === 429 ) { const retryAfter = response . headers . get ( \"Retry-After\" ); const waitTime = retryAfter ? parseInt ( retryAfter , 10 ) * 1000 : ( 2 ** retries ) * 1000 ; await new Promise ( resolve => setTimeout ( resolve , waitTime )); } else { // Exponential backoff with jitter const waitTime = ( 2 ** retries ) * 1000 + Math . random () * 1000 ; await new Promise ( resolve => setTimeout ( resolve , waitTime )); } retries ++ ; } catch ( error ) { if ( retries === maxRetries - 1 ) { throw error ; } retries ++ ; } } }","title":"Example Retry Implementation"},{"location":"api/error-codes/#error-reporting","text":"If you encounter an error that seems incorrect or requires assistance, please include: The complete error response (including the request_id ) The request that triggered the error Timestamp of the request Any relevant context about the conditions Submit error reports through the developer dashboard or contact support.","title":"Error Reporting"},{"location":"api/getting-started/","text":"GraphQL API Documentation \u00b6 Related Documentation: - Technical: GraphQL API - Detailed technical implementation of the GraphQL API - API Reference: Routes - Complementary REST API routes - Service Availability: Rate Limiting - Understanding API rate limits Authentication \u00b6 The API supports three authentication methods: API Key Authentication \u00b6 For server-to-server integrations, use an API key: curl -H \"X-API-Key: your-api-key\" https://api.opossumsearch.com/v1/graphql Bearer Token Authentication \u00b6 For user-authenticated requests, use a Bearer token: curl -H \"Authorization: Bearer your-token\" https://api.opossumsearch.com/v1/graphql OAuth 2.0 \u00b6 For web applications, use OAuth 2.0 flow: Redirect users to /oauth/authorize Handle the callback at your redirect URI Exchange the code for an access token Use the access token in Bearer header See OAuth Configuration for detailed OAuth setup instructions. Available Tools \u00b6 Apollo Studio Integration \u00b6 Apollo Studio provides powerful tools for exploring and monitoring your GraphQL API. To use Apollo Studio: Visit http://localhost:5000/graphql in your browser Open Apollo Studio Connect to your local endpoint Explore the schema, run queries, and monitor performance GraphQL Voyager \u00b6 GraphQL Voyager provides an interactive visualization of your API schema: Visit http://localhost:5000/voyager in your browser Explore the relationships between types Click on nodes to see detailed type information Use the search function to find specific types or fields Self-Documenting Schema \u00b6 The GraphQL schema includes comprehensive descriptions for all types and fields: # Example query with descriptions query { service_status { # Get current status of all backend services with visualization name # The name of the service (e.g., 'gemini', 'ollama', 'transformers') available # Whether the service is currently available status # Human-readable status: 'online', 'degraded', or 'offline' response_time # Response time in milliseconds for the last health check } } Monitoring and Metrics \u00b6 Health Checks \u00b6 Endpoint: /.well-known/apollo/server-health Purpose: Check the health of the GraphQL service Schema Information \u00b6 Endpoint: /graphql/schema Purpose: Get the complete GraphQL schema in SDL format Metrics \u00b6 Endpoint: /graphql/metrics Purpose: Get Prometheus-compatible metrics for monitoring Best Practices \u00b6 Use the schema descriptions to understand field purposes Take advantage of GraphQL batching for multiple operations Monitor query performance in Apollo Studio Use the Voyager visualization to understand schema relationships Enable tracing during development for performance insights Example Queries \u00b6 # Get service status with visualization query GetServiceStatus { service_status { service_data svg_content last_updated } } # Process an image with effects mutation ProcessImage ( $imageData : String !, $effects : ImageEffects ) { process_image ( image_data : $imageData , effects : $effects ) { processed_image thumbnail info { width height format } } } # Send a chat message mutation SendChatMessage ( $input : ChatInput !) { chat ( input : $input ) { response next_stage has_svg svg_content } }","title":"API Reference"},{"location":"api/getting-started/#graphql-api-documentation","text":"Related Documentation: - Technical: GraphQL API - Detailed technical implementation of the GraphQL API - API Reference: Routes - Complementary REST API routes - Service Availability: Rate Limiting - Understanding API rate limits","title":"GraphQL API Documentation"},{"location":"api/getting-started/#authentication","text":"The API supports three authentication methods:","title":"Authentication"},{"location":"api/getting-started/#api-key-authentication","text":"For server-to-server integrations, use an API key: curl -H \"X-API-Key: your-api-key\" https://api.opossumsearch.com/v1/graphql","title":"API Key Authentication"},{"location":"api/getting-started/#bearer-token-authentication","text":"For user-authenticated requests, use a Bearer token: curl -H \"Authorization: Bearer your-token\" https://api.opossumsearch.com/v1/graphql","title":"Bearer Token Authentication"},{"location":"api/getting-started/#oauth-20","text":"For web applications, use OAuth 2.0 flow: Redirect users to /oauth/authorize Handle the callback at your redirect URI Exchange the code for an access token Use the access token in Bearer header See OAuth Configuration for detailed OAuth setup instructions.","title":"OAuth 2.0"},{"location":"api/getting-started/#available-tools","text":"","title":"Available Tools"},{"location":"api/getting-started/#apollo-studio-integration","text":"Apollo Studio provides powerful tools for exploring and monitoring your GraphQL API. To use Apollo Studio: Visit http://localhost:5000/graphql in your browser Open Apollo Studio Connect to your local endpoint Explore the schema, run queries, and monitor performance","title":"Apollo Studio Integration"},{"location":"api/getting-started/#graphql-voyager","text":"GraphQL Voyager provides an interactive visualization of your API schema: Visit http://localhost:5000/voyager in your browser Explore the relationships between types Click on nodes to see detailed type information Use the search function to find specific types or fields","title":"GraphQL Voyager"},{"location":"api/getting-started/#self-documenting-schema","text":"The GraphQL schema includes comprehensive descriptions for all types and fields: # Example query with descriptions query { service_status { # Get current status of all backend services with visualization name # The name of the service (e.g., 'gemini', 'ollama', 'transformers') available # Whether the service is currently available status # Human-readable status: 'online', 'degraded', or 'offline' response_time # Response time in milliseconds for the last health check } }","title":"Self-Documenting Schema"},{"location":"api/getting-started/#monitoring-and-metrics","text":"","title":"Monitoring and Metrics"},{"location":"api/getting-started/#health-checks","text":"Endpoint: /.well-known/apollo/server-health Purpose: Check the health of the GraphQL service","title":"Health Checks"},{"location":"api/getting-started/#schema-information","text":"Endpoint: /graphql/schema Purpose: Get the complete GraphQL schema in SDL format","title":"Schema Information"},{"location":"api/getting-started/#metrics","text":"Endpoint: /graphql/metrics Purpose: Get Prometheus-compatible metrics for monitoring","title":"Metrics"},{"location":"api/getting-started/#best-practices","text":"Use the schema descriptions to understand field purposes Take advantage of GraphQL batching for multiple operations Monitor query performance in Apollo Studio Use the Voyager visualization to understand schema relationships Enable tracing during development for performance insights","title":"Best Practices"},{"location":"api/getting-started/#example-queries","text":"# Get service status with visualization query GetServiceStatus { service_status { service_data svg_content last_updated } } # Process an image with effects mutation ProcessImage ( $imageData : String !, $effects : ImageEffects ) { process_image ( image_data : $imageData , effects : $effects ) { processed_image thumbnail info { width height format } } } # Send a chat message mutation SendChatMessage ( $input : ChatInput !) { chat ( input : $input ) { response next_stage has_svg svg_content } }","title":"Example Queries"},{"location":"api/health-endpoints/","text":"Health Endpoints \u00b6 Overview \u00b6 Opossum Search provides dedicated health check endpoints to monitor the status and availability of various system components. These endpoints are essential for monitoring, alerting, and ensuring service reliability. For a list of available API routes, see API Routes . For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . To learn about real-time notifications, refer to Webhooks . Base Health Endpoint \u00b6 GET /health Provides an overall status of the Opossum Search system. Authentication : Not required Response Format : { \"status\" : \"healthy\" , \"version\" : \"1.8.4\" , \"timestamp\" : \"2023-03-15T14:32:45Z\" , \"components\" : { \"api\" : \"healthy\" , \"models\" : \"healthy\" , \"cache\" : \"healthy\" , \"database\" : \"healthy\" }, \"uptime\" : 1209467 } Possible Status Values : healthy : All systems operating normally degraded : System is operating with reduced capabilities unhealthy : System is experiencing significant issues Component-Specific Health Endpoints \u00b6 API Gateway Health \u00b6 GET /health/api Checks the health of the API gateway. Response Format : { \"status\" : \"healthy\" , \"version\" : \"1.8.4\" , \"response_time\" : 12 , \"load\" : 0.35 , \"request_rate\" : 453.2 , \"error_rate\" : 0.02 } Model Service Health \u00b6 GET /health/models Checks the availability of AI models. Response Format : { \"status\" : \"healthy\" , \"models\" : [ { \"name\" : \"gemini\" , \"status\" : \"healthy\" , \"latency\" : 345 , \"success_rate\" : 0.998 , \"last_checked\" : \"2023-03-15T14:30:15Z\" }, { \"name\" : \"ollama\" , \"status\" : \"healthy\" , \"latency\" : 123 , \"success_rate\" : 0.999 , \"last_checked\" : \"2023-03-15T14:30:10Z\" }, { \"name\" : \"local-transformers\" , \"status\" : \"healthy\" , \"latency\" : 78 , \"success_rate\" : 1.0 , \"last_checked\" : \"2023-03-15T14:30:05Z\" } ] } Cache Health \u00b6 GET /health/cache Checks the health of the Redis cache. Response Format : { \"status\" : \"healthy\" , \"redis_version\" : \"6.2.6\" , \"latency\" : 2 , \"hit_rate\" : 0.87 , \"memory_usage\" : { \"used\" : \"1.2GB\" , \"total\" : \"4GB\" , \"percentage\" : 30 }, \"eviction_rate\" : 0.001 } Database Health \u00b6 GET /health/database Checks the health of the database. Response Format : { \"status\" : \"healthy\" , \"database_type\" : \"PostgreSQL\" , \"version\" : \"13.4\" , \"latency\" : 5 , \"connections\" : { \"active\" : 12 , \"idle\" : 8 , \"max\" : 100 }, \"last_successful_query\" : \"2023-03-15T14:32:40Z\" } Storage Health \u00b6 GET /health/storage Checks the health of the object storage system. Response Format : { \"status\" : \"healthy\" , \"storage_type\" : \"S3\" , \"latency\" : 24 , \"space\" : { \"used\" : \"234GB\" , \"total\" : \"1TB\" , \"percentage\" : 23.4 }, \"last_successful_operation\" : \"2023-03-15T14:31:45Z\" } Deep Health Checks \u00b6 For more detailed diagnostics, use the deep health check endpoint: GET /health/deep This performs thorough testing of each component, including: Database query execution Cache read/write operations Model inference tests Storage operations Inter-service communication Authentication : Required (Admin scope) Response Format : Similar to the base health endpoint but with extended details for each component. Integration with Monitoring Systems \u00b6 Prometheus Endpoint \u00b6 GET /metrics Exposes metrics in Prometheus format for scraping. Authentication : Optional (configurable) Sample Output : # HELP opossum_api_requests_total Total number of API requests # TYPE opossum_api_requests_total counter opossum_api_requests_total{method=\"GET\",endpoint=\"/search\"} 12453 # HELP opossum_api_request_duration_seconds Request duration in seconds # TYPE opossum_api_request_duration_seconds histogram opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"0.1\"} 8234 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"0.5\"} 11021 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"1.0\"} 11987 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"2.0\"} 12320 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"+Inf\"} 12453 opossum_api_request_duration_seconds_sum{method=\"GET\",endpoint=\"/search\"} 3945.7 opossum_api_request_duration_seconds_count{method=\"GET\",endpoint=\"/search\"} 12453 # HELP opossum_model_latency_seconds AI model inference latency in seconds # TYPE opossum_model_latency_seconds gauge opossum_model_latency_seconds{model=\"gemini\"} 0.345 opossum_model_latency_seconds{model=\"ollama\"} 0.123 opossum_model_latency_seconds{model=\"local-transformers\"} 0.078 Kubernetes Probes \u00b6 The health endpoints can be used as Kubernetes probes: Liveness Probe \u00b6 livenessProbe : httpGet : path : /health port : 8080 initialDelaySeconds : 30 periodSeconds : 10 timeoutSeconds : 5 failureThreshold : 3 Readiness Probe \u00b6 readinessProbe : httpGet : path : /health/api port : 8080 initialDelaySeconds : 5 periodSeconds : 5 timeoutSeconds : 3 successThreshold : 1 failureThreshold : 2 Service Status Page Integration \u00b6 The health endpoints can be integrated with status page providers: Statuspage.io Example \u00b6 GET /health/status-page Returns a status page compatible format: { \"page\" : { \"id\" : \"opossum-search\" , \"name\" : \"Opossum Search Status\" , \"url\" : \"https://status.opossumsearch.com\" }, \"components\" : [ { \"id\" : \"api\" , \"name\" : \"API\" , \"status\" : \"operational\" }, { \"id\" : \"search\" , \"name\" : \"Search Service\" , \"status\" : \"operational\" }, { \"id\" : \"models\" , \"name\" : \"AI Models\" , \"status\" : \"operational\" }, { \"id\" : \"image-processing\" , \"name\" : \"Image Processing\" , \"status\" : \"operational\" } ], \"incidents\" : [] } Health Check Best Practices \u00b6 Monitoring Recommendations \u00b6 Check Frequency : Poll the health endpoint every 30-60 seconds Alert Thresholds : Set up alerts for: Status changes from healthy to degraded or unhealthy Component status changes Latency increases beyond thresholds Error rate increases Dashboard Visualization : Create dashboards showing: Overall system health over time Component-specific health metrics Correlation between health and user-facing metrics Response Codes \u00b6 HTTP Status Meaning 200 System is healthy 200 System is degraded (check response body) 503 System is unhealthy 500 Health check itself failed Custom Health Checks \u00b6 Enterprise customers can create custom health checks through the API: POST /health/custom { \"name\" : \"custom-model-check\" , \"endpoint\" : \"models\" , \"check_type\" : \"latency\" , \"parameters\" : { \"threshold\" : 500 , \"sample_size\" : 5 }, \"notification_channels\" : [ \"email:alerts@example.com\" , \"slack:channel-id\" ], \"schedule\" : \"*/5 * * * *\" } Rate Limits \u00b6 Health endpoints have separate rate limits to ensure availability during incidents: Endpoint Unauthenticated Requests Authenticated Requests /health 60 rpm 300 rpm /health/{component} 20 rpm 100 rpm /health/deep Not available 10 rpm /metrics Not available 60 rpm rpm = requests per minute Health Data Retention \u00b6 Health check data is retained according to the following schedule: Data Type Retention Period Status changes 90 days Component metrics 30 days Raw health check responses 7 days Detailed diagnostic logs 3 days Maintenance Mode \u00b6 During scheduled maintenance, health endpoints will indicate maintenance mode: { \"status\" : \"maintenance\" , \"version\" : \"1.8.4\" , \"timestamp\" : \"2023-03-15T14:32:45Z\" , \"maintenance\" : { \"scheduled_start\" : \"2023-03-15T14:00:00Z\" , \"scheduled_end\" : \"2023-03-15T16:00:00Z\" , \"description\" : \"Database upgrade\" , \"affected_components\" : [ \"database\" , \"cache\" ] } }","title":"Health Endpoints"},{"location":"api/health-endpoints/#health-endpoints","text":"","title":"Health Endpoints"},{"location":"api/health-endpoints/#overview","text":"Opossum Search provides dedicated health check endpoints to monitor the status and availability of various system components. These endpoints are essential for monitoring, alerting, and ensuring service reliability. For a list of available API routes, see API Routes . For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . To learn about real-time notifications, refer to Webhooks .","title":"Overview"},{"location":"api/health-endpoints/#base-health-endpoint","text":"GET /health Provides an overall status of the Opossum Search system. Authentication : Not required Response Format : { \"status\" : \"healthy\" , \"version\" : \"1.8.4\" , \"timestamp\" : \"2023-03-15T14:32:45Z\" , \"components\" : { \"api\" : \"healthy\" , \"models\" : \"healthy\" , \"cache\" : \"healthy\" , \"database\" : \"healthy\" }, \"uptime\" : 1209467 } Possible Status Values : healthy : All systems operating normally degraded : System is operating with reduced capabilities unhealthy : System is experiencing significant issues","title":"Base Health Endpoint"},{"location":"api/health-endpoints/#component-specific-health-endpoints","text":"","title":"Component-Specific Health Endpoints"},{"location":"api/health-endpoints/#api-gateway-health","text":"GET /health/api Checks the health of the API gateway. Response Format : { \"status\" : \"healthy\" , \"version\" : \"1.8.4\" , \"response_time\" : 12 , \"load\" : 0.35 , \"request_rate\" : 453.2 , \"error_rate\" : 0.02 }","title":"API Gateway Health"},{"location":"api/health-endpoints/#model-service-health","text":"GET /health/models Checks the availability of AI models. Response Format : { \"status\" : \"healthy\" , \"models\" : [ { \"name\" : \"gemini\" , \"status\" : \"healthy\" , \"latency\" : 345 , \"success_rate\" : 0.998 , \"last_checked\" : \"2023-03-15T14:30:15Z\" }, { \"name\" : \"ollama\" , \"status\" : \"healthy\" , \"latency\" : 123 , \"success_rate\" : 0.999 , \"last_checked\" : \"2023-03-15T14:30:10Z\" }, { \"name\" : \"local-transformers\" , \"status\" : \"healthy\" , \"latency\" : 78 , \"success_rate\" : 1.0 , \"last_checked\" : \"2023-03-15T14:30:05Z\" } ] }","title":"Model Service Health"},{"location":"api/health-endpoints/#cache-health","text":"GET /health/cache Checks the health of the Redis cache. Response Format : { \"status\" : \"healthy\" , \"redis_version\" : \"6.2.6\" , \"latency\" : 2 , \"hit_rate\" : 0.87 , \"memory_usage\" : { \"used\" : \"1.2GB\" , \"total\" : \"4GB\" , \"percentage\" : 30 }, \"eviction_rate\" : 0.001 }","title":"Cache Health"},{"location":"api/health-endpoints/#database-health","text":"GET /health/database Checks the health of the database. Response Format : { \"status\" : \"healthy\" , \"database_type\" : \"PostgreSQL\" , \"version\" : \"13.4\" , \"latency\" : 5 , \"connections\" : { \"active\" : 12 , \"idle\" : 8 , \"max\" : 100 }, \"last_successful_query\" : \"2023-03-15T14:32:40Z\" }","title":"Database Health"},{"location":"api/health-endpoints/#storage-health","text":"GET /health/storage Checks the health of the object storage system. Response Format : { \"status\" : \"healthy\" , \"storage_type\" : \"S3\" , \"latency\" : 24 , \"space\" : { \"used\" : \"234GB\" , \"total\" : \"1TB\" , \"percentage\" : 23.4 }, \"last_successful_operation\" : \"2023-03-15T14:31:45Z\" }","title":"Storage Health"},{"location":"api/health-endpoints/#deep-health-checks","text":"For more detailed diagnostics, use the deep health check endpoint: GET /health/deep This performs thorough testing of each component, including: Database query execution Cache read/write operations Model inference tests Storage operations Inter-service communication Authentication : Required (Admin scope) Response Format : Similar to the base health endpoint but with extended details for each component.","title":"Deep Health Checks"},{"location":"api/health-endpoints/#integration-with-monitoring-systems","text":"","title":"Integration with Monitoring Systems"},{"location":"api/health-endpoints/#prometheus-endpoint","text":"GET /metrics Exposes metrics in Prometheus format for scraping. Authentication : Optional (configurable) Sample Output : # HELP opossum_api_requests_total Total number of API requests # TYPE opossum_api_requests_total counter opossum_api_requests_total{method=\"GET\",endpoint=\"/search\"} 12453 # HELP opossum_api_request_duration_seconds Request duration in seconds # TYPE opossum_api_request_duration_seconds histogram opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"0.1\"} 8234 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"0.5\"} 11021 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"1.0\"} 11987 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"2.0\"} 12320 opossum_api_request_duration_seconds_bucket{method=\"GET\",endpoint=\"/search\",le=\"+Inf\"} 12453 opossum_api_request_duration_seconds_sum{method=\"GET\",endpoint=\"/search\"} 3945.7 opossum_api_request_duration_seconds_count{method=\"GET\",endpoint=\"/search\"} 12453 # HELP opossum_model_latency_seconds AI model inference latency in seconds # TYPE opossum_model_latency_seconds gauge opossum_model_latency_seconds{model=\"gemini\"} 0.345 opossum_model_latency_seconds{model=\"ollama\"} 0.123 opossum_model_latency_seconds{model=\"local-transformers\"} 0.078","title":"Prometheus Endpoint"},{"location":"api/health-endpoints/#kubernetes-probes","text":"The health endpoints can be used as Kubernetes probes:","title":"Kubernetes Probes"},{"location":"api/health-endpoints/#liveness-probe","text":"livenessProbe : httpGet : path : /health port : 8080 initialDelaySeconds : 30 periodSeconds : 10 timeoutSeconds : 5 failureThreshold : 3","title":"Liveness Probe"},{"location":"api/health-endpoints/#readiness-probe","text":"readinessProbe : httpGet : path : /health/api port : 8080 initialDelaySeconds : 5 periodSeconds : 5 timeoutSeconds : 3 successThreshold : 1 failureThreshold : 2","title":"Readiness Probe"},{"location":"api/health-endpoints/#service-status-page-integration","text":"The health endpoints can be integrated with status page providers:","title":"Service Status Page Integration"},{"location":"api/health-endpoints/#statuspageio-example","text":"GET /health/status-page Returns a status page compatible format: { \"page\" : { \"id\" : \"opossum-search\" , \"name\" : \"Opossum Search Status\" , \"url\" : \"https://status.opossumsearch.com\" }, \"components\" : [ { \"id\" : \"api\" , \"name\" : \"API\" , \"status\" : \"operational\" }, { \"id\" : \"search\" , \"name\" : \"Search Service\" , \"status\" : \"operational\" }, { \"id\" : \"models\" , \"name\" : \"AI Models\" , \"status\" : \"operational\" }, { \"id\" : \"image-processing\" , \"name\" : \"Image Processing\" , \"status\" : \"operational\" } ], \"incidents\" : [] }","title":"Statuspage.io Example"},{"location":"api/health-endpoints/#health-check-best-practices","text":"","title":"Health Check Best Practices"},{"location":"api/health-endpoints/#monitoring-recommendations","text":"Check Frequency : Poll the health endpoint every 30-60 seconds Alert Thresholds : Set up alerts for: Status changes from healthy to degraded or unhealthy Component status changes Latency increases beyond thresholds Error rate increases Dashboard Visualization : Create dashboards showing: Overall system health over time Component-specific health metrics Correlation between health and user-facing metrics","title":"Monitoring Recommendations"},{"location":"api/health-endpoints/#response-codes","text":"HTTP Status Meaning 200 System is healthy 200 System is degraded (check response body) 503 System is unhealthy 500 Health check itself failed","title":"Response Codes"},{"location":"api/health-endpoints/#custom-health-checks","text":"Enterprise customers can create custom health checks through the API: POST /health/custom { \"name\" : \"custom-model-check\" , \"endpoint\" : \"models\" , \"check_type\" : \"latency\" , \"parameters\" : { \"threshold\" : 500 , \"sample_size\" : 5 }, \"notification_channels\" : [ \"email:alerts@example.com\" , \"slack:channel-id\" ], \"schedule\" : \"*/5 * * * *\" }","title":"Custom Health Checks"},{"location":"api/health-endpoints/#rate-limits","text":"Health endpoints have separate rate limits to ensure availability during incidents: Endpoint Unauthenticated Requests Authenticated Requests /health 60 rpm 300 rpm /health/{component} 20 rpm 100 rpm /health/deep Not available 10 rpm /metrics Not available 60 rpm rpm = requests per minute","title":"Rate Limits"},{"location":"api/health-endpoints/#health-data-retention","text":"Health check data is retained according to the following schedule: Data Type Retention Period Status changes 90 days Component metrics 30 days Raw health check responses 7 days Detailed diagnostic logs 3 days","title":"Health Data Retention"},{"location":"api/health-endpoints/#maintenance-mode","text":"During scheduled maintenance, health endpoints will indicate maintenance mode: { \"status\" : \"maintenance\" , \"version\" : \"1.8.4\" , \"timestamp\" : \"2023-03-15T14:32:45Z\" , \"maintenance\" : { \"scheduled_start\" : \"2023-03-15T14:00:00Z\" , \"scheduled_end\" : \"2023-03-15T16:00:00Z\" , \"description\" : \"Database upgrade\" , \"affected_components\" : [ \"database\" , \"cache\" ] } }","title":"Maintenance Mode"},{"location":"api/oauth-configuration/","text":"OAuth 2.0 Configuration \u00b6 Overview \u00b6 Opossum Search supports OAuth 2.0 authentication for secure user authorization. This document explains how to configure and use OAuth 2.0 in your applications. Prerequisites \u00b6 A registered OAuth application (client ID and secret) HTTPS enabled for your redirect URI Valid API credentials OAuth Flow Configuration \u00b6 1. Authorization Request \u00b6 GET /oauth/authorize Required Parameters: client_id - Your OAuth client ID redirect_uri - URL to return to after authorization response_type - Must be \"code\" scope - Space-separated list of requested permissions Optional Parameters: state - Random string to prevent CSRF attacks prompt - \"none\", \"login\", or \"consent\" Example Request: https://api.opossumsearch.com/oauth/authorize? client_id=your_client_id& redirect_uri=https://your-app.com/callback& response_type=code& scope=read write& state=random_state_string 2. Token Exchange \u00b6 POST /oauth/token Required Parameters: grant_type - Must be \"authorization_code\" code - The authorization code received redirect_uri - Same URI used in authorization client_id - Your OAuth client ID client_secret - Your OAuth client secret Example Request: curl -X POST https://api.opossumsearch.com/oauth/token \\ -d grant_type = authorization_code \\ -d code = received_auth_code \\ -d redirect_uri = https://your-app.com/callback \\ -d client_id = your_client_id \\ -d client_secret = your_client_secret Example Response: { \"access_token\" : \"access_token_value\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 3600 , \"refresh_token\" : \"refresh_token_value\" , \"scope\" : \"read write\" } 3. Token Refresh \u00b6 POST /oauth/token Required Parameters: grant_type - Must be \"refresh_token\" refresh_token - The refresh token client_id - Your OAuth client ID client_secret - Your OAuth client secret Example Request: curl -X POST https://api.opossumsearch.com/oauth/token \\ -d grant_type = refresh_token \\ -d refresh_token = refresh_token_value \\ -d client_id = your_client_id \\ -d client_secret = your_client_secret Scopes \u00b6 Scope Description read Read-only access to public data write Ability to modify data admin Administrative access (requires approval) Security Considerations \u00b6 Always use HTTPS for all OAuth endpoints Validate the state parameter to prevent CSRF Store tokens securely (encrypted at rest) Never expose client secrets in client-side code Implement token rotation for long-lived sessions Error Handling \u00b6 Common Error Responses \u00b6 Error Code Description Resolution invalid_request Missing required parameter Check request parameters invalid_client Invalid client credentials Verify client ID and secret invalid_grant Invalid authorization code Request new authorization invalid_scope Requested scope not valid Check scope permissions Example Error Response: \u00b6 { \"error\" : \"invalid_request\" , \"error_description\" : \"Missing required parameter: grant_type\" , \"error_uri\" : \"https://api.opossumsearch.com/docs/oauth-errors#invalid_request\" } Implementation Examples \u00b6 JavaScript/Node.js \u00b6 const config = { clientId : 'your_client_id' , clientSecret : 'your_client_secret' , redirectUri : 'https://your-app.com/callback' }; // Authorization URL construction const authUrl = `https://api.opossumsearch.com/oauth/authorize? client_id= ${ config . clientId } & redirect_uri= ${ encodeURIComponent ( config . redirectUri ) } & response_type=code& scope=read%20write` ; // Token exchange async function exchangeCode ( code ) { const response = await fetch ( 'https://api.opossumsearch.com/oauth/token' , { method : 'POST' , headers : { 'Content-Type' : 'application/x-www-form-urlencoded' , }, body : new URLSearchParams ({ grant_type : 'authorization_code' , code , redirect_uri : config . redirectUri , client_id : config . clientId , client_secret : config . clientSecret , }), }); return response . json (); } Python \u00b6 import requests config = { 'client_id' : 'your_client_id' , 'client_secret' : 'your_client_secret' , 'redirect_uri' : 'https://your-app.com/callback' } # Authorization URL construction auth_url = ( 'https://api.opossumsearch.com/oauth/authorize?' f 'client_id= { config [ \"client_id\" ] } &' f 'redirect_uri= { quote ( config [ \"redirect_uri\" ]) } &' 'response_type=code&' 'scope=read%20write' ) # Token exchange def exchange_code ( code ): response = requests . post ( 'https://api.opossumsearch.com/oauth/token' , data = { 'grant_type' : 'authorization_code' , 'code' : code , 'redirect_uri' : config [ 'redirect_uri' ], 'client_id' : config [ 'client_id' ], 'client_secret' : config [ 'client_secret' ], } ) return response . json ()","title":"OAuth Configuration"},{"location":"api/oauth-configuration/#oauth-20-configuration","text":"","title":"OAuth 2.0 Configuration"},{"location":"api/oauth-configuration/#overview","text":"Opossum Search supports OAuth 2.0 authentication for secure user authorization. This document explains how to configure and use OAuth 2.0 in your applications.","title":"Overview"},{"location":"api/oauth-configuration/#prerequisites","text":"A registered OAuth application (client ID and secret) HTTPS enabled for your redirect URI Valid API credentials","title":"Prerequisites"},{"location":"api/oauth-configuration/#oauth-flow-configuration","text":"","title":"OAuth Flow Configuration"},{"location":"api/oauth-configuration/#1-authorization-request","text":"GET /oauth/authorize Required Parameters: client_id - Your OAuth client ID redirect_uri - URL to return to after authorization response_type - Must be \"code\" scope - Space-separated list of requested permissions Optional Parameters: state - Random string to prevent CSRF attacks prompt - \"none\", \"login\", or \"consent\" Example Request: https://api.opossumsearch.com/oauth/authorize? client_id=your_client_id& redirect_uri=https://your-app.com/callback& response_type=code& scope=read write& state=random_state_string","title":"1. Authorization Request"},{"location":"api/oauth-configuration/#2-token-exchange","text":"POST /oauth/token Required Parameters: grant_type - Must be \"authorization_code\" code - The authorization code received redirect_uri - Same URI used in authorization client_id - Your OAuth client ID client_secret - Your OAuth client secret Example Request: curl -X POST https://api.opossumsearch.com/oauth/token \\ -d grant_type = authorization_code \\ -d code = received_auth_code \\ -d redirect_uri = https://your-app.com/callback \\ -d client_id = your_client_id \\ -d client_secret = your_client_secret Example Response: { \"access_token\" : \"access_token_value\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 3600 , \"refresh_token\" : \"refresh_token_value\" , \"scope\" : \"read write\" }","title":"2. Token Exchange"},{"location":"api/oauth-configuration/#3-token-refresh","text":"POST /oauth/token Required Parameters: grant_type - Must be \"refresh_token\" refresh_token - The refresh token client_id - Your OAuth client ID client_secret - Your OAuth client secret Example Request: curl -X POST https://api.opossumsearch.com/oauth/token \\ -d grant_type = refresh_token \\ -d refresh_token = refresh_token_value \\ -d client_id = your_client_id \\ -d client_secret = your_client_secret","title":"3. Token Refresh"},{"location":"api/oauth-configuration/#scopes","text":"Scope Description read Read-only access to public data write Ability to modify data admin Administrative access (requires approval)","title":"Scopes"},{"location":"api/oauth-configuration/#security-considerations","text":"Always use HTTPS for all OAuth endpoints Validate the state parameter to prevent CSRF Store tokens securely (encrypted at rest) Never expose client secrets in client-side code Implement token rotation for long-lived sessions","title":"Security Considerations"},{"location":"api/oauth-configuration/#error-handling","text":"","title":"Error Handling"},{"location":"api/oauth-configuration/#common-error-responses","text":"Error Code Description Resolution invalid_request Missing required parameter Check request parameters invalid_client Invalid client credentials Verify client ID and secret invalid_grant Invalid authorization code Request new authorization invalid_scope Requested scope not valid Check scope permissions","title":"Common Error Responses"},{"location":"api/oauth-configuration/#example-error-response","text":"{ \"error\" : \"invalid_request\" , \"error_description\" : \"Missing required parameter: grant_type\" , \"error_uri\" : \"https://api.opossumsearch.com/docs/oauth-errors#invalid_request\" }","title":"Example Error Response:"},{"location":"api/oauth-configuration/#implementation-examples","text":"","title":"Implementation Examples"},{"location":"api/oauth-configuration/#javascriptnodejs","text":"const config = { clientId : 'your_client_id' , clientSecret : 'your_client_secret' , redirectUri : 'https://your-app.com/callback' }; // Authorization URL construction const authUrl = `https://api.opossumsearch.com/oauth/authorize? client_id= ${ config . clientId } & redirect_uri= ${ encodeURIComponent ( config . redirectUri ) } & response_type=code& scope=read%20write` ; // Token exchange async function exchangeCode ( code ) { const response = await fetch ( 'https://api.opossumsearch.com/oauth/token' , { method : 'POST' , headers : { 'Content-Type' : 'application/x-www-form-urlencoded' , }, body : new URLSearchParams ({ grant_type : 'authorization_code' , code , redirect_uri : config . redirectUri , client_id : config . clientId , client_secret : config . clientSecret , }), }); return response . json (); }","title":"JavaScript/Node.js"},{"location":"api/oauth-configuration/#python","text":"import requests config = { 'client_id' : 'your_client_id' , 'client_secret' : 'your_client_secret' , 'redirect_uri' : 'https://your-app.com/callback' } # Authorization URL construction auth_url = ( 'https://api.opossumsearch.com/oauth/authorize?' f 'client_id= { config [ \"client_id\" ] } &' f 'redirect_uri= { quote ( config [ \"redirect_uri\" ]) } &' 'response_type=code&' 'scope=read%20write' ) # Token exchange def exchange_code ( code ): response = requests . post ( 'https://api.opossumsearch.com/oauth/token' , data = { 'grant_type' : 'authorization_code' , 'code' : code , 'redirect_uri' : config [ 'redirect_uri' ], 'client_id' : config [ 'client_id' ], 'client_secret' : config [ 'client_secret' ], } ) return response . json ()","title":"Python"},{"location":"api/rate-limits/","text":"Rate Limits \u00b6 Overview \u00b6 To ensure optimal performance and fair usage for all users, Opossum Search implements rate limiting across all API endpoints. This document outlines the rate limit policies, quotas, and best practices for handling rate-limiting scenarios. For a list of available API routes, see API Routes . For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . To learn about real-time notifications, refer to Webhooks . For system status and component health, see Health Endpoints . Rate Limit Structure \u00b6 Opossum Search uses a tiered rate limiting structure based on: API Key / User : Limits per unique API key Endpoint : Different limits for different operations IP Address : To prevent abuse, even with valid credentials Resource Type : Different limits for different resources (models, storage, etc.) Default Rate Limits \u00b6 Plan Requests per Minute Concurrent Requests Daily Quota Basic 60 5 10,000 Professional 300 20 50,000 Enterprise 1,200 100 Customizable Endpoint-Specific Limits \u00b6 Different endpoints have different resource requirements and corresponding rate limits: Endpoint Category Basic (RPM) Professional (RPM) Enterprise (RPM) Search 60 300 1,200 Image Analysis 20 100 400 SVG Generation 30 150 600 Conversation 40 200 800 Configuration 15 60 240 Health/Status 120 600 2,400 RPM = Requests Per Minute Model-Specific Limits \u00b6 Different AI models have different resource requirements: Model Basic (RPM) Professional (RPM) Enterprise (RPM) Gemini 30 150 600 Ollama 60 300 1,200 Local Models 120 600 2,400 Bulk Operation Limits \u00b6 For bulk operations: Operation Max Items Per Request Max Requests Per Hour Multi-Search 10 100 Batch Image Analysis 5 50 Bulk SVG Generation 20 200 Rate Limit Headers \u00b6 Each API response includes rate limit information in the headers: Header Description X-Rate-Limit-Limit The rate limit ceiling for the given endpoint X-Rate-Limit-Remaining The number of requests left for the time window X-Rate-Limit-Reset The time at which the current rate limit window resets in UTC epoch seconds X-Rate-Limit-Used The number of requests made in the current time window X-Rate-Limit-Resource The resource being rate-limited (e.g., \"search\", \"model:gemini\") Example headers: X-Rate-Limit-Limit: 60 X-Rate-Limit-Remaining: 45 X-Rate-Limit-Reset: 1616172903 X-Rate-Limit-Used: 15 X-Rate-Limit-Resource: search Rate Limit Response \u00b6 When a rate limit is exceeded, the API responds with: HTTP Status Code: 429 Too Many Requests Response Body: { \"success\" : false , \"error\" : { \"code\" : \"rate_limit_exceeded\" , \"message\" : \"You have exceeded the rate limit for this endpoint\" , \"details\" : [ { \"limit\" : 60 , \"remaining\" : 0 , \"reset\" : 1616172903 , \"resource\" : \"search\" } ] }, \"meta\" : { \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" } } The API may also include a Retry-After header indicating the number of seconds to wait before retrying. Handling Rate Limits \u00b6 Best Practices \u00b6 Implement backoff strategies : Use exponential backoff with jitter Respect the Retry-After header when provided Limit maximum retry attempts Optimize request patterns : Batch requests when possible using bulk endpoints Cache responses for frequently accessed data Prioritize critical requests when nearing limits Monitor usage : Track rate limit headers in responses Set up alerts for approaching limits Use the dashboard to monitor historical usage Example Backoff Implementation \u00b6 async function makeRequestWithBackoff ( url , options , maxRetries = 5 ) { let retries = 0 ; while ( retries < maxRetries ) { try { const response = await fetch ( url , options ); if ( response . status !== 429 ) { return response ; } // Handle rate limiting retries ++ ; // Get retry time from header or calculate exponential backoff const retryAfter = response . headers . get ( 'Retry-After' ); const waitTime = retryAfter ? parseInt ( retryAfter , 10 ) * 1000 : Math . min ( Math . pow ( 2 , retries ) * 1000 + Math . random () * 1000 , 60000 ); console . log ( `Rate limited. Retrying in ${ waitTime } ms (attempt ${ retries } of ${ maxRetries } )` ); await new Promise ( resolve => setTimeout ( resolve , waitTime )); } catch ( error ) { retries ++ ; if ( retries >= maxRetries ) { throw error ; } // Exponential backoff for network errors const waitTime = Math . min ( Math . pow ( 2 , retries ) * 1000 + Math . random () * 1000 , 60000 ); await new Promise ( resolve => setTimeout ( resolve , waitTime )); } } throw new Error ( `Failed after ${ maxRetries } retries` ); } Monitoring Rate Limit Usage \u00b6 The dashboard provides real-time and historical rate limit usage: Real-time Monitor : View current usage and remaining capacity Usage Reports : Analyze usage patterns over time Alerts : Set up notifications for approaching limits Rate Limit Events : Log of rate limit events Quota Extensions \u00b6 For temporary increases in capacity: Burst Capacity : Short-term rate limit increases for planned events Emergency Increases : For critical scenarios Gradual Scaling : For growing applications Contact support for quota extension requests. Rate Limiting Strategies \u00b6 Opossum Search implements several rate limiting algorithms: Token Bucket : Primary algorithm for most endpoints Leaky Bucket : For high-throughput endpoints Sliding Window : For conversation and session-based endpoints Concurrent Request Limiting : For resource-intensive operations Regional Considerations \u00b6 Rate limits may vary by region due to infrastructure differences: Region Adjustment Factor us-east 1.0x (baseline) us-west 1.0x eu-central 0.9x ap-southeast 0.8x Custom Rate Limit Plans \u00b6 Enterprise customers can request custom rate limit configurations: Model-specific adjustments Endpoint-specific quotas Time-based quota allocation Reserved capacity Contact your account manager to discuss custom rate limit plans. Exemptions and Whitelisting \u00b6 Critical services that require exemptions from standard rate limits: Status Page Monitors : Health checking services Authorized Partners : Strategic integration partners Internal Services : Opossum internal monitoring Exemption requests require security review and approval.","title":"Rate Limits"},{"location":"api/rate-limits/#rate-limits","text":"","title":"Rate Limits"},{"location":"api/rate-limits/#overview","text":"To ensure optimal performance and fair usage for all users, Opossum Search implements rate limiting across all API endpoints. This document outlines the rate limit policies, quotas, and best practices for handling rate-limiting scenarios. For a list of available API routes, see API Routes . For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . To learn about real-time notifications, refer to Webhooks . For system status and component health, see Health Endpoints .","title":"Overview"},{"location":"api/rate-limits/#rate-limit-structure","text":"Opossum Search uses a tiered rate limiting structure based on: API Key / User : Limits per unique API key Endpoint : Different limits for different operations IP Address : To prevent abuse, even with valid credentials Resource Type : Different limits for different resources (models, storage, etc.)","title":"Rate Limit Structure"},{"location":"api/rate-limits/#default-rate-limits","text":"Plan Requests per Minute Concurrent Requests Daily Quota Basic 60 5 10,000 Professional 300 20 50,000 Enterprise 1,200 100 Customizable","title":"Default Rate Limits"},{"location":"api/rate-limits/#endpoint-specific-limits","text":"Different endpoints have different resource requirements and corresponding rate limits: Endpoint Category Basic (RPM) Professional (RPM) Enterprise (RPM) Search 60 300 1,200 Image Analysis 20 100 400 SVG Generation 30 150 600 Conversation 40 200 800 Configuration 15 60 240 Health/Status 120 600 2,400 RPM = Requests Per Minute","title":"Endpoint-Specific Limits"},{"location":"api/rate-limits/#model-specific-limits","text":"Different AI models have different resource requirements: Model Basic (RPM) Professional (RPM) Enterprise (RPM) Gemini 30 150 600 Ollama 60 300 1,200 Local Models 120 600 2,400","title":"Model-Specific Limits"},{"location":"api/rate-limits/#bulk-operation-limits","text":"For bulk operations: Operation Max Items Per Request Max Requests Per Hour Multi-Search 10 100 Batch Image Analysis 5 50 Bulk SVG Generation 20 200","title":"Bulk Operation Limits"},{"location":"api/rate-limits/#rate-limit-headers","text":"Each API response includes rate limit information in the headers: Header Description X-Rate-Limit-Limit The rate limit ceiling for the given endpoint X-Rate-Limit-Remaining The number of requests left for the time window X-Rate-Limit-Reset The time at which the current rate limit window resets in UTC epoch seconds X-Rate-Limit-Used The number of requests made in the current time window X-Rate-Limit-Resource The resource being rate-limited (e.g., \"search\", \"model:gemini\") Example headers: X-Rate-Limit-Limit: 60 X-Rate-Limit-Remaining: 45 X-Rate-Limit-Reset: 1616172903 X-Rate-Limit-Used: 15 X-Rate-Limit-Resource: search","title":"Rate Limit Headers"},{"location":"api/rate-limits/#rate-limit-response","text":"When a rate limit is exceeded, the API responds with: HTTP Status Code: 429 Too Many Requests Response Body: { \"success\" : false , \"error\" : { \"code\" : \"rate_limit_exceeded\" , \"message\" : \"You have exceeded the rate limit for this endpoint\" , \"details\" : [ { \"limit\" : 60 , \"remaining\" : 0 , \"reset\" : 1616172903 , \"resource\" : \"search\" } ] }, \"meta\" : { \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" } } The API may also include a Retry-After header indicating the number of seconds to wait before retrying.","title":"Rate Limit Response"},{"location":"api/rate-limits/#handling-rate-limits","text":"","title":"Handling Rate Limits"},{"location":"api/rate-limits/#best-practices","text":"Implement backoff strategies : Use exponential backoff with jitter Respect the Retry-After header when provided Limit maximum retry attempts Optimize request patterns : Batch requests when possible using bulk endpoints Cache responses for frequently accessed data Prioritize critical requests when nearing limits Monitor usage : Track rate limit headers in responses Set up alerts for approaching limits Use the dashboard to monitor historical usage","title":"Best Practices"},{"location":"api/rate-limits/#example-backoff-implementation","text":"async function makeRequestWithBackoff ( url , options , maxRetries = 5 ) { let retries = 0 ; while ( retries < maxRetries ) { try { const response = await fetch ( url , options ); if ( response . status !== 429 ) { return response ; } // Handle rate limiting retries ++ ; // Get retry time from header or calculate exponential backoff const retryAfter = response . headers . get ( 'Retry-After' ); const waitTime = retryAfter ? parseInt ( retryAfter , 10 ) * 1000 : Math . min ( Math . pow ( 2 , retries ) * 1000 + Math . random () * 1000 , 60000 ); console . log ( `Rate limited. Retrying in ${ waitTime } ms (attempt ${ retries } of ${ maxRetries } )` ); await new Promise ( resolve => setTimeout ( resolve , waitTime )); } catch ( error ) { retries ++ ; if ( retries >= maxRetries ) { throw error ; } // Exponential backoff for network errors const waitTime = Math . min ( Math . pow ( 2 , retries ) * 1000 + Math . random () * 1000 , 60000 ); await new Promise ( resolve => setTimeout ( resolve , waitTime )); } } throw new Error ( `Failed after ${ maxRetries } retries` ); }","title":"Example Backoff Implementation"},{"location":"api/rate-limits/#monitoring-rate-limit-usage","text":"The dashboard provides real-time and historical rate limit usage: Real-time Monitor : View current usage and remaining capacity Usage Reports : Analyze usage patterns over time Alerts : Set up notifications for approaching limits Rate Limit Events : Log of rate limit events","title":"Monitoring Rate Limit Usage"},{"location":"api/rate-limits/#quota-extensions","text":"For temporary increases in capacity: Burst Capacity : Short-term rate limit increases for planned events Emergency Increases : For critical scenarios Gradual Scaling : For growing applications Contact support for quota extension requests.","title":"Quota Extensions"},{"location":"api/rate-limits/#rate-limiting-strategies","text":"Opossum Search implements several rate limiting algorithms: Token Bucket : Primary algorithm for most endpoints Leaky Bucket : For high-throughput endpoints Sliding Window : For conversation and session-based endpoints Concurrent Request Limiting : For resource-intensive operations","title":"Rate Limiting Strategies"},{"location":"api/rate-limits/#regional-considerations","text":"Rate limits may vary by region due to infrastructure differences: Region Adjustment Factor us-east 1.0x (baseline) us-west 1.0x eu-central 0.9x ap-southeast 0.8x","title":"Regional Considerations"},{"location":"api/rate-limits/#custom-rate-limit-plans","text":"Enterprise customers can request custom rate limit configurations: Model-specific adjustments Endpoint-specific quotas Time-based quota allocation Reserved capacity Contact your account manager to discuss custom rate limit plans.","title":"Custom Rate Limit Plans"},{"location":"api/rate-limits/#exemptions-and-whitelisting","text":"Critical services that require exemptions from standard rate limits: Status Page Monitors : Health checking services Authorized Partners : Strategic integration partners Internal Services : Opossum internal monitoring Exemption requests require security review and approval.","title":"Exemptions and Whitelisting"},{"location":"api/request-response/","text":"Request and Response Formats \u00b6 Overview \u00b6 This document details the standard request and response formats used across the Opossum Search API, including headers, body structures, and status codes. For a list of available API routes, see API Routes . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . To learn about real-time notifications, refer to Webhooks . For system status and component health, see Health Endpoints . Common Headers \u00b6 Request Headers \u00b6 Header Description Required Content-Type The format of the request body (usually application/json ) Yes (for requests with bodies) Accept The format the client can accept (usually application/json ) No (defaults to application/json ) X-API-Key API key for authentication Yes (unless using Bearer) Authorization Bearer token for authentication ( Bearer <token> ) Yes (unless using API key) X-Request-ID Unique ID for tracking the request No (created if not provided) X-Correlation-ID ID for tracking related requests No User-Agent Client application identifier No Response Headers \u00b6 Header Description Content-Type Format of response body (usually application/json ) X-Request-ID Mirrors the request ID (or generated) X-Rate-Limit-Limit Rate limit ceiling for the endpoint X-Rate-Limit-Remaining Number of requests left for the time window X-Rate-Limit-Reset Seconds until the rate limit resets X-Processing-Time Time in milliseconds to process the request X-Cache-Status Cache status ( HIT , MISS , STALE , SKIP ) X-Version API version Standard Response Format \u00b6 All API responses follow a consistent structure: { \"success\" : true , \"data\" : { ... }, \"meta\" : { \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" , \"processing_time\" : 123 , \"model_used\" : \"gemini\" } } For errors: { \"success\" : false , \"error\" : { \"code\" : \"invalid_request\" , \"message\" : \"The request was invalid\" , \"details\" : [ ... ] }, \"meta\" : { \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" , \"processing_time\" : 45 } } HTTP Status Codes \u00b6 Code Description 200 OK - The request succeeded 201 Created - A new resource was created 202 Accepted - The request was accepted for processing 204 No Content - The request succeeded but returns no content 400 Bad Request - The request could not be understood 401 Unauthorized - Authentication failed 403 Forbidden - Authentication succeeded but insufficient permissions 404 Not Found - The requested resource does not exist 409 Conflict - The request conflicts with the current state 422 Unprocessable Entity - Validation errors 429 Too Many Requests - Rate limit exceeded 500 Internal Server Error - Something went wrong on the server 503 Service Unavailable - The service is temporarily unavailable 504 Gateway Timeout - A dependent service timed out Pagination \u00b6 For endpoints that return collections, pagination is applied: Request Parameters \u00b6 Parameter Description Default limit Number of items per page 20 offset Number of items to skip 0 cursor Opaque cursor for more efficient pagination - Paginated Response \u00b6 { \"success\" : true , \"data\" : [ ... ], \"meta\" : { \"pagination\" : { \"total\" : 243 , \"limit\" : 20 , \"offset\" : 40 , \"next_cursor\" : \"Y3Vyc29yOjYw\" , \"prev_cursor\" : \"Y3Vyc29yOjIw\" }, \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" } } Search Request Format \u00b6 Basic Search \u00b6 { \"query\" : \"opossum behavior\" , \"options\" : { \"model\" : \"gemini\" , \"mode\" : \"balanced\" , \"limit\" : 10 }, \"filters\" : { \"date_range\" : { \"from\" : \"2023-01-01\" , \"to\" : \"2023-12-31\" }, \"content_type\" : [ \"article\" , \"research\" ] }, \"context\" : { \"user_location\" : \"North America\" , \"previous_searches\" : [ \"marsupial species\" ] } } Image Analysis \u00b6 { \"image_url\" : \"https://example.com/images/opossum.jpg\" , \"options\" : { \"detail_level\" : \"high\" , \"analysis_type\" : \"object_detection\" } } Response Examples \u00b6 Search Response \u00b6 { \"success\" : true , \"data\" : { \"results\" : [ { \"title\" : \"North American Opossum Behavior Patterns\" , \"snippet\" : \"The opossum is known for its defensive behavior of 'playing dead' when threatened...\" , \"source\" : \"Wildlife Journal\" , \"relevance_score\" : 0.92 , \"url\" : \"https://example.com/articles/opossum-behavior\" }, // Additional results... ], \"suggested_queries\" : [ \"opossum diet\" , \"opossum habitat\" ], \"knowledge_graph\" : { \"entity\" : \"Virginia opossum\" , \"attributes\" : { \"scientific_name\" : \"Didelphis virginiana\" , \"lifespan\" : \"2-4 years\" , \"diet\" : \"Omnivore\" } } }, \"meta\" : { \"processing_time\" : 156 , \"model_used\" : \"gemini\" , \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" , \"cache_status\" : \"MISS\" } } Image Analysis Response \u00b6 { \"success\" : true , \"data\" : { \"objects_detected\" : [ { \"label\" : \"opossum\" , \"confidence\" : 0.97 , \"bounding_box\" : { \"x\" : 23 , \"y\" : 45 , \"width\" : 200 , \"height\" : 150 } }, { \"label\" : \"tree\" , \"confidence\" : 0.85 , \"bounding_box\" : { \"x\" : 300 , \"y\" : 10 , \"width\" : 100 , \"height\" : 400 } } ], \"scene_description\" : \"An opossum climbing on a tree at night\" , \"tags\" : [ \"wildlife\" , \"nocturnal\" , \"marsupial\" , \"forest\" ] }, \"meta\" : { \"processing_time\" : 345 , \"model_used\" : \"gemini-vision\" , \"request_id\" : \"req_9g8h7j6k5l4k3j2h\" , \"image_dimensions\" : { \"width\" : 1024 , \"height\" : 768 } } } Error Response Example \u00b6 { \"success\" : false , \"error\" : { \"code\" : \"invalid_parameter\" , \"message\" : \"Invalid value for parameter 'model'\" , \"details\" : [ { \"field\" : \"model\" , \"message\" : \"Model 'gpt-5' is not available. Available models: gemini, ollama, local\" } ] }, \"meta\" : { \"request_id\" : \"req_5d6f7g8h9j0k1l2m\" , \"processing_time\" : 12 } } Versioning \u00b6 The API is versioned using URL path. The current version is v1 . CORS \u00b6 The API supports Cross-Origin Resource Sharing (CORS) for browser applications: Header Value Access-Control-Allow-Origin * or configured domains Access-Control-Allow-Methods GET, POST, PUT, PATCH, DELETE, OPTIONS Access-Control-Allow-Headers Origin, Content-Type, Accept, Authorization, X-API-Key, X-Request-ID Access-Control-Max-Age 86400 (24 hours) Request Timeouts \u00b6 Operation Timeout Standard search 10 seconds Image analysis 30 seconds SVG generation 15 seconds Bulk operations 60 seconds Requests exceeding these timeouts will receive a 504 Gateway Timeout response.","title":"Request/Response"},{"location":"api/request-response/#request-and-response-formats","text":"","title":"Request and Response Formats"},{"location":"api/request-response/#overview","text":"This document details the standard request and response formats used across the Opossum Search API, including headers, body structures, and status codes. For a list of available API routes, see API Routes . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . To learn about real-time notifications, refer to Webhooks . For system status and component health, see Health Endpoints .","title":"Overview"},{"location":"api/request-response/#common-headers","text":"","title":"Common Headers"},{"location":"api/request-response/#request-headers","text":"Header Description Required Content-Type The format of the request body (usually application/json ) Yes (for requests with bodies) Accept The format the client can accept (usually application/json ) No (defaults to application/json ) X-API-Key API key for authentication Yes (unless using Bearer) Authorization Bearer token for authentication ( Bearer <token> ) Yes (unless using API key) X-Request-ID Unique ID for tracking the request No (created if not provided) X-Correlation-ID ID for tracking related requests No User-Agent Client application identifier No","title":"Request Headers"},{"location":"api/request-response/#response-headers","text":"Header Description Content-Type Format of response body (usually application/json ) X-Request-ID Mirrors the request ID (or generated) X-Rate-Limit-Limit Rate limit ceiling for the endpoint X-Rate-Limit-Remaining Number of requests left for the time window X-Rate-Limit-Reset Seconds until the rate limit resets X-Processing-Time Time in milliseconds to process the request X-Cache-Status Cache status ( HIT , MISS , STALE , SKIP ) X-Version API version","title":"Response Headers"},{"location":"api/request-response/#standard-response-format","text":"All API responses follow a consistent structure: { \"success\" : true , \"data\" : { ... }, \"meta\" : { \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" , \"processing_time\" : 123 , \"model_used\" : \"gemini\" } } For errors: { \"success\" : false , \"error\" : { \"code\" : \"invalid_request\" , \"message\" : \"The request was invalid\" , \"details\" : [ ... ] }, \"meta\" : { \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" , \"processing_time\" : 45 } }","title":"Standard Response Format"},{"location":"api/request-response/#http-status-codes","text":"Code Description 200 OK - The request succeeded 201 Created - A new resource was created 202 Accepted - The request was accepted for processing 204 No Content - The request succeeded but returns no content 400 Bad Request - The request could not be understood 401 Unauthorized - Authentication failed 403 Forbidden - Authentication succeeded but insufficient permissions 404 Not Found - The requested resource does not exist 409 Conflict - The request conflicts with the current state 422 Unprocessable Entity - Validation errors 429 Too Many Requests - Rate limit exceeded 500 Internal Server Error - Something went wrong on the server 503 Service Unavailable - The service is temporarily unavailable 504 Gateway Timeout - A dependent service timed out","title":"HTTP Status Codes"},{"location":"api/request-response/#pagination","text":"For endpoints that return collections, pagination is applied:","title":"Pagination"},{"location":"api/request-response/#request-parameters","text":"Parameter Description Default limit Number of items per page 20 offset Number of items to skip 0 cursor Opaque cursor for more efficient pagination -","title":"Request Parameters"},{"location":"api/request-response/#paginated-response","text":"{ \"success\" : true , \"data\" : [ ... ], \"meta\" : { \"pagination\" : { \"total\" : 243 , \"limit\" : 20 , \"offset\" : 40 , \"next_cursor\" : \"Y3Vyc29yOjYw\" , \"prev_cursor\" : \"Y3Vyc29yOjIw\" }, \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" } }","title":"Paginated Response"},{"location":"api/request-response/#search-request-format","text":"","title":"Search Request Format"},{"location":"api/request-response/#basic-search","text":"{ \"query\" : \"opossum behavior\" , \"options\" : { \"model\" : \"gemini\" , \"mode\" : \"balanced\" , \"limit\" : 10 }, \"filters\" : { \"date_range\" : { \"from\" : \"2023-01-01\" , \"to\" : \"2023-12-31\" }, \"content_type\" : [ \"article\" , \"research\" ] }, \"context\" : { \"user_location\" : \"North America\" , \"previous_searches\" : [ \"marsupial species\" ] } }","title":"Basic Search"},{"location":"api/request-response/#image-analysis","text":"{ \"image_url\" : \"https://example.com/images/opossum.jpg\" , \"options\" : { \"detail_level\" : \"high\" , \"analysis_type\" : \"object_detection\" } }","title":"Image Analysis"},{"location":"api/request-response/#response-examples","text":"","title":"Response Examples"},{"location":"api/request-response/#search-response","text":"{ \"success\" : true , \"data\" : { \"results\" : [ { \"title\" : \"North American Opossum Behavior Patterns\" , \"snippet\" : \"The opossum is known for its defensive behavior of 'playing dead' when threatened...\" , \"source\" : \"Wildlife Journal\" , \"relevance_score\" : 0.92 , \"url\" : \"https://example.com/articles/opossum-behavior\" }, // Additional results... ], \"suggested_queries\" : [ \"opossum diet\" , \"opossum habitat\" ], \"knowledge_graph\" : { \"entity\" : \"Virginia opossum\" , \"attributes\" : { \"scientific_name\" : \"Didelphis virginiana\" , \"lifespan\" : \"2-4 years\" , \"diet\" : \"Omnivore\" } } }, \"meta\" : { \"processing_time\" : 156 , \"model_used\" : \"gemini\" , \"request_id\" : \"req_8f7h6g5j4k3l2j1k\" , \"cache_status\" : \"MISS\" } }","title":"Search Response"},{"location":"api/request-response/#image-analysis-response","text":"{ \"success\" : true , \"data\" : { \"objects_detected\" : [ { \"label\" : \"opossum\" , \"confidence\" : 0.97 , \"bounding_box\" : { \"x\" : 23 , \"y\" : 45 , \"width\" : 200 , \"height\" : 150 } }, { \"label\" : \"tree\" , \"confidence\" : 0.85 , \"bounding_box\" : { \"x\" : 300 , \"y\" : 10 , \"width\" : 100 , \"height\" : 400 } } ], \"scene_description\" : \"An opossum climbing on a tree at night\" , \"tags\" : [ \"wildlife\" , \"nocturnal\" , \"marsupial\" , \"forest\" ] }, \"meta\" : { \"processing_time\" : 345 , \"model_used\" : \"gemini-vision\" , \"request_id\" : \"req_9g8h7j6k5l4k3j2h\" , \"image_dimensions\" : { \"width\" : 1024 , \"height\" : 768 } } }","title":"Image Analysis Response"},{"location":"api/request-response/#error-response-example","text":"{ \"success\" : false , \"error\" : { \"code\" : \"invalid_parameter\" , \"message\" : \"Invalid value for parameter 'model'\" , \"details\" : [ { \"field\" : \"model\" , \"message\" : \"Model 'gpt-5' is not available. Available models: gemini, ollama, local\" } ] }, \"meta\" : { \"request_id\" : \"req_5d6f7g8h9j0k1l2m\" , \"processing_time\" : 12 } }","title":"Error Response Example"},{"location":"api/request-response/#versioning","text":"The API is versioned using URL path. The current version is v1 .","title":"Versioning"},{"location":"api/request-response/#cors","text":"The API supports Cross-Origin Resource Sharing (CORS) for browser applications: Header Value Access-Control-Allow-Origin * or configured domains Access-Control-Allow-Methods GET, POST, PUT, PATCH, DELETE, OPTIONS Access-Control-Allow-Headers Origin, Content-Type, Accept, Authorization, X-API-Key, X-Request-ID Access-Control-Max-Age 86400 (24 hours)","title":"CORS"},{"location":"api/request-response/#request-timeouts","text":"Operation Timeout Standard search 10 seconds Image analysis 30 seconds SVG generation 15 seconds Bulk operations 60 seconds Requests exceeding these timeouts will receive a 504 Gateway Timeout response.","title":"Request Timeouts"},{"location":"api/routes/","text":"API Routes Documentation \u00b6 For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . To learn about real-time notifications, refer to Webhooks . For system status and component health, see Health Endpoints . Base URL \u00b6 All API endpoints will be available at: https://api.opossumsearch.com/v1 Authentication \u00b6 Most endpoints require authentication via one of these methods: API key (passed as X-API-Key header) Bearer token (passed as Authorization: Bearer <token> header) OAuth 2.0 (for user-authenticated requests) See Authentication for detailed instructions. Core Search Endpoints \u00b6 Search \u00b6 GET /search Performs a search using specified parameters. Query Parameters: q (required): The search query text model (optional): Specific model to use (\"gemini\", \"ollama\", etc.) mode (optional): Search mode (\"accurate\", \"creative\", \"balanced\") limit (optional): Maximum number of results (default: 10) include_images (optional): Whether to allow image results (default: false) Example Request: GET /search?q=how%20do%20opossums%20sleep&model=gemini&mode=accurate&limit=5 Multi-Search \u00b6 POST /multi-search Performs multiple searches in a single request. Request Body: { \"searches\" : [ { \"q\" : \"opossum diet\" , \"model\" : \"gemini\" }, { \"q\" : \"opossum habitat\" , \"model\" : \"ollama\" } ], \"common\" : { \"mode\" : \"balanced\" , \"limit\" : 3 } } Image Analysis \u00b6 POST /analyze-image Analyzes the content of an image. Request Body: Multipart form with image field containing the image file Additional parameters as form fields Example Request: POST /analyze-image Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\"image\"; filename=\"opossum.jpg\" Content-Type: image/jpeg (binary image data) ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\"detail_level\" high ------WebKitFormBoundary7MA4YWxkTrZu0gW-- Conversation Management \u00b6 Start Conversation \u00b6 POST /conversation Starts a new conversation session. Request Body: { \"initial_message\" : \"Tell me about opossums\" , \"settings\" : { \"model\" : \"gemini\" , \"mode\" : \"creative\" } } Continue Conversation \u00b6 POST /conversation/{conversation_id}/message Adds a message to an existing conversation. Request Body: { \"message\" : \"How long do they live?\" , \"context\" : { \"location\" : \"North America\" } } List Conversations \u00b6 GET /conversations Lists all conversations for the authenticated user. Query Parameters: limit (optional): Maximum number of conversations (default: 20) offset (optional): Pagination offset (default: 0) status (optional): Filter by status (\"active\", \"archived\") SVG Generation \u00b6 Generate Visualization \u00b6 POST /generate-svg Generates an SVG visualization based on provided data. Request Body: { \"visualization_type\" : \"data_chart\" , \"data\" : [ { \"label\" : \"Jan\" , \"value\" : 10 }, { \"label\" : \"Feb\" , \"value\" : 15 }, { \"label\" : \"Mar\" , \"value\" : 8 } ], \"options\" : { \"width\" : 800 , \"height\" : 400 , \"colors\" : [ \"#1f77b4\" , \"#ff7f0e\" , \"#2ca02c\" ] } } Get SVG Template \u00b6 GET /svg-templates/{template_id} Retrieves a specific SVG template. System Configuration \u00b6 Get Configuration \u00b6 GET /config Retrieves current system configuration (admin only). Update Configuration \u00b6 PATCH /config Updates system configuration parameters (admin only). Request Body: { \"cache\" : { \"ttl\" : 3600 , \"max_size\" : \"2GB\" }, \"models\" : { \"default\" : \"gemini\" } } Health and Status \u00b6 System Status \u00b6 GET /status Returns overall system status and health. Component Health \u00b6 GET /health/{component} Returns health information for a specific component. Valid components: api , models , cache , database , all Model Management \u00b6 List Available Models \u00b6 GET /models Lists all available AI models with capabilities. Model Details \u00b6 GET /models/{model_id} Returns detailed information about a specific model. Model Performance \u00b6 GET /models/{model_id}/performance Returns performance metrics for a specific model.","title":"Routes"},{"location":"api/routes/#api-routes-documentation","text":"For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . To learn about real-time notifications, refer to Webhooks . For system status and component health, see Health Endpoints .","title":"API Routes Documentation"},{"location":"api/routes/#base-url","text":"All API endpoints will be available at: https://api.opossumsearch.com/v1","title":"Base URL"},{"location":"api/routes/#authentication","text":"Most endpoints require authentication via one of these methods: API key (passed as X-API-Key header) Bearer token (passed as Authorization: Bearer <token> header) OAuth 2.0 (for user-authenticated requests) See Authentication for detailed instructions.","title":"Authentication"},{"location":"api/routes/#core-search-endpoints","text":"","title":"Core Search Endpoints"},{"location":"api/routes/#search","text":"GET /search Performs a search using specified parameters. Query Parameters: q (required): The search query text model (optional): Specific model to use (\"gemini\", \"ollama\", etc.) mode (optional): Search mode (\"accurate\", \"creative\", \"balanced\") limit (optional): Maximum number of results (default: 10) include_images (optional): Whether to allow image results (default: false) Example Request: GET /search?q=how%20do%20opossums%20sleep&model=gemini&mode=accurate&limit=5","title":"Search"},{"location":"api/routes/#multi-search","text":"POST /multi-search Performs multiple searches in a single request. Request Body: { \"searches\" : [ { \"q\" : \"opossum diet\" , \"model\" : \"gemini\" }, { \"q\" : \"opossum habitat\" , \"model\" : \"ollama\" } ], \"common\" : { \"mode\" : \"balanced\" , \"limit\" : 3 } }","title":"Multi-Search"},{"location":"api/routes/#image-analysis","text":"POST /analyze-image Analyzes the content of an image. Request Body: Multipart form with image field containing the image file Additional parameters as form fields Example Request: POST /analyze-image Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\"image\"; filename=\"opossum.jpg\" Content-Type: image/jpeg (binary image data) ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\"detail_level\" high ------WebKitFormBoundary7MA4YWxkTrZu0gW--","title":"Image Analysis"},{"location":"api/routes/#conversation-management","text":"","title":"Conversation Management"},{"location":"api/routes/#start-conversation","text":"POST /conversation Starts a new conversation session. Request Body: { \"initial_message\" : \"Tell me about opossums\" , \"settings\" : { \"model\" : \"gemini\" , \"mode\" : \"creative\" } }","title":"Start Conversation"},{"location":"api/routes/#continue-conversation","text":"POST /conversation/{conversation_id}/message Adds a message to an existing conversation. Request Body: { \"message\" : \"How long do they live?\" , \"context\" : { \"location\" : \"North America\" } }","title":"Continue Conversation"},{"location":"api/routes/#list-conversations","text":"GET /conversations Lists all conversations for the authenticated user. Query Parameters: limit (optional): Maximum number of conversations (default: 20) offset (optional): Pagination offset (default: 0) status (optional): Filter by status (\"active\", \"archived\")","title":"List Conversations"},{"location":"api/routes/#svg-generation","text":"","title":"SVG Generation"},{"location":"api/routes/#generate-visualization","text":"POST /generate-svg Generates an SVG visualization based on provided data. Request Body: { \"visualization_type\" : \"data_chart\" , \"data\" : [ { \"label\" : \"Jan\" , \"value\" : 10 }, { \"label\" : \"Feb\" , \"value\" : 15 }, { \"label\" : \"Mar\" , \"value\" : 8 } ], \"options\" : { \"width\" : 800 , \"height\" : 400 , \"colors\" : [ \"#1f77b4\" , \"#ff7f0e\" , \"#2ca02c\" ] } }","title":"Generate Visualization"},{"location":"api/routes/#get-svg-template","text":"GET /svg-templates/{template_id} Retrieves a specific SVG template.","title":"Get SVG Template"},{"location":"api/routes/#system-configuration","text":"","title":"System Configuration"},{"location":"api/routes/#get-configuration","text":"GET /config Retrieves current system configuration (admin only).","title":"Get Configuration"},{"location":"api/routes/#update-configuration","text":"PATCH /config Updates system configuration parameters (admin only). Request Body: { \"cache\" : { \"ttl\" : 3600 , \"max_size\" : \"2GB\" }, \"models\" : { \"default\" : \"gemini\" } }","title":"Update Configuration"},{"location":"api/routes/#health-and-status","text":"","title":"Health and Status"},{"location":"api/routes/#system-status","text":"GET /status Returns overall system status and health.","title":"System Status"},{"location":"api/routes/#component-health","text":"GET /health/{component} Returns health information for a specific component. Valid components: api , models , cache , database , all","title":"Component Health"},{"location":"api/routes/#model-management","text":"","title":"Model Management"},{"location":"api/routes/#list-available-models","text":"GET /models Lists all available AI models with capabilities.","title":"List Available Models"},{"location":"api/routes/#model-details","text":"GET /models/{model_id} Returns detailed information about a specific model.","title":"Model Details"},{"location":"api/routes/#model-performance","text":"GET /models/{model_id}/performance Returns performance metrics for a specific model.","title":"Model Performance"},{"location":"api/webhooks/","text":"Webhooks \u00b6 Overview \u00b6 Opossum Search provides webhooks to notify your application about events as they happen. This document covers webhook configuration, event types, payload format, and best practices for processing webhook events. For a list of available API routes, see API Routes . For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . For system status and component health, see Health Endpoints . What are Webhooks? \u00b6 Webhooks are HTTP callbacks that allow Opossum Search to send real-time notifications to your application when specific events occur. Rather than polling the API for updates, your application can receive automatic notifications. Webhook Events \u00b6 Opossum Search supports the following webhook event types: Event Type Description search.completed A search operation has completed analysis.completed An image or content analysis has completed conversation.updated A conversation has been updated with new messages model.status_changed An AI model's status has changed (online/offline) system.status_changed The system status has changed job.completed A long-running job has completed error.threshold_exceeded Error rate has exceeded configured threshold cache.invalidated Cache for specified resources has been invalidated quota.threshold_reached Usage quota has reached a configured threshold user.action_required User action is required (e.g., payment, verification) Setting Up Webhooks \u00b6 Dashboard Configuration \u00b6 Navigate to the Opossum Search Developer Dashboard Go to Settings > Webhooks Click Add Endpoint Configure your webhook: Endpoint URL: Your server's URL to receive webhook events Events: Select which events to subscribe to Secret: Generate a secret to verify webhook payloads Description: Optional description for your reference API Configuration \u00b6 You can also configure webhooks programmatically: POST /webhooks Request body: { \"url\" : \"https://example.com/webhooks/opossum\" , \"events\" : [ \"search.completed\" , \"model.status_changed\" ], \"description\" : \"Production webhook for search notifications\" , \"active\" : true , \"version\" : \"v1\" } Webhook Payload Format \u00b6 All webhook payloads follow this standard format: { \"id\" : \"evt_8f7h6g5j4k3l2j1k\" , \"created\" : \"2023-03-15T14:32:45Z\" , \"type\" : \"search.completed\" , \"data\" : { // Event-specific data }, \"account\" : \"acc_1234567890\" , \"api_version\" : \"v1\" } Event-Specific Payloads \u00b6 search.completed \u00b6 { \"id\" : \"evt_8f7h6g5j4k3l2j1k\" , \"created\" : \"2023-03-15T14:32:45Z\" , \"type\" : \"search.completed\" , \"data\" : { \"search_id\" : \"srch_1234567890\" , \"query\" : \"opossum behavior\" , \"status\" : \"completed\" , \"duration_ms\" : 432 , \"result_count\" : 15 , \"model_used\" : \"gemini\" , \"cache_hit\" : false }, \"account\" : \"acc_1234567890\" , \"api_version\" : \"v1\" } model.status_changed \u00b6 { \"id\" : \"evt_8f7h6g5j4k3l2j1k\" , \"created\" : \"2023-03-15T14:32:45Z\" , \"type\" : \"model.status_changed\" , \"data\" : { \"model\" : \"gemini\" , \"previous_status\" : \"online\" , \"current_status\" : \"degraded\" , \"reason\" : \"increased_latency\" , \"estimated_recovery\" : \"2023-03-15T15:00:00Z\" }, \"account\" : \"acc_1234567890\" , \"api_version\" : \"v1\" } Webhook Signature Verification \u00b6 For security, all webhooks include a signature header that you should verify: Opossum Search signs the payload using HMAC-SHA256 with your webhook secret The signature is included in the X-Opossum-Signature header Verify this signature to ensure the webhook is authentic Example Verification (Node.js) \u00b6 const crypto = require ( 'crypto' ); function verifyWebhookSignature ( payload , header , secret ) { const signature = crypto . createHmac ( 'sha256' , secret ) . update ( payload ) . digest ( 'hex' ); return crypto . timingSafeEqual ( Buffer . from ( signature ), Buffer . from ( header ) ); } // In your webhook handler app . post ( '/webhooks/opossum' , ( req , res ) => { const payload = JSON . stringify ( req . body ); const signature = req . headers [ 'x-opossum-signature' ]; const webhookSecret = process . env . OPOSSUM_WEBHOOK_SECRET ; if ( ! verifyWebhookSignature ( payload , signature , webhookSecret )) { return res . status ( 401 ). send ( 'Invalid signature' ); } // Process the webhook const event = req . body ; // Handle different event types switch ( event . type ) { case 'search.completed' : handleSearchCompleted ( event . data ); break ; // Handle other event types } res . status ( 200 ). send ( 'Webhook received' ); }); Webhook Best Practices \u00b6 Security \u00b6 Always verify signatures : Validate the X-Opossum-Signature header Use HTTPS endpoints : Ensure webhook URLs use HTTPS Rotate webhook secrets : Periodically update your webhook secrets Implement IP filtering : Optionally restrict to Opossum Search IP ranges Reliability \u00b6 Return 2xx quickly : Respond with a 2xx status code as soon as possible Process asynchronously : Handle webhook logic outside the request-response cycle Implement idempotency : Handle duplicate webhook deliveries gracefully Store raw payloads : Save raw webhook data before processing Monitoring \u00b6 Log webhook receipts : Keep records of all received webhooks Track processing success : Monitor webhook processing success rates Set up alerts : Create alerts for webhook delivery failures Check webhook dashboard : Use the Opossum Search dashboard to view delivery stats Webhook Delivery \u00b6 Retry Policy \u00b6 If your endpoint returns a non-2xx response, Opossum Search will retry with this schedule: Initial retry: 5 seconds Second retry: 30 seconds Third retry: 5 minutes Fourth retry: 30 minutes Fifth retry: 2 hours Final retry: 5 hours After all retry attempts fail, the webhook will be marked as failed in the dashboard. Testing Webhooks \u00b6 The dashboard provides a \"Test Webhook\" feature to send sample events to your endpoint: Go to Settings > Webhooks Select a configured webhook Click Test Webhook Choose an event type Send the test event You can also use the API to test webhooks: POST /webhooks/{webhook_id}/test { \"event_type\" : \"search.completed\" } Webhook Logs \u00b6 The dashboard provides logs of all webhook delivery attempts: Event ID Timestamp Destination URL Response status Response time Retry count Logs are retained for 30 days. Managing Webhooks \u00b6 Listing Webhooks \u00b6 GET /webhooks Updating a Webhook \u00b6 PATCH /webhooks/{webhook_id} { \"events\" : [ \"search.completed\" , \"analysis.completed\" ], \"active\" : false } Deleting a Webhook \u00b6 DELETE /webhooks/{webhook_id} Webhook Quotas and Limits \u00b6 Plan Max Webhook Endpoints Max Events per Minute Event Retention Basic 5 60 7 days Professional 20 300 30 days Enterprise 100 1,200 90 days Troubleshooting \u00b6 Common webhook issues and solutions: Webhook Not Receiving Events \u00b6 Check the webhook's status in the dashboard Verify the endpoint URL is correct and accessible Ensure your server accepts POST requests Check that you're subscribed to the expected events Failed Signature Verification \u00b6 Confirm you're using the correct webhook secret Ensure you're validating against the raw request body Check for any body parsing middleware that might modify the payload Events Missing or Delayed \u00b6 Check webhook logs for delivery failures Verify your endpoint responds with 2xx status codes Check for rate limiting on your server Ensure your server can handle concurrent webhook requests","title":"Webhook Integration"},{"location":"api/webhooks/#webhooks","text":"","title":"Webhooks"},{"location":"api/webhooks/#overview","text":"Opossum Search provides webhooks to notify your application about events as they happen. This document covers webhook configuration, event types, payload format, and best practices for processing webhook events. For a list of available API routes, see API Routes . For detailed information about request and response formats, see Request/Response Documentation . For error code details, see Error Codes . Information on API usage limits can be found in Rate Limits . For system status and component health, see Health Endpoints .","title":"Overview"},{"location":"api/webhooks/#what-are-webhooks","text":"Webhooks are HTTP callbacks that allow Opossum Search to send real-time notifications to your application when specific events occur. Rather than polling the API for updates, your application can receive automatic notifications.","title":"What are Webhooks?"},{"location":"api/webhooks/#webhook-events","text":"Opossum Search supports the following webhook event types: Event Type Description search.completed A search operation has completed analysis.completed An image or content analysis has completed conversation.updated A conversation has been updated with new messages model.status_changed An AI model's status has changed (online/offline) system.status_changed The system status has changed job.completed A long-running job has completed error.threshold_exceeded Error rate has exceeded configured threshold cache.invalidated Cache for specified resources has been invalidated quota.threshold_reached Usage quota has reached a configured threshold user.action_required User action is required (e.g., payment, verification)","title":"Webhook Events"},{"location":"api/webhooks/#setting-up-webhooks","text":"","title":"Setting Up Webhooks"},{"location":"api/webhooks/#dashboard-configuration","text":"Navigate to the Opossum Search Developer Dashboard Go to Settings > Webhooks Click Add Endpoint Configure your webhook: Endpoint URL: Your server's URL to receive webhook events Events: Select which events to subscribe to Secret: Generate a secret to verify webhook payloads Description: Optional description for your reference","title":"Dashboard Configuration"},{"location":"api/webhooks/#api-configuration","text":"You can also configure webhooks programmatically: POST /webhooks Request body: { \"url\" : \"https://example.com/webhooks/opossum\" , \"events\" : [ \"search.completed\" , \"model.status_changed\" ], \"description\" : \"Production webhook for search notifications\" , \"active\" : true , \"version\" : \"v1\" }","title":"API Configuration"},{"location":"api/webhooks/#webhook-payload-format","text":"All webhook payloads follow this standard format: { \"id\" : \"evt_8f7h6g5j4k3l2j1k\" , \"created\" : \"2023-03-15T14:32:45Z\" , \"type\" : \"search.completed\" , \"data\" : { // Event-specific data }, \"account\" : \"acc_1234567890\" , \"api_version\" : \"v1\" }","title":"Webhook Payload Format"},{"location":"api/webhooks/#event-specific-payloads","text":"","title":"Event-Specific Payloads"},{"location":"api/webhooks/#searchcompleted","text":"{ \"id\" : \"evt_8f7h6g5j4k3l2j1k\" , \"created\" : \"2023-03-15T14:32:45Z\" , \"type\" : \"search.completed\" , \"data\" : { \"search_id\" : \"srch_1234567890\" , \"query\" : \"opossum behavior\" , \"status\" : \"completed\" , \"duration_ms\" : 432 , \"result_count\" : 15 , \"model_used\" : \"gemini\" , \"cache_hit\" : false }, \"account\" : \"acc_1234567890\" , \"api_version\" : \"v1\" }","title":"search.completed"},{"location":"api/webhooks/#modelstatus_changed","text":"{ \"id\" : \"evt_8f7h6g5j4k3l2j1k\" , \"created\" : \"2023-03-15T14:32:45Z\" , \"type\" : \"model.status_changed\" , \"data\" : { \"model\" : \"gemini\" , \"previous_status\" : \"online\" , \"current_status\" : \"degraded\" , \"reason\" : \"increased_latency\" , \"estimated_recovery\" : \"2023-03-15T15:00:00Z\" }, \"account\" : \"acc_1234567890\" , \"api_version\" : \"v1\" }","title":"model.status_changed"},{"location":"api/webhooks/#webhook-signature-verification","text":"For security, all webhooks include a signature header that you should verify: Opossum Search signs the payload using HMAC-SHA256 with your webhook secret The signature is included in the X-Opossum-Signature header Verify this signature to ensure the webhook is authentic","title":"Webhook Signature Verification"},{"location":"api/webhooks/#example-verification-nodejs","text":"const crypto = require ( 'crypto' ); function verifyWebhookSignature ( payload , header , secret ) { const signature = crypto . createHmac ( 'sha256' , secret ) . update ( payload ) . digest ( 'hex' ); return crypto . timingSafeEqual ( Buffer . from ( signature ), Buffer . from ( header ) ); } // In your webhook handler app . post ( '/webhooks/opossum' , ( req , res ) => { const payload = JSON . stringify ( req . body ); const signature = req . headers [ 'x-opossum-signature' ]; const webhookSecret = process . env . OPOSSUM_WEBHOOK_SECRET ; if ( ! verifyWebhookSignature ( payload , signature , webhookSecret )) { return res . status ( 401 ). send ( 'Invalid signature' ); } // Process the webhook const event = req . body ; // Handle different event types switch ( event . type ) { case 'search.completed' : handleSearchCompleted ( event . data ); break ; // Handle other event types } res . status ( 200 ). send ( 'Webhook received' ); });","title":"Example Verification (Node.js)"},{"location":"api/webhooks/#webhook-best-practices","text":"","title":"Webhook Best Practices"},{"location":"api/webhooks/#security","text":"Always verify signatures : Validate the X-Opossum-Signature header Use HTTPS endpoints : Ensure webhook URLs use HTTPS Rotate webhook secrets : Periodically update your webhook secrets Implement IP filtering : Optionally restrict to Opossum Search IP ranges","title":"Security"},{"location":"api/webhooks/#reliability","text":"Return 2xx quickly : Respond with a 2xx status code as soon as possible Process asynchronously : Handle webhook logic outside the request-response cycle Implement idempotency : Handle duplicate webhook deliveries gracefully Store raw payloads : Save raw webhook data before processing","title":"Reliability"},{"location":"api/webhooks/#monitoring","text":"Log webhook receipts : Keep records of all received webhooks Track processing success : Monitor webhook processing success rates Set up alerts : Create alerts for webhook delivery failures Check webhook dashboard : Use the Opossum Search dashboard to view delivery stats","title":"Monitoring"},{"location":"api/webhooks/#webhook-delivery","text":"","title":"Webhook Delivery"},{"location":"api/webhooks/#retry-policy","text":"If your endpoint returns a non-2xx response, Opossum Search will retry with this schedule: Initial retry: 5 seconds Second retry: 30 seconds Third retry: 5 minutes Fourth retry: 30 minutes Fifth retry: 2 hours Final retry: 5 hours After all retry attempts fail, the webhook will be marked as failed in the dashboard.","title":"Retry Policy"},{"location":"api/webhooks/#testing-webhooks","text":"The dashboard provides a \"Test Webhook\" feature to send sample events to your endpoint: Go to Settings > Webhooks Select a configured webhook Click Test Webhook Choose an event type Send the test event You can also use the API to test webhooks: POST /webhooks/{webhook_id}/test { \"event_type\" : \"search.completed\" }","title":"Testing Webhooks"},{"location":"api/webhooks/#webhook-logs","text":"The dashboard provides logs of all webhook delivery attempts: Event ID Timestamp Destination URL Response status Response time Retry count Logs are retained for 30 days.","title":"Webhook Logs"},{"location":"api/webhooks/#managing-webhooks","text":"","title":"Managing Webhooks"},{"location":"api/webhooks/#listing-webhooks","text":"GET /webhooks","title":"Listing Webhooks"},{"location":"api/webhooks/#updating-a-webhook","text":"PATCH /webhooks/{webhook_id} { \"events\" : [ \"search.completed\" , \"analysis.completed\" ], \"active\" : false }","title":"Updating a Webhook"},{"location":"api/webhooks/#deleting-a-webhook","text":"DELETE /webhooks/{webhook_id}","title":"Deleting a Webhook"},{"location":"api/webhooks/#webhook-quotas-and-limits","text":"Plan Max Webhook Endpoints Max Events per Minute Event Retention Basic 5 60 7 days Professional 20 300 30 days Enterprise 100 1,200 90 days","title":"Webhook Quotas and Limits"},{"location":"api/webhooks/#troubleshooting","text":"Common webhook issues and solutions:","title":"Troubleshooting"},{"location":"api/webhooks/#webhook-not-receiving-events","text":"Check the webhook's status in the dashboard Verify the endpoint URL is correct and accessible Ensure your server accepts POST requests Check that you're subscribed to the expected events","title":"Webhook Not Receiving Events"},{"location":"api/webhooks/#failed-signature-verification","text":"Confirm you're using the correct webhook secret Ensure you're validating against the raw request body Check for any body parsing middleware that might modify the payload","title":"Failed Signature Verification"},{"location":"api/webhooks/#events-missing-or-delayed","text":"Check webhook logs for delivery failures Verify your endpoint responds with 2xx status codes Check for rate limiting on your server Ensure your server can handle concurrent webhook requests","title":"Events Missing or Delayed"},{"location":"conversation/conversation-flow/","text":"Conversation Flow \u00b6 The Opossum chatbot implements a sophisticated conversation management system that coordinates multiple components to handle user interactions. Architecture \u00b6 ConversationFactory \u00b6 The system uses a factory pattern to create and manage conversation components: class ConversationFactory : def __init__ ( self ): self . conversation_manager = ConversationManager () self . response_generator = ResponseGenerator () State Management \u00b6 The ConversationState class maintains: Session tracking Message history Topic transitions Context information User preferences class ConversationState : def __init__ ( self , session_id : str ): self . session_id = session_id self . current_stage = \"greeting\" self . last_topic = None self . topic_history = [] self . message_history = [] self . last_interaction = datetime . now () self . context = {} self . user_preferences = {} Conversation Lifecycle \u00b6 Initialization Create session-specific conversation state Initialize sentiment tracker Set up response generator Message Processing Check cache for existing responses Update conversation state Track sentiment and engagement Generate contextual responses Cleanup Auto-expire inactive sessions Clean up resources Save relevant analytics Component Integration \u00b6 graph TD A[User Request] --> B[ConversationFactory] B --> C[ConversationState] B --> D[SentimentTracker] B --> E[ResponseGenerator] C --> F[Topic Detection] D --> F F --> E E --> G[Model Backend] G --> H[Response] Session Management \u00b6 Sessions timeout after 30 minutes of inactivity Automatic cleanup runs every 5 minutes Resources are freed immediately on explicit end def get_conversation ( self , session_id : str ) -> ConversationState : \"\"\"Get or create a conversation state for a session.\"\"\" self . _cleanup_expired () if session_id not in self . conversations : self . conversations [ session_id ] = ConversationState ( session_id ) return self . conversations [ session_id ] Context Window \u00b6 Conversations maintain a context window of recent messages: def get_context_window ( self , window_size : int = 5 ) -> List [ Dict [ str , Any ]]: \"\"\"Get recent conversation context.\"\"\" return self . message_history [ - window_size :] if self . message_history else [] Error Handling \u00b6 The system provides several layers of resilience: Cache Layer Caches successful responses Reduces duplicate processing Improves response time Fallback Chain Model backend failures Topic detection issues General error handling State Recovery Session restoration Context preservation Graceful degradation Example Flow \u00b6 User sends message: { \"message\" : \"Tell me about opossums\" , \"session_id\" : \"user123\" , \"has_image\" : false } System processes: Creates/retrieves conversation state Analyzes message sentiment Determines topic and stage Generates contextual response Updates conversation history Response includes: { \"response\" : \"...\" , \"next_stage\" : \"general_info\" , \"sentiment\" : { \"polarity\" : 0.2 , \"subjectivity\" : 0.5 }, \"needs_reengagement\" : false } Configuration \u00b6 Key configuration parameters: CACHE_TTL = 600 # Response cache lifetime (10 minutes) SESSION_TIMEOUT = 1800 # Session timeout (30 minutes) CLEANUP_INTERVAL = 300 # Cleanup interval (5 minutes)","title":"Conversation Flow"},{"location":"conversation/conversation-flow/#conversation-flow","text":"The Opossum chatbot implements a sophisticated conversation management system that coordinates multiple components to handle user interactions.","title":"Conversation Flow"},{"location":"conversation/conversation-flow/#architecture","text":"","title":"Architecture"},{"location":"conversation/conversation-flow/#conversationfactory","text":"The system uses a factory pattern to create and manage conversation components: class ConversationFactory : def __init__ ( self ): self . conversation_manager = ConversationManager () self . response_generator = ResponseGenerator ()","title":"ConversationFactory"},{"location":"conversation/conversation-flow/#state-management","text":"The ConversationState class maintains: Session tracking Message history Topic transitions Context information User preferences class ConversationState : def __init__ ( self , session_id : str ): self . session_id = session_id self . current_stage = \"greeting\" self . last_topic = None self . topic_history = [] self . message_history = [] self . last_interaction = datetime . now () self . context = {} self . user_preferences = {}","title":"State Management"},{"location":"conversation/conversation-flow/#conversation-lifecycle","text":"Initialization Create session-specific conversation state Initialize sentiment tracker Set up response generator Message Processing Check cache for existing responses Update conversation state Track sentiment and engagement Generate contextual responses Cleanup Auto-expire inactive sessions Clean up resources Save relevant analytics","title":"Conversation Lifecycle"},{"location":"conversation/conversation-flow/#component-integration","text":"graph TD A[User Request] --> B[ConversationFactory] B --> C[ConversationState] B --> D[SentimentTracker] B --> E[ResponseGenerator] C --> F[Topic Detection] D --> F F --> E E --> G[Model Backend] G --> H[Response]","title":"Component Integration"},{"location":"conversation/conversation-flow/#session-management","text":"Sessions timeout after 30 minutes of inactivity Automatic cleanup runs every 5 minutes Resources are freed immediately on explicit end def get_conversation ( self , session_id : str ) -> ConversationState : \"\"\"Get or create a conversation state for a session.\"\"\" self . _cleanup_expired () if session_id not in self . conversations : self . conversations [ session_id ] = ConversationState ( session_id ) return self . conversations [ session_id ]","title":"Session Management"},{"location":"conversation/conversation-flow/#context-window","text":"Conversations maintain a context window of recent messages: def get_context_window ( self , window_size : int = 5 ) -> List [ Dict [ str , Any ]]: \"\"\"Get recent conversation context.\"\"\" return self . message_history [ - window_size :] if self . message_history else []","title":"Context Window"},{"location":"conversation/conversation-flow/#error-handling","text":"The system provides several layers of resilience: Cache Layer Caches successful responses Reduces duplicate processing Improves response time Fallback Chain Model backend failures Topic detection issues General error handling State Recovery Session restoration Context preservation Graceful degradation","title":"Error Handling"},{"location":"conversation/conversation-flow/#example-flow","text":"User sends message: { \"message\" : \"Tell me about opossums\" , \"session_id\" : \"user123\" , \"has_image\" : false } System processes: Creates/retrieves conversation state Analyzes message sentiment Determines topic and stage Generates contextual response Updates conversation history Response includes: { \"response\" : \"...\" , \"next_stage\" : \"general_info\" , \"sentiment\" : { \"polarity\" : 0.2 , \"subjectivity\" : 0.5 }, \"needs_reengagement\" : false }","title":"Example Flow"},{"location":"conversation/conversation-flow/#configuration","text":"Key configuration parameters: CACHE_TTL = 600 # Response cache lifetime (10 minutes) SESSION_TIMEOUT = 1800 # Session timeout (30 minutes) CLEANUP_INTERVAL = 300 # Cleanup interval (5 minutes)","title":"Configuration"},{"location":"conversation/markov-generation/","text":"Markov Chain Text Generation \u00b6 The Markov chain text generation system is used specifically for creating ambient background text in the UI, combining opossum facts with technical terminology. This is not part of the actual conversation system but rather provides an engaging visual element. Implementation \u00b6 Text Sources \u00b6 The system uses two distinct text corpora: # Opossum-related text corpus OPOSSUM_TEXTS = [ \"\"\"The Virginia opossum is the only marsupial found north of Mexico...\"\"\" , \"\"\"Opossums have 50 teeth, more than any other North American land mammal...\"\"\" , # Additional opossum facts... ] # Technical/AI text corpus TECH_TEXTS = [ \"\"\"Natural language processing (NLP) is a field of artificial intelligence...\"\"\" , \"\"\"Machine learning models learn from data, identify patterns...\"\"\" , # Additional technical content... ] Generation Process \u00b6 Model Creation \u00b6 def _get_markov_models (): \"\"\"Initialize and cache Markov models.\"\"\" global _opossum_model , _tech_model , _combined_model if ( _opossum_model is None ): _opossum_model = markovify . Text ( \" \" . join ( OPOSSUM_TEXTS ), state_size = 2 ) _tech_model = markovify . Text ( \" \" . join ( TECH_TEXTS ), state_size = 2 ) _combined_model = markovify . combine ([ _opossum_model , _tech_model ], [ 1.5 , 1 ]) Text Generation Methods \u00b6 Pure Opossum Text Uses opossum-only Markov chain Generates nature-focused content Maintains factual accuracy Combined Tech-Opossum Merges both models Creates unique hybrid content Weights opossum content higher (1.5:1) Tech with Opossum Terms Injects opossum terminology into tech text Creates amusing technical-sounding content Maintains readability Emoji Integration \u00b6 The system incorporates emojis into the generated text: EMOJI_CATEGORIES = [ 'animal' , 'nature' , 'science' , 'computer' , 'office' , 'technical_symbol' ] Special Event: Opossum National Day \u00b6 On October 18th, the system switches to opossum-specific emojis: opossum_emojis = [ \"\ud83e\udd9d\" , # Closest to opossum \"\ud83c\udf19\" , # Nocturnal \"\ud83c\udf33\" , # Habitat \"\ud83c\udf43\" , # Nature \"\ud83d\udc3e\" , # Paw prints \"\ud83c\udf3f\" , # Forest \"\ud83e\udd8a\" , # Another marsupial-like emoji \"\ud83c\udf11\" , # Night time \"\ud83c\udf34\" , # Trees \"\ud83e\udea8\" # Habitat ] Technical Enhancements \u00b6 Line Generation Types \u00b6 Pure Markov Chain line = opossum_model . make_short_sentence ( 80 ) Combined Model line = combined_model . make_short_sentence ( 100 ) Technical Mashup term1 = random . choice ( opossum_terms ) term2 = random . choice ( tech_terms ) patterns = [ f \"The { term1 } exhibits properties similar to { term2 } \" , f \" { term2 } analysis of { term1 } reveals interesting patterns\" ] Visual Formatting \u00b6 The system adds technical-looking prefixes: prefixes = [ f \"[ANALYSIS- { random . randint ( 100 , 999 ) } ]\" , f \"<opossum-token- { random . randint ( 1000 , 9999 ) } >\" , f \"// TOKEN_ { random . randint ( 10000 , 99999 ) } :\" , f \"/* { random . choice ([ 'DEBUG' , 'INFO' , 'TRACE' ]) } : */\" , ] Usage Example \u00b6 # Generate background text gibberish_text = _generate_nlp_gibberish ( num_lines = 25 ) # Example output: # [ANALYSIS-247] The opossum's prehensile tail demonstrates neural network adaptability \ud83e\udd9d # <opossum-token-4891> Analyzing marsupial behavior through transformer architecture \ud83d\udd2c # /* DEBUG */ Virginia opossum embeddings show remarkable feature extraction \ud83e\uddea Configuration \u00b6 Line generation: 10-100 lines per request Emoji probability: 70% per line Technical prefix probability: 30% per line Model state size: 2 (for coherent sentence structure) Opossum:Tech weight ratio: 1.5:1","title":"Markov Generation"},{"location":"conversation/markov-generation/#markov-chain-text-generation","text":"The Markov chain text generation system is used specifically for creating ambient background text in the UI, combining opossum facts with technical terminology. This is not part of the actual conversation system but rather provides an engaging visual element.","title":"Markov Chain Text Generation"},{"location":"conversation/markov-generation/#implementation","text":"","title":"Implementation"},{"location":"conversation/markov-generation/#text-sources","text":"The system uses two distinct text corpora: # Opossum-related text corpus OPOSSUM_TEXTS = [ \"\"\"The Virginia opossum is the only marsupial found north of Mexico...\"\"\" , \"\"\"Opossums have 50 teeth, more than any other North American land mammal...\"\"\" , # Additional opossum facts... ] # Technical/AI text corpus TECH_TEXTS = [ \"\"\"Natural language processing (NLP) is a field of artificial intelligence...\"\"\" , \"\"\"Machine learning models learn from data, identify patterns...\"\"\" , # Additional technical content... ]","title":"Text Sources"},{"location":"conversation/markov-generation/#generation-process","text":"","title":"Generation Process"},{"location":"conversation/markov-generation/#model-creation","text":"def _get_markov_models (): \"\"\"Initialize and cache Markov models.\"\"\" global _opossum_model , _tech_model , _combined_model if ( _opossum_model is None ): _opossum_model = markovify . Text ( \" \" . join ( OPOSSUM_TEXTS ), state_size = 2 ) _tech_model = markovify . Text ( \" \" . join ( TECH_TEXTS ), state_size = 2 ) _combined_model = markovify . combine ([ _opossum_model , _tech_model ], [ 1.5 , 1 ])","title":"Model Creation"},{"location":"conversation/markov-generation/#text-generation-methods","text":"Pure Opossum Text Uses opossum-only Markov chain Generates nature-focused content Maintains factual accuracy Combined Tech-Opossum Merges both models Creates unique hybrid content Weights opossum content higher (1.5:1) Tech with Opossum Terms Injects opossum terminology into tech text Creates amusing technical-sounding content Maintains readability","title":"Text Generation Methods"},{"location":"conversation/markov-generation/#emoji-integration","text":"The system incorporates emojis into the generated text: EMOJI_CATEGORIES = [ 'animal' , 'nature' , 'science' , 'computer' , 'office' , 'technical_symbol' ]","title":"Emoji Integration"},{"location":"conversation/markov-generation/#special-event-opossum-national-day","text":"On October 18th, the system switches to opossum-specific emojis: opossum_emojis = [ \"\ud83e\udd9d\" , # Closest to opossum \"\ud83c\udf19\" , # Nocturnal \"\ud83c\udf33\" , # Habitat \"\ud83c\udf43\" , # Nature \"\ud83d\udc3e\" , # Paw prints \"\ud83c\udf3f\" , # Forest \"\ud83e\udd8a\" , # Another marsupial-like emoji \"\ud83c\udf11\" , # Night time \"\ud83c\udf34\" , # Trees \"\ud83e\udea8\" # Habitat ]","title":"Special Event: Opossum National Day"},{"location":"conversation/markov-generation/#technical-enhancements","text":"","title":"Technical Enhancements"},{"location":"conversation/markov-generation/#line-generation-types","text":"Pure Markov Chain line = opossum_model . make_short_sentence ( 80 ) Combined Model line = combined_model . make_short_sentence ( 100 ) Technical Mashup term1 = random . choice ( opossum_terms ) term2 = random . choice ( tech_terms ) patterns = [ f \"The { term1 } exhibits properties similar to { term2 } \" , f \" { term2 } analysis of { term1 } reveals interesting patterns\" ]","title":"Line Generation Types"},{"location":"conversation/markov-generation/#visual-formatting","text":"The system adds technical-looking prefixes: prefixes = [ f \"[ANALYSIS- { random . randint ( 100 , 999 ) } ]\" , f \"<opossum-token- { random . randint ( 1000 , 9999 ) } >\" , f \"// TOKEN_ { random . randint ( 10000 , 99999 ) } :\" , f \"/* { random . choice ([ 'DEBUG' , 'INFO' , 'TRACE' ]) } : */\" , ]","title":"Visual Formatting"},{"location":"conversation/markov-generation/#usage-example","text":"# Generate background text gibberish_text = _generate_nlp_gibberish ( num_lines = 25 ) # Example output: # [ANALYSIS-247] The opossum's prehensile tail demonstrates neural network adaptability \ud83e\udd9d # <opossum-token-4891> Analyzing marsupial behavior through transformer architecture \ud83d\udd2c # /* DEBUG */ Virginia opossum embeddings show remarkable feature extraction \ud83e\uddea","title":"Usage Example"},{"location":"conversation/markov-generation/#configuration","text":"Line generation: 10-100 lines per request Emoji probability: 70% per line Technical prefix probability: 30% per line Model state size: 2 (for coherent sentence structure) Opossum:Tech weight ratio: 1.5:1","title":"Configuration"},{"location":"conversation/nlp-components/","text":"NLP Components \u00b6 The Opossum system uses several Natural Language Processing (NLP) components for different aspects of conversation handling. Sentence Transformers \u00b6 Model Configuration \u00b6 Model: all-MiniLM-L6-v2 Purpose: Semantic similarity and topic detection Implementation: Sentence-transformers library Usage Areas \u00b6 Topic Detection Embedding generation for topic sentences Similarity scoring for topic matching Follow-up question detection Message Understanding User intent classification Context preservation Semantic similarity calculations Sentiment Analysis \u00b6 The system uses TextBlob for sentiment analysis: def analyze_message ( self , message : str , is_follow_up : bool = False ) -> Dict [ str , Any ]: \"\"\"Analyze message sentiment and update metrics.\"\"\" blob = TextBlob ( message ) sentiment_scores = { \"polarity\" : blob . sentiment . polarity , \"subjectivity\" : blob . sentiment . subjectivity } # Update sentiment history self . sentiment_history . append ( sentiment_scores [ \"polarity\" ]) # Calculate sentiment trend if len ( self . sentiment_history ) >= 2 : trend = np . polyfit ( range ( len ( self . sentiment_history )), list ( self . sentiment_history ), 1 )[ 0 ] self . engagement_metrics [ \"sentiment_trend\" ] = float ( trend ) # Update engagement metrics... return { \"sentiment\" : sentiment_scores , \"engagement\" : self . engagement_metrics } Metrics Tracked \u00b6 Message polarity (-1 to 1) Subjectivity (0 to 1) Engagement trends Response patterns Topic Classification \u00b6 Topic detection uses a combination of: Semantic Embeddings Pre-computed topic embeddings Real-time message embedding Cosine similarity matching Rule-based Detection Follow-up indicators Greeting patterns Closing phrases Topic Categories \u00b6 topic_sentences = { \"snake_resistance\" : \"Opossums are resistant to snake venom...\" , \"florida_opossums\" : \"Opossums in Florida...\" , \"diet_query\" : \"What opossums eat...\" , \"habitat_query\" : \"Where opossums live...\" , \"behavior_query\" : \"Opossum behavior...\" , } Performance Optimization \u00b6 Caching Strategy \u00b6 Topic embeddings pre-computed Sentiment scores cached Recent analyses retained Batch Processing \u00b6 Message history analysis Trend calculation Engagement scoring Integration Points \u00b6 graph TD A[Raw Message] --> B[Sentence Transformer] A --> C[Sentiment Analysis] B --> D[Topic Detection] C --> E[Engagement Tracking] D --> F[Response Generation] E --> F Error Handling \u00b6 Model Loading try : self . model = SentenceTransformer ( Config . SENTENCE_TRANSFORMER_MODEL ) except ImportError : logger . error ( \"Required NLP packages not available\" ) raise Embedding Generation try : message_embedding = self . model . encode ( user_message . lower ()) except Exception as e : logger . error ( f \"Embedding generation failed: { e } \" ) return \"general_info\" # Fallback topic Configuration \u00b6 Key NLP settings from Config: SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2' SIMILARITY_THRESHOLD = 0.35 CACHE_TTL = 600 # 10 minutes MAX_CONTEXT_WINDOW = 5 Engagement Detection \u00b6 The system identifies several engagement signals: Sentiment trend (declining vs. improving) Message length over time Follow-up question frequency Overall conversation mood Re-engagement Triggers \u00b6 def _needs_reengagement ( self ) -> bool : \"\"\"Determine if conversation needs reengagement strategies.\"\"\" if not self . sentiment_history : return False conditions = [ self . engagement_metrics [ \"sentiment_trend\" ] < - 0.2 , self . engagement_metrics [ \"avg_response_length\" ] < 10 , len ( self . sentiment_history ) >= 3 and all ( s < 0 for s in list ( self . sentiment_history )[ - 3 :]) ] return any ( conditions )","title":"NLP Components"},{"location":"conversation/nlp-components/#nlp-components","text":"The Opossum system uses several Natural Language Processing (NLP) components for different aspects of conversation handling.","title":"NLP Components"},{"location":"conversation/nlp-components/#sentence-transformers","text":"","title":"Sentence Transformers"},{"location":"conversation/nlp-components/#model-configuration","text":"Model: all-MiniLM-L6-v2 Purpose: Semantic similarity and topic detection Implementation: Sentence-transformers library","title":"Model Configuration"},{"location":"conversation/nlp-components/#usage-areas","text":"Topic Detection Embedding generation for topic sentences Similarity scoring for topic matching Follow-up question detection Message Understanding User intent classification Context preservation Semantic similarity calculations","title":"Usage Areas"},{"location":"conversation/nlp-components/#sentiment-analysis","text":"The system uses TextBlob for sentiment analysis: def analyze_message ( self , message : str , is_follow_up : bool = False ) -> Dict [ str , Any ]: \"\"\"Analyze message sentiment and update metrics.\"\"\" blob = TextBlob ( message ) sentiment_scores = { \"polarity\" : blob . sentiment . polarity , \"subjectivity\" : blob . sentiment . subjectivity } # Update sentiment history self . sentiment_history . append ( sentiment_scores [ \"polarity\" ]) # Calculate sentiment trend if len ( self . sentiment_history ) >= 2 : trend = np . polyfit ( range ( len ( self . sentiment_history )), list ( self . sentiment_history ), 1 )[ 0 ] self . engagement_metrics [ \"sentiment_trend\" ] = float ( trend ) # Update engagement metrics... return { \"sentiment\" : sentiment_scores , \"engagement\" : self . engagement_metrics }","title":"Sentiment Analysis"},{"location":"conversation/nlp-components/#metrics-tracked","text":"Message polarity (-1 to 1) Subjectivity (0 to 1) Engagement trends Response patterns","title":"Metrics Tracked"},{"location":"conversation/nlp-components/#topic-classification","text":"Topic detection uses a combination of: Semantic Embeddings Pre-computed topic embeddings Real-time message embedding Cosine similarity matching Rule-based Detection Follow-up indicators Greeting patterns Closing phrases","title":"Topic Classification"},{"location":"conversation/nlp-components/#topic-categories","text":"topic_sentences = { \"snake_resistance\" : \"Opossums are resistant to snake venom...\" , \"florida_opossums\" : \"Opossums in Florida...\" , \"diet_query\" : \"What opossums eat...\" , \"habitat_query\" : \"Where opossums live...\" , \"behavior_query\" : \"Opossum behavior...\" , }","title":"Topic Categories"},{"location":"conversation/nlp-components/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"conversation/nlp-components/#caching-strategy","text":"Topic embeddings pre-computed Sentiment scores cached Recent analyses retained","title":"Caching Strategy"},{"location":"conversation/nlp-components/#batch-processing","text":"Message history analysis Trend calculation Engagement scoring","title":"Batch Processing"},{"location":"conversation/nlp-components/#integration-points","text":"graph TD A[Raw Message] --> B[Sentence Transformer] A --> C[Sentiment Analysis] B --> D[Topic Detection] C --> E[Engagement Tracking] D --> F[Response Generation] E --> F","title":"Integration Points"},{"location":"conversation/nlp-components/#error-handling","text":"Model Loading try : self . model = SentenceTransformer ( Config . SENTENCE_TRANSFORMER_MODEL ) except ImportError : logger . error ( \"Required NLP packages not available\" ) raise Embedding Generation try : message_embedding = self . model . encode ( user_message . lower ()) except Exception as e : logger . error ( f \"Embedding generation failed: { e } \" ) return \"general_info\" # Fallback topic","title":"Error Handling"},{"location":"conversation/nlp-components/#configuration","text":"Key NLP settings from Config: SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2' SIMILARITY_THRESHOLD = 0.35 CACHE_TTL = 600 # 10 minutes MAX_CONTEXT_WINDOW = 5","title":"Configuration"},{"location":"conversation/nlp-components/#engagement-detection","text":"The system identifies several engagement signals: Sentiment trend (declining vs. improving) Message length over time Follow-up question frequency Overall conversation mood","title":"Engagement Detection"},{"location":"conversation/nlp-components/#re-engagement-triggers","text":"def _needs_reengagement ( self ) -> bool : \"\"\"Determine if conversation needs reengagement strategies.\"\"\" if not self . sentiment_history : return False conditions = [ self . engagement_metrics [ \"sentiment_trend\" ] < - 0.2 , self . engagement_metrics [ \"avg_response_length\" ] < 10 , len ( self . sentiment_history ) >= 3 and all ( s < 0 for s in list ( self . sentiment_history )[ - 3 :]) ] return any ( conditions )","title":"Re-engagement Triggers"},{"location":"conversation/response-generation/","text":"Response Generation \u00b6 The response generation system combines sentiment analysis, conversation state, and model backend selection to generate contextually appropriate responses. Core Components \u00b6 ResponseGenerator \u00b6 The ResponseGenerator class orchestrates: Topic-aware prompt creation Sentiment-based response adjustment Engagement monitoring and reengagement Backend model selection Engagement Management \u00b6 The system maintains user engagement through: Reengagement Prompts \u00b6 reengagement_prompts = { \"snake_resistance\" : [ \"Would you like to know more about how opossums handle encounters with snakes?\" , \"Did you know opossums have fascinating adaptations for dealing with venomous snakes?\" ], \"diet_query\" : [ \"I know some interesting facts about opossum dietary preferences.\" , \"Would you like to learn about the unique things opossums eat?\" ] # ... other topic-specific prompts } Engagement Detection \u00b6 Monitors message length Tracks sentiment trends Identifies follow-up questions Measures topic coherence Context-Aware Generation \u00b6 Prompt Creation \u00b6 The system builds prompts that include: Current conversation stage Recent message history User sentiment analysis Topic relevance scores Example prompt structure: Current topic: {conversation_state.current_stage} User sentiment: {sentiment_analysis['sentiment']['polarity']} Recent context: {context_window} User message: {user_message} Response Assembly \u00b6 Base response generation from model Engagement prompt addition when needed Message history updates Metadata attachment Integration with Model Backends \u00b6 The response generator works with multiple model backends: Gemini Backend High-capability responses Multimodal processing Advanced reasoning Ollama Backend Local processing Custom model support Efficient response generation Transformers Backend Fallback capability Offline operation Basic response generation Performance Considerations \u00b6 Caching Strategy \u00b6 Caches complete responses Stores sentiment analysis Preserves context windows Response Times \u00b6 Operation Target Time Basic Response < 1s With Context < 2s With Image < 3s Error Handling \u00b6 Graceful Degradation if sentiment_tracker . get_engagement_summary ()[ \"needs_reengagement\" ]: response_text = self . _add_engagement_prompt ( response_text , next_stage ) Fallback Responses Default to simpler responses Preserve conversation context Maintain user engagement Monitoring and Metrics \u00b6 The system tracks: Response generation times Engagement success rates Sentiment trends Topic transition effectiveness Example Flow \u00b6 graph TD A[User Input] --> B[Context Assembly] B --> C[Sentiment Analysis] C --> D[Topic Detection] D --> E[Prompt Creation] E --> F[Model Generation] F --> G[Engagement Check] G --> H[Response Assembly] H --> I[Final Response] Sample Code \u00b6 async def generate_response ( self , user_message : str , conversation_state : ConversationState , sentiment_tracker : SentimentTracker , model_backend : Any ) -> Dict [ str , Any ]: # Analyze message sentiment sentiment_analysis = sentiment_tracker . analyze_message ( user_message , self . _is_follow_up ( user_message ) ) # Determine next conversation stage next_stage = self . topic_detector . determine_next_stage ( user_message , conversation_state . current_stage ) # Update conversation state conversation_state . update_stage ( next_stage ) # Generate base response base_prompt = self . _create_prompt ( user_message , conversation_state , sentiment_analysis ) response_text = await model_backend . generate_response ( base_prompt , next_stage ) # Add engagement prompts if needed if sentiment_tracker . get_engagement_summary ()[ \"needs_reengagement\" ]: response_text = self . _add_engagement_prompt ( response_text , next_stage ) # Record the interaction conversation_state . add_message ( ... ) return { \"response\" : response_text , \"next_stage\" : next_stage , \"sentiment\" : sentiment_analysis , \"needs_reengagement\" : sentiment_tracker . get_engagement_summary ()[ \"needs_reengagement\" ] } Re-engagement Strategies \u00b6 The system can detect waning user interest and automatically add re-engagement prompts: def _add_engagement_prompt ( self , response : str , stage : str ) -> str : \"\"\"Add an engagement prompt to the response if appropriate.\"\"\" if stage in self . reengagement_prompts : prompt = self . reengagement_prompts [ stage ][ 0 ] return f \" { response } \\n\\n { prompt } \" return response","title":"Response Generation"},{"location":"conversation/response-generation/#response-generation","text":"The response generation system combines sentiment analysis, conversation state, and model backend selection to generate contextually appropriate responses.","title":"Response Generation"},{"location":"conversation/response-generation/#core-components","text":"","title":"Core Components"},{"location":"conversation/response-generation/#responsegenerator","text":"The ResponseGenerator class orchestrates: Topic-aware prompt creation Sentiment-based response adjustment Engagement monitoring and reengagement Backend model selection","title":"ResponseGenerator"},{"location":"conversation/response-generation/#engagement-management","text":"The system maintains user engagement through:","title":"Engagement Management"},{"location":"conversation/response-generation/#reengagement-prompts","text":"reengagement_prompts = { \"snake_resistance\" : [ \"Would you like to know more about how opossums handle encounters with snakes?\" , \"Did you know opossums have fascinating adaptations for dealing with venomous snakes?\" ], \"diet_query\" : [ \"I know some interesting facts about opossum dietary preferences.\" , \"Would you like to learn about the unique things opossums eat?\" ] # ... other topic-specific prompts }","title":"Reengagement Prompts"},{"location":"conversation/response-generation/#engagement-detection","text":"Monitors message length Tracks sentiment trends Identifies follow-up questions Measures topic coherence","title":"Engagement Detection"},{"location":"conversation/response-generation/#context-aware-generation","text":"","title":"Context-Aware Generation"},{"location":"conversation/response-generation/#prompt-creation","text":"The system builds prompts that include: Current conversation stage Recent message history User sentiment analysis Topic relevance scores Example prompt structure: Current topic: {conversation_state.current_stage} User sentiment: {sentiment_analysis['sentiment']['polarity']} Recent context: {context_window} User message: {user_message}","title":"Prompt Creation"},{"location":"conversation/response-generation/#response-assembly","text":"Base response generation from model Engagement prompt addition when needed Message history updates Metadata attachment","title":"Response Assembly"},{"location":"conversation/response-generation/#integration-with-model-backends","text":"The response generator works with multiple model backends: Gemini Backend High-capability responses Multimodal processing Advanced reasoning Ollama Backend Local processing Custom model support Efficient response generation Transformers Backend Fallback capability Offline operation Basic response generation","title":"Integration with Model Backends"},{"location":"conversation/response-generation/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"conversation/response-generation/#caching-strategy","text":"Caches complete responses Stores sentiment analysis Preserves context windows","title":"Caching Strategy"},{"location":"conversation/response-generation/#response-times","text":"Operation Target Time Basic Response < 1s With Context < 2s With Image < 3s","title":"Response Times"},{"location":"conversation/response-generation/#error-handling","text":"Graceful Degradation if sentiment_tracker . get_engagement_summary ()[ \"needs_reengagement\" ]: response_text = self . _add_engagement_prompt ( response_text , next_stage ) Fallback Responses Default to simpler responses Preserve conversation context Maintain user engagement","title":"Error Handling"},{"location":"conversation/response-generation/#monitoring-and-metrics","text":"The system tracks: Response generation times Engagement success rates Sentiment trends Topic transition effectiveness","title":"Monitoring and Metrics"},{"location":"conversation/response-generation/#example-flow","text":"graph TD A[User Input] --> B[Context Assembly] B --> C[Sentiment Analysis] C --> D[Topic Detection] D --> E[Prompt Creation] E --> F[Model Generation] F --> G[Engagement Check] G --> H[Response Assembly] H --> I[Final Response]","title":"Example Flow"},{"location":"conversation/response-generation/#sample-code","text":"async def generate_response ( self , user_message : str , conversation_state : ConversationState , sentiment_tracker : SentimentTracker , model_backend : Any ) -> Dict [ str , Any ]: # Analyze message sentiment sentiment_analysis = sentiment_tracker . analyze_message ( user_message , self . _is_follow_up ( user_message ) ) # Determine next conversation stage next_stage = self . topic_detector . determine_next_stage ( user_message , conversation_state . current_stage ) # Update conversation state conversation_state . update_stage ( next_stage ) # Generate base response base_prompt = self . _create_prompt ( user_message , conversation_state , sentiment_analysis ) response_text = await model_backend . generate_response ( base_prompt , next_stage ) # Add engagement prompts if needed if sentiment_tracker . get_engagement_summary ()[ \"needs_reengagement\" ]: response_text = self . _add_engagement_prompt ( response_text , next_stage ) # Record the interaction conversation_state . add_message ( ... ) return { \"response\" : response_text , \"next_stage\" : next_stage , \"sentiment\" : sentiment_analysis , \"needs_reengagement\" : sentiment_tracker . get_engagement_summary ()[ \"needs_reengagement\" ] }","title":"Sample Code"},{"location":"conversation/response-generation/#re-engagement-strategies","text":"The system can detect waning user interest and automatically add re-engagement prompts: def _add_engagement_prompt ( self , response : str , stage : str ) -> str : \"\"\"Add an engagement prompt to the response if appropriate.\"\"\" if stage in self . reengagement_prompts : prompt = self . reengagement_prompts [ stage ][ 0 ] return f \" { response } \\n\\n { prompt } \" return response","title":"Re-engagement Strategies"},{"location":"conversation/topic-detection/","text":"Topic Detection \u00b6 The topic detection system uses sentence transformers and semantic similarity to manage conversation flow and context. Overview \u00b6 The TopicDetector class analyzes user messages to: Classify message intent and topic Determine appropriate conversation stage transitions Maintain topic continuity throughout conversations Implementation \u00b6 The TopicDetector class implements NLP-based topic detection using the following components: Pre-defined Topics \u00b6 topic_sentences = { \"snake_resistance\" : \"Opossums are resistant to snake venom and can eat venomous snakes\" , \"florida_opossums\" : \"Opossums in Florida and the southeastern United States\" , \"diet_query\" : \"What opossums eat and their diet habits\" , \"habitat_query\" : \"Where opossums live and their natural habitat\" , \"behavior_query\" : \"Opossum behavior, playing dead, and nocturnal activities\" , \"general_info\" : \"General information about opossums\" , \"closing\" : \"Thank you, goodbye, or ending the conversation\" } Sentence Transformer Model \u00b6 Uses the all-MiniLM-L6-v2 model for generating embeddings Pre-computes embeddings for all topic sentences during initialization Calculates cosine similarity between user messages and topic embeddings Stage Determination \u00b6 The topic detector determines conversation stages through: Automatic progression from greeting to initial query Semantic similarity scoring between user message and topic embeddings Follow-up detection using indicator words: more, also, another, explain, elaborate why, how, what, tell me more Configuration \u00b6 Topic detection can be tuned through the following settings: SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2' SIMILARITY_THRESHOLD = 0.35 # Minimum similarity score for topic change Stages \u00b6 The system tracks conversation through defined stages: greeting : Initial conversation stage snake_resistance : Discussions about opossum immunity to snake venom florida_opossums : Regional opossum information diet_query : Questions about opossum diet habitat_query : Information about opossum habitats behavior_query : Questions about opossum behaviors Integration \u00b6 The topic detector integrates with other conversation components through: Conversation State : Updates the current stage based on detected topics Response Generator : Informs response generation with topic context Model Selection : Helps choose appropriate model based on topic requirements Example Flow \u00b6 graph TD A[User Message] --> B[Encode Message] B --> C[Calculate Similarities] C --> D{Score > Threshold?} D -- Yes --> E[Change Topic] D -- No --> F{Is Follow-up?} F -- Yes --> G[Maintain Topic] F -- No --> H[Default Topic] # Determine the next conversation stage next_stage = topic_detector . determine_next_stage ( user_message , conversation_state . current_stage ) # Update conversation state conversation_state . update_stage ( next_stage )","title":"Topic Detection"},{"location":"conversation/topic-detection/#topic-detection","text":"The topic detection system uses sentence transformers and semantic similarity to manage conversation flow and context.","title":"Topic Detection"},{"location":"conversation/topic-detection/#overview","text":"The TopicDetector class analyzes user messages to: Classify message intent and topic Determine appropriate conversation stage transitions Maintain topic continuity throughout conversations","title":"Overview"},{"location":"conversation/topic-detection/#implementation","text":"The TopicDetector class implements NLP-based topic detection using the following components:","title":"Implementation"},{"location":"conversation/topic-detection/#pre-defined-topics","text":"topic_sentences = { \"snake_resistance\" : \"Opossums are resistant to snake venom and can eat venomous snakes\" , \"florida_opossums\" : \"Opossums in Florida and the southeastern United States\" , \"diet_query\" : \"What opossums eat and their diet habits\" , \"habitat_query\" : \"Where opossums live and their natural habitat\" , \"behavior_query\" : \"Opossum behavior, playing dead, and nocturnal activities\" , \"general_info\" : \"General information about opossums\" , \"closing\" : \"Thank you, goodbye, or ending the conversation\" }","title":"Pre-defined Topics"},{"location":"conversation/topic-detection/#sentence-transformer-model","text":"Uses the all-MiniLM-L6-v2 model for generating embeddings Pre-computes embeddings for all topic sentences during initialization Calculates cosine similarity between user messages and topic embeddings","title":"Sentence Transformer Model"},{"location":"conversation/topic-detection/#stage-determination","text":"The topic detector determines conversation stages through: Automatic progression from greeting to initial query Semantic similarity scoring between user message and topic embeddings Follow-up detection using indicator words: more, also, another, explain, elaborate why, how, what, tell me more","title":"Stage Determination"},{"location":"conversation/topic-detection/#configuration","text":"Topic detection can be tuned through the following settings: SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2' SIMILARITY_THRESHOLD = 0.35 # Minimum similarity score for topic change","title":"Configuration"},{"location":"conversation/topic-detection/#stages","text":"The system tracks conversation through defined stages: greeting : Initial conversation stage snake_resistance : Discussions about opossum immunity to snake venom florida_opossums : Regional opossum information diet_query : Questions about opossum diet habitat_query : Information about opossum habitats behavior_query : Questions about opossum behaviors","title":"Stages"},{"location":"conversation/topic-detection/#integration","text":"The topic detector integrates with other conversation components through: Conversation State : Updates the current stage based on detected topics Response Generator : Informs response generation with topic context Model Selection : Helps choose appropriate model based on topic requirements","title":"Integration"},{"location":"conversation/topic-detection/#example-flow","text":"graph TD A[User Message] --> B[Encode Message] B --> C[Calculate Similarities] C --> D{Score > Threshold?} D -- Yes --> E[Change Topic] D -- No --> F{Is Follow-up?} F -- Yes --> G[Maintain Topic] F -- No --> H[Default Topic] # Determine the next conversation stage next_stage = topic_detector . determine_next_stage ( user_message , conversation_state . current_stage ) # Update conversation state conversation_state . update_stage ( next_stage )","title":"Example Flow"},{"location":"features/background-stories/","text":"Background Stories \u00b6 Tales from the Opossum Community \u00b6 While we haven't collected human customer testimonials yet, our namesake marsupials have plenty to say about Opossum Search. The following \"testimonials\" come from our unofficial mascots and quality assurance team. Phoebe's Midnight Research Adventure \u00b6 Phoebe, 3-year-old Virginia Opossum, Academic Researcher *hiss* *click* *sniff* It was approximately 2:37 AM when I needed to look up whether persimmons were in season yet. As a nocturnal researcher with 13 joeys in my pouch, I don't have time for slow search engines. Opossum Search not only told me about persimmon seasonality but also provided a helpful SVG map of fruiting trees in my area. The night-mode interface was easy on my sensitive eyes, and the response was faster than I could bare my 50 teeth. When the system went offline briefly, it played dead just like I would! But it recovered quickly, clearly implementing the same resilience strategies we opossums have perfected over 70 million years. Five stars and a tail wiggle! *chitter* *snuffle* Morton's Technical Evaluation \u00b6 Morton, 4-year-old Opossum, IT Infrastructure Specialist *low growl* *click-click* As someone who spends most nights hanging by my tail from server racks, I appreciate good infrastructure design. Opossum Search's hybrid model selection reminds me of how I select the optimal trash cans behind restaurants\u2014maximum reward with minimal effort. The Redis caching implementation is particularly impressive. I tested it by repeatedly requesting the same information about tick removal techniques, and the response time improved substantially on subsequent requests. Very cache. Much speed. *sniff* When I intentionally caused a service outage by chewing through some virtual cables, the system gracefully degraded to alternative services instead of completely failing. As we say in the marsupial DevOps community: \"If you can't play dead, at least play slow.\" *rapid clicking* *defensive hiss* Daisy's Creative Studio Experience \u00b6 Daisy, 2-year-old Opossum, Graphic Designer *soft chirp* *snuffle* Before discovering Opossum Search, my night-time design workflow was as messy as a garbage bin tipped over by a raccoon. Now, everything is organized! The image processing pipeline handled my paw-painted JPEGs beautifully, even when they were slightly muddy from my midnight foraging. The SVG generation feature created vector graphics that looked like I spent hours meticulously crafting them with my tiny opossum claws. During National Opossum Day, the special visualizations made me so excited I accidentally played dead for 3 hours! When I woke up, my work was still there waiting for me. My only suggestion? More marsupial-themed templates! *pleased clicking sounds* Four paws up! Would recommend to all creatures nocturnal and otherwise. Captain Hissy's Security Assessment \u00b6 Captain Hissy, 5-year-old Opossum, Cybersecurity Expert *defensive open-mouth hiss* *teeth clicking* As someone who spends their life protecting their territory from threats, I appreciate robust security measures. I attempted to breach Opossum Search using classic opossum hacking techniques\u2014playing dead until the system let down its guard, climbing through unsecured ports, and even trying to inject SQL through my standard diet of ticks and grubs. Nothing worked. *frustrated snort* The authentication system immediately recognized I wasn't an authorized user, the input validation caught my attempts to inject malicious code disguised as tick recipes, and the rate limiting prevented my brute force attempts using my 50 teeth. Highly secure. Would fail to hack again. 10/10 for security implementation. *admiring but slightly disappointed hiss* Waddles' Infrastructure Migration Story \u00b6 Waddles, 3.5-year-old Opossum, System Administrator *soft clicking* *shuffle shuffle* When our den needed to migrate from an old hollow log to a new burrow under the garden shed, I was worried about service disruption. Moving 7 joeys while maintaining 99.99% uptime is no small feat! Using Opossum Search's deployment guide as a template, we implemented a blue-green migration strategy. We set up the new den while maintaining the old one, performed a controlled switchover during low-traffic hours (mid-day, when we're all sleeping anyway), and implemented health checks to verify the new den's stability. Thanks to the resilience patterns we learned, not a single joey was misplaced during the migration. The observability implementation even let us track ambient temperature across both environments. *contented purring noise* Next week we're implementing the same sharded caching strategy for our winter food storage. I'll let you know how it goes! Penny's Performance Optimization Tale \u00b6 Penny, 1.5-year-old Virginia Opossum, Software Performance Engineer *analytical sniffing* *thoughtful tail curl* When I joined the nocturnal code review team at Opossum Search, the first thing I noticed was how the Chat2SVG pipeline was structured. As someone who carries up to 13 joeys in my pouch, I know a thing or two about optimizing limited resources! The original implementation was perfectly fine\u2014each stage operating independently, saving files to disk between operations\u2014much like how I used to store snacks in separate tree hollows. But my whiskers twitched when I saw those disk I/O operations. *disapproving hiss* My suggestion? Memory streaming! Instead of writing the SVG to disk after template generation and then reading it back for detail enhancement, why not keep it all in working memory? My implementation used shared temporary directories and in-memory data transport that reduced processing time by 42%\u2014almost as efficient as my technique for gathering ticks! The resource monitoring was brilliantly adaptive too. When my den-mates were running their nightly foraging simulations (heavy CPU usage), the system automatically scaled back to \"medium\" detail level instead of \"high,\" ensuring my visualization of \"opossum eating blueberries\" still completed without timing out. My favorite feature? The pipeline controller that handles the entire three-stage process. It reminded me of how I organize my joeys\u2014smallest at the center of the pouch, largest at the edges for optimal carrying efficiency. Five prehensile tails up! Would optimize again! *satisfied chittering* *proud tail wiggle* Marvin's Jitter Implementation Story \u00b6 Marvin, 2.5-year-old Virginia Opossum, Distributed Systems Engineer *excited chittering* *rhythmic tail swishes* Before I joined the Opossum Search resilience team, we had what I call the \"startled opossum problem.\" Whenever our Gemini service recovered after being down, every single request would immediately rush to it\u2014like how all 12 of my siblings would sprint toward a freshly tipped trash can, causing the whole thing to topple over again! *frustrated hiss* That's when I implemented what we marsupials call \"the Shake and Scatter\" technique, but you bipeds know as \"request jittering.\" It works exactly like how my family forages! You see, when I discover a fallen bird feeder, I don't tell everyone at once. I tell some siblings to check the garden hose, others to investigate the compost heap, and only a few to join me at the feeder. That way, we don't overwhelm any one food source. *smug clicking* My jittering algorithm works the same way: when a service recovers, I programmed a 20% chance that requests would still use the backup service even though the primary was available again. This created a natural \"scattered approach\" to service recovery. # My brilliant code, inspired by opossum foraging patterns if recovered_service and random . random () < 0.2 : # 20% chance # Keep using alternative service selected_service = backup_service log . info ( \"Applying jitter: Using backup even though primary is available\" ) The results were immediate! Instead of services repeatedly failing from the \"thundering herd\" of requests, they gracefully recovered\u2014just like how we possums naturally distribute ourselves when foraging to avoid over-competition. For high-volume APIs, I even implemented what I call \"adaptive jitter.\" Just like how I adjust my foraging strategy based on how many siblings are already at the trash can, the system dynamically adjusts jitter percentages based on current service health metrics! My proudest moment? During last month's major infrastructure update, our systems recovered 73% faster with jittering than without it. Almost as impressive as the time I found three discarded hamburgers and didn't tell anyone until I'd eaten my fill! *self-satisfied purr* Remember fellow engineers: when systems fail, don't play dead\u2014implement jitter instead! *professional tail curl* *technical sniffing* Brenda's Circuit Breaker Saga \u00b6 Brenda, 4.5-year-old Virginia Opossum, Senior Reliability Engineer *cautious sniffing* *slow, deliberate tail movements* They call me Brenda the Breaker, not because I break things, mind you, but because I stop others from breaking everything else! It's a subtle distinction, much like the difference between playing dead and actually being dead. * *meaningful pause** Let me tell you about the Great Persimmon API Fiasco of last autumn. Our primary persimmon ripeness detector (a highly sophisticated API involving trained squirrels) started returning errors. First one, then two... just like when a curious raccoon approaches my den. *narrowed eyes* Now, a naive system would just keep poking the failing API, right? Wrong! That's how you get a cascading failure\u2014like waking up a whole den of grumpy opossums at once. Instead, I implemented the circuit breaker pattern. # My elegant circuit breaker logic failure_count += 1 if failure_count >= self . failure_threshold : self . state = CircuitState . OPEN self . last_failure_time = time . monotonic () log . warning ( f \"Circuit breaker for { self . name } tripped! Moving to OPEN state.\" ) After five consecutive failures (my personal threshold for 'dangerously annoying'), the circuit breaker tripped ! * Click! Just like that, Opossum Search stopped sending requests to the Persimmon API. We entered the OPEN state. It's like when I play dead\u2014I stop interacting with the threat (the failing service) and give it space. Let it calm down. Let it recover. *another dramatic pause** After a minute (the reset_timeout ), the breaker cautiously moved to the HALF-OPEN state. It sent one tentative request, like me peeking one eye open to see if the raccoon has left. If that request succeeded? Snap! Back to * CLOSED , business as usual. If it failed? Thump! Back to OPEN *, more playing dead time needed. This simple pattern saved us! Instead of the whole search system grinding to a halt because of some unreliable squirrels, we gracefully isolated the problem, used fallback data (last known persimmon locations), and allowed the API to recover without pressure. It's the opossum way: anticipate trouble, isolate the problem, give it space, and cautiously test before resuming normal operations. Resilience isn't about never failing; it's about failing smartly . *nods sagely* *resumes cautious sniffing* Olivia's Observability Odyssey \u00b6 Olivia, 2-year-old Virginia Opossum, Observability Engineer *attentive ear twitching* *careful sniffing* As a nocturnal creature with heightened senses, I've always believed I was born for observability engineering. My 50 teeth can detect the slightest irregularities in food texture, my sensitive whiskers can feel system vibrations others miss, and my keen nose can \"smell\" trouble brewing in production before alerts even trigger. *proud snuffle* When I joined the Opossum Search monitoring team, they were still using what I call the \"raccoon approach\" to observability\u2014only noticing problems when something loud crashed in the garbage cans. *disapproving hiss* I implemented what we opossums call the \"Three Senses Framework\" for comprehensive monitoring: vision (metrics), hearing (logs), and smell (traces). Just as I use multiple senses to navigate through the darkness, our systems now perceive their environment through multiple complementary signals. My proudest achievement was implementing our distributed trace sampling system. It works exactly like how we opossums forage! When I'm searching for ticks\u2014my preferred protein source\u2014I don't meticulously inspect every square inch of my territory. Instead, I use adaptive sampling based on where I've had success before. def sample_trace ( trace_context ): # Smart sampling logic inspired by opossum foraging patterns if trace_context . has_error or trace_context . duration > HIGH_LATENCY_THRESHOLD : # Always capture problematic traces (just like I always eat ticks I find) return True elif trace_context . service in CRITICAL_SERVICES : # Sample 30% of important services (like prime foraging areas) return random . random () < 0.3 else : # Sample only 5% of normal traces (like routine territory checks) return random . random () < 0.05 When system anomalies occur, we no longer play dead and hope the problem resolves itself. Our new alerting system categorizes issues by severity, just like I categorize potential threats\u2014from \"harmless garden hose\" to \"aggressive neighborhood cat\" to \"PANIC! HUMAN WITH BROOM!\" The custom dashboards I designed maintain my lower body temperature preference of 95\u00b0F\u2014I mean, they run with lower CPU utilization while still showing key vitals. And just like how I can rotate my rear feet 180\u00b0 for climbing down trees head-first, our monitoring views can pivot instantly between service-level and system-wide perspectives! After implementing our new observability stack, mean time to detection dropped by 76%, almost as impressive as my ability to process 4,000 ticks per season! *self-satisfied chitter* Remember fellow engineers: in observability as in opossum life, it's not just about playing dead when things go wrong\u2014it's about having the senses to avoid danger in the first place! *vigilant tail curl* *nocturnal clicking sounds* Penelope's Prompt Polishing Prowess \u00b6 Penelope \"Penny\" Promptwhisperer, 2-year-old Virginia Opossum, DSPy Optimization Specialist *soft clicking* *intense sniffing at the screen* You know, crafting the perfect prompt is like teaching a joey how to ask nicely for a grub instead of just hissing vaguely in the direction of the log pile. Before I joined the Opossum Search team, some of our internal prompts were... well, let's just say they were more hiss than helpful. *gentle sigh* My specialty? DSPy optimization! I took our basic Q&A pipeline, the one that uses the OpossumFactTool , and decided it needed some refinement. Manually tweaking prompts felt as inefficient as trying to carry 14 joeys without a pouch! So, I turned to BootstrapFewShot . I fed it examples from our opossum_dataset_converted.json \u2014good questions paired with good answers\u2014and let it work its magic. It was like showing the LM exactly how a well-behaved opossum answers questions: factually, concisely, and without unnecessary rambling about shiny objects. # My secret sauce - letting DSPy do the hard work! teleprompter = dspy . BootstrapFewShot ( metric = my_custom_opossum_metric , max_bootstrapped_demos = 3 ) optimized_qa_module = teleprompter . compile ( student = unoptimized_qa , trainset = training_data ) The results? Chef's kiss ... or, uh, opossum's appreciative sniff . The optimized prompts led to answers that were 30% more accurate according to my metrics (which mostly involve checking if the answer correctly states that opossums are immune to most snake venom). It was almost as satisfying as finding an unattended bowl of cat food! Now, our pipelines don't just answer questions; they answer them elegantly . It's the difference between a clumsy joey tumbling out of a tree and a graceful adult gliding down using its prehensile tail. Precision, efficiency, and a touch of marsupial magic! *satisfied tail curl* *resumes sniffing at the prompt output* Implementation Note \u00b6 These fictional testimonials are included as a lighthearted nod to our marsupial namesake. They serve to illustrate Opossum Search features in an engaging way while acknowledging the project's playful side. While we wait for actual human user stories, we hope these tales provide both entertainment and insight into the system's capabilities. If you're an actual opossum using our system, please contact us\u2014we'd love to hear your story too! Note: No opossums were involved in the development of this software. Any resemblance to actual opossums, living or playing dead, is purely coincidental.","title":"Background Stories"},{"location":"features/background-stories/#background-stories","text":"","title":"Background Stories"},{"location":"features/background-stories/#tales-from-the-opossum-community","text":"While we haven't collected human customer testimonials yet, our namesake marsupials have plenty to say about Opossum Search. The following \"testimonials\" come from our unofficial mascots and quality assurance team.","title":"Tales from the Opossum Community"},{"location":"features/background-stories/#phoebes-midnight-research-adventure","text":"Phoebe, 3-year-old Virginia Opossum, Academic Researcher *hiss* *click* *sniff* It was approximately 2:37 AM when I needed to look up whether persimmons were in season yet. As a nocturnal researcher with 13 joeys in my pouch, I don't have time for slow search engines. Opossum Search not only told me about persimmon seasonality but also provided a helpful SVG map of fruiting trees in my area. The night-mode interface was easy on my sensitive eyes, and the response was faster than I could bare my 50 teeth. When the system went offline briefly, it played dead just like I would! But it recovered quickly, clearly implementing the same resilience strategies we opossums have perfected over 70 million years. Five stars and a tail wiggle! *chitter* *snuffle*","title":"Phoebe's Midnight Research Adventure"},{"location":"features/background-stories/#mortons-technical-evaluation","text":"Morton, 4-year-old Opossum, IT Infrastructure Specialist *low growl* *click-click* As someone who spends most nights hanging by my tail from server racks, I appreciate good infrastructure design. Opossum Search's hybrid model selection reminds me of how I select the optimal trash cans behind restaurants\u2014maximum reward with minimal effort. The Redis caching implementation is particularly impressive. I tested it by repeatedly requesting the same information about tick removal techniques, and the response time improved substantially on subsequent requests. Very cache. Much speed. *sniff* When I intentionally caused a service outage by chewing through some virtual cables, the system gracefully degraded to alternative services instead of completely failing. As we say in the marsupial DevOps community: \"If you can't play dead, at least play slow.\" *rapid clicking* *defensive hiss*","title":"Morton's Technical Evaluation"},{"location":"features/background-stories/#daisys-creative-studio-experience","text":"Daisy, 2-year-old Opossum, Graphic Designer *soft chirp* *snuffle* Before discovering Opossum Search, my night-time design workflow was as messy as a garbage bin tipped over by a raccoon. Now, everything is organized! The image processing pipeline handled my paw-painted JPEGs beautifully, even when they were slightly muddy from my midnight foraging. The SVG generation feature created vector graphics that looked like I spent hours meticulously crafting them with my tiny opossum claws. During National Opossum Day, the special visualizations made me so excited I accidentally played dead for 3 hours! When I woke up, my work was still there waiting for me. My only suggestion? More marsupial-themed templates! *pleased clicking sounds* Four paws up! Would recommend to all creatures nocturnal and otherwise.","title":"Daisy's Creative Studio Experience"},{"location":"features/background-stories/#captain-hissys-security-assessment","text":"Captain Hissy, 5-year-old Opossum, Cybersecurity Expert *defensive open-mouth hiss* *teeth clicking* As someone who spends their life protecting their territory from threats, I appreciate robust security measures. I attempted to breach Opossum Search using classic opossum hacking techniques\u2014playing dead until the system let down its guard, climbing through unsecured ports, and even trying to inject SQL through my standard diet of ticks and grubs. Nothing worked. *frustrated snort* The authentication system immediately recognized I wasn't an authorized user, the input validation caught my attempts to inject malicious code disguised as tick recipes, and the rate limiting prevented my brute force attempts using my 50 teeth. Highly secure. Would fail to hack again. 10/10 for security implementation. *admiring but slightly disappointed hiss*","title":"Captain Hissy's Security Assessment"},{"location":"features/background-stories/#waddles-infrastructure-migration-story","text":"Waddles, 3.5-year-old Opossum, System Administrator *soft clicking* *shuffle shuffle* When our den needed to migrate from an old hollow log to a new burrow under the garden shed, I was worried about service disruption. Moving 7 joeys while maintaining 99.99% uptime is no small feat! Using Opossum Search's deployment guide as a template, we implemented a blue-green migration strategy. We set up the new den while maintaining the old one, performed a controlled switchover during low-traffic hours (mid-day, when we're all sleeping anyway), and implemented health checks to verify the new den's stability. Thanks to the resilience patterns we learned, not a single joey was misplaced during the migration. The observability implementation even let us track ambient temperature across both environments. *contented purring noise* Next week we're implementing the same sharded caching strategy for our winter food storage. I'll let you know how it goes!","title":"Waddles' Infrastructure Migration Story"},{"location":"features/background-stories/#pennys-performance-optimization-tale","text":"Penny, 1.5-year-old Virginia Opossum, Software Performance Engineer *analytical sniffing* *thoughtful tail curl* When I joined the nocturnal code review team at Opossum Search, the first thing I noticed was how the Chat2SVG pipeline was structured. As someone who carries up to 13 joeys in my pouch, I know a thing or two about optimizing limited resources! The original implementation was perfectly fine\u2014each stage operating independently, saving files to disk between operations\u2014much like how I used to store snacks in separate tree hollows. But my whiskers twitched when I saw those disk I/O operations. *disapproving hiss* My suggestion? Memory streaming! Instead of writing the SVG to disk after template generation and then reading it back for detail enhancement, why not keep it all in working memory? My implementation used shared temporary directories and in-memory data transport that reduced processing time by 42%\u2014almost as efficient as my technique for gathering ticks! The resource monitoring was brilliantly adaptive too. When my den-mates were running their nightly foraging simulations (heavy CPU usage), the system automatically scaled back to \"medium\" detail level instead of \"high,\" ensuring my visualization of \"opossum eating blueberries\" still completed without timing out. My favorite feature? The pipeline controller that handles the entire three-stage process. It reminded me of how I organize my joeys\u2014smallest at the center of the pouch, largest at the edges for optimal carrying efficiency. Five prehensile tails up! Would optimize again! *satisfied chittering* *proud tail wiggle*","title":"Penny's Performance Optimization Tale"},{"location":"features/background-stories/#marvins-jitter-implementation-story","text":"Marvin, 2.5-year-old Virginia Opossum, Distributed Systems Engineer *excited chittering* *rhythmic tail swishes* Before I joined the Opossum Search resilience team, we had what I call the \"startled opossum problem.\" Whenever our Gemini service recovered after being down, every single request would immediately rush to it\u2014like how all 12 of my siblings would sprint toward a freshly tipped trash can, causing the whole thing to topple over again! *frustrated hiss* That's when I implemented what we marsupials call \"the Shake and Scatter\" technique, but you bipeds know as \"request jittering.\" It works exactly like how my family forages! You see, when I discover a fallen bird feeder, I don't tell everyone at once. I tell some siblings to check the garden hose, others to investigate the compost heap, and only a few to join me at the feeder. That way, we don't overwhelm any one food source. *smug clicking* My jittering algorithm works the same way: when a service recovers, I programmed a 20% chance that requests would still use the backup service even though the primary was available again. This created a natural \"scattered approach\" to service recovery. # My brilliant code, inspired by opossum foraging patterns if recovered_service and random . random () < 0.2 : # 20% chance # Keep using alternative service selected_service = backup_service log . info ( \"Applying jitter: Using backup even though primary is available\" ) The results were immediate! Instead of services repeatedly failing from the \"thundering herd\" of requests, they gracefully recovered\u2014just like how we possums naturally distribute ourselves when foraging to avoid over-competition. For high-volume APIs, I even implemented what I call \"adaptive jitter.\" Just like how I adjust my foraging strategy based on how many siblings are already at the trash can, the system dynamically adjusts jitter percentages based on current service health metrics! My proudest moment? During last month's major infrastructure update, our systems recovered 73% faster with jittering than without it. Almost as impressive as the time I found three discarded hamburgers and didn't tell anyone until I'd eaten my fill! *self-satisfied purr* Remember fellow engineers: when systems fail, don't play dead\u2014implement jitter instead! *professional tail curl* *technical sniffing*","title":"Marvin's Jitter Implementation Story"},{"location":"features/background-stories/#brendas-circuit-breaker-saga","text":"Brenda, 4.5-year-old Virginia Opossum, Senior Reliability Engineer *cautious sniffing* *slow, deliberate tail movements* They call me Brenda the Breaker, not because I break things, mind you, but because I stop others from breaking everything else! It's a subtle distinction, much like the difference between playing dead and actually being dead. * *meaningful pause** Let me tell you about the Great Persimmon API Fiasco of last autumn. Our primary persimmon ripeness detector (a highly sophisticated API involving trained squirrels) started returning errors. First one, then two... just like when a curious raccoon approaches my den. *narrowed eyes* Now, a naive system would just keep poking the failing API, right? Wrong! That's how you get a cascading failure\u2014like waking up a whole den of grumpy opossums at once. Instead, I implemented the circuit breaker pattern. # My elegant circuit breaker logic failure_count += 1 if failure_count >= self . failure_threshold : self . state = CircuitState . OPEN self . last_failure_time = time . monotonic () log . warning ( f \"Circuit breaker for { self . name } tripped! Moving to OPEN state.\" ) After five consecutive failures (my personal threshold for 'dangerously annoying'), the circuit breaker tripped ! * Click! Just like that, Opossum Search stopped sending requests to the Persimmon API. We entered the OPEN state. It's like when I play dead\u2014I stop interacting with the threat (the failing service) and give it space. Let it calm down. Let it recover. *another dramatic pause** After a minute (the reset_timeout ), the breaker cautiously moved to the HALF-OPEN state. It sent one tentative request, like me peeking one eye open to see if the raccoon has left. If that request succeeded? Snap! Back to * CLOSED , business as usual. If it failed? Thump! Back to OPEN *, more playing dead time needed. This simple pattern saved us! Instead of the whole search system grinding to a halt because of some unreliable squirrels, we gracefully isolated the problem, used fallback data (last known persimmon locations), and allowed the API to recover without pressure. It's the opossum way: anticipate trouble, isolate the problem, give it space, and cautiously test before resuming normal operations. Resilience isn't about never failing; it's about failing smartly . *nods sagely* *resumes cautious sniffing*","title":"Brenda's Circuit Breaker Saga"},{"location":"features/background-stories/#olivias-observability-odyssey","text":"Olivia, 2-year-old Virginia Opossum, Observability Engineer *attentive ear twitching* *careful sniffing* As a nocturnal creature with heightened senses, I've always believed I was born for observability engineering. My 50 teeth can detect the slightest irregularities in food texture, my sensitive whiskers can feel system vibrations others miss, and my keen nose can \"smell\" trouble brewing in production before alerts even trigger. *proud snuffle* When I joined the Opossum Search monitoring team, they were still using what I call the \"raccoon approach\" to observability\u2014only noticing problems when something loud crashed in the garbage cans. *disapproving hiss* I implemented what we opossums call the \"Three Senses Framework\" for comprehensive monitoring: vision (metrics), hearing (logs), and smell (traces). Just as I use multiple senses to navigate through the darkness, our systems now perceive their environment through multiple complementary signals. My proudest achievement was implementing our distributed trace sampling system. It works exactly like how we opossums forage! When I'm searching for ticks\u2014my preferred protein source\u2014I don't meticulously inspect every square inch of my territory. Instead, I use adaptive sampling based on where I've had success before. def sample_trace ( trace_context ): # Smart sampling logic inspired by opossum foraging patterns if trace_context . has_error or trace_context . duration > HIGH_LATENCY_THRESHOLD : # Always capture problematic traces (just like I always eat ticks I find) return True elif trace_context . service in CRITICAL_SERVICES : # Sample 30% of important services (like prime foraging areas) return random . random () < 0.3 else : # Sample only 5% of normal traces (like routine territory checks) return random . random () < 0.05 When system anomalies occur, we no longer play dead and hope the problem resolves itself. Our new alerting system categorizes issues by severity, just like I categorize potential threats\u2014from \"harmless garden hose\" to \"aggressive neighborhood cat\" to \"PANIC! HUMAN WITH BROOM!\" The custom dashboards I designed maintain my lower body temperature preference of 95\u00b0F\u2014I mean, they run with lower CPU utilization while still showing key vitals. And just like how I can rotate my rear feet 180\u00b0 for climbing down trees head-first, our monitoring views can pivot instantly between service-level and system-wide perspectives! After implementing our new observability stack, mean time to detection dropped by 76%, almost as impressive as my ability to process 4,000 ticks per season! *self-satisfied chitter* Remember fellow engineers: in observability as in opossum life, it's not just about playing dead when things go wrong\u2014it's about having the senses to avoid danger in the first place! *vigilant tail curl* *nocturnal clicking sounds*","title":"Olivia's Observability Odyssey"},{"location":"features/background-stories/#penelopes-prompt-polishing-prowess","text":"Penelope \"Penny\" Promptwhisperer, 2-year-old Virginia Opossum, DSPy Optimization Specialist *soft clicking* *intense sniffing at the screen* You know, crafting the perfect prompt is like teaching a joey how to ask nicely for a grub instead of just hissing vaguely in the direction of the log pile. Before I joined the Opossum Search team, some of our internal prompts were... well, let's just say they were more hiss than helpful. *gentle sigh* My specialty? DSPy optimization! I took our basic Q&A pipeline, the one that uses the OpossumFactTool , and decided it needed some refinement. Manually tweaking prompts felt as inefficient as trying to carry 14 joeys without a pouch! So, I turned to BootstrapFewShot . I fed it examples from our opossum_dataset_converted.json \u2014good questions paired with good answers\u2014and let it work its magic. It was like showing the LM exactly how a well-behaved opossum answers questions: factually, concisely, and without unnecessary rambling about shiny objects. # My secret sauce - letting DSPy do the hard work! teleprompter = dspy . BootstrapFewShot ( metric = my_custom_opossum_metric , max_bootstrapped_demos = 3 ) optimized_qa_module = teleprompter . compile ( student = unoptimized_qa , trainset = training_data ) The results? Chef's kiss ... or, uh, opossum's appreciative sniff . The optimized prompts led to answers that were 30% more accurate according to my metrics (which mostly involve checking if the answer correctly states that opossums are immune to most snake venom). It was almost as satisfying as finding an unattended bowl of cat food! Now, our pipelines don't just answer questions; they answer them elegantly . It's the difference between a clumsy joey tumbling out of a tree and a graceful adult gliding down using its prehensile tail. Precision, efficiency, and a touch of marsupial magic! *satisfied tail curl* *resumes sniffing at the prompt output*","title":"Penelope's Prompt Polishing Prowess"},{"location":"features/background-stories/#implementation-note","text":"These fictional testimonials are included as a lighthearted nod to our marsupial namesake. They serve to illustrate Opossum Search features in an engaging way while acknowledging the project's playful side. While we wait for actual human user stories, we hope these tales provide both entertainment and insight into the system's capabilities. If you're an actual opossum using our system, please contact us\u2014we'd love to hear your story too! Note: No opossums were involved in the development of this software. Any resemblance to actual opossums, living or playing dead, is purely coincidental.","title":"Implementation Note"},{"location":"features/easter-eggs/","text":"Easter Eggs \u00b6 National Opossum Day (October 18) \u00b6 Every year on October 18, Opossum Search celebrates National Opossum Day with a special Easter egg that activates automatically for all users. Activation \u00b6 This Easter egg is time-triggered and automatically activates when: The date is October 18 (any year) Any search query is performed Features \u00b6 When activated, the following special features appear: Festive UI Elements The standard logo is replaced with a party hat-wearing opossum Search results are accompanied by small animated opossums that scurry across the screen Confetti animation plays on the first search of the day Special Responses Queries receive opossum-themed responses, regardless of the question The system adds \"playing possum\" jokes to responses Easter egg text appears in a special purple font Hidden Command: \"possum party\" Typing \"possum party\" triggers a full-screen animation of dancing opossums This command unlocks a temporary \"opossum mode\" for the remainder of the session In \"opossum mode\", all error messages are replaced with opossum memes Technical Implementation \u00b6 # app/features/easter_eggs.py def check_for_easter_eggs ( request_date , query ): \"\"\"Check if any Easter eggs should be activated\"\"\" # Check for National Opossum Day (October 18) if request_date . month == 10 and request_date . day == 18 : return { \"easter_egg\" : \"national_opossum_day\" , \"activate\" : True , \"ui_theme\" : \"party_opossum\" , \"response_modifiers\" : [ \"opossum_jokes\" , \"purple_text\" ], \"animations\" : [ \"confetti\" , \"scurrying_opossums\" ] } # Check for \"possum party\" command if query . lower () == \"possum party\" : return { \"easter_egg\" : \"possum_party\" , \"activate\" : True , \"special_mode\" : \"opossum_mode\" , \"animation\" : \"dancing_opossums\" , \"duration\" : \"session\" } return { \"activate\" : False } User Discovery \u00b6 This Easter egg is deliberately undocumented in the user-facing documentation to encourage discovery. However, subtle hints appear throughout the application starting on October 1: The 'O' in the Opossum Search logo subtly changes to have a small tail The loading animation occasionally shows a brief opossum silhouette Help documentation updated with the cryptic message \"Something special happens when opossums celebrate...\" History \u00b6 This Easter egg was added in version 0.8.3 (September 2024) and has been a beloved tradition ever since. Each year, new opossum animations and jokes are added to keep the experience fresh. Additional Easter Eggs \u00b6 Opossum Search contains several other Easter eggs: April Fool's Day Reversal - All search results are delivered in reverse order Developer Mode - Activated by the Konami code Hidden Terminal - A simulated terminal accessed through a specific key combination","title":"Easter Eggs"},{"location":"features/easter-eggs/#easter-eggs","text":"","title":"Easter Eggs"},{"location":"features/easter-eggs/#national-opossum-day-october-18","text":"Every year on October 18, Opossum Search celebrates National Opossum Day with a special Easter egg that activates automatically for all users.","title":"National Opossum Day (October 18)"},{"location":"features/easter-eggs/#activation","text":"This Easter egg is time-triggered and automatically activates when: The date is October 18 (any year) Any search query is performed","title":"Activation"},{"location":"features/easter-eggs/#features","text":"When activated, the following special features appear: Festive UI Elements The standard logo is replaced with a party hat-wearing opossum Search results are accompanied by small animated opossums that scurry across the screen Confetti animation plays on the first search of the day Special Responses Queries receive opossum-themed responses, regardless of the question The system adds \"playing possum\" jokes to responses Easter egg text appears in a special purple font Hidden Command: \"possum party\" Typing \"possum party\" triggers a full-screen animation of dancing opossums This command unlocks a temporary \"opossum mode\" for the remainder of the session In \"opossum mode\", all error messages are replaced with opossum memes","title":"Features"},{"location":"features/easter-eggs/#technical-implementation","text":"# app/features/easter_eggs.py def check_for_easter_eggs ( request_date , query ): \"\"\"Check if any Easter eggs should be activated\"\"\" # Check for National Opossum Day (October 18) if request_date . month == 10 and request_date . day == 18 : return { \"easter_egg\" : \"national_opossum_day\" , \"activate\" : True , \"ui_theme\" : \"party_opossum\" , \"response_modifiers\" : [ \"opossum_jokes\" , \"purple_text\" ], \"animations\" : [ \"confetti\" , \"scurrying_opossums\" ] } # Check for \"possum party\" command if query . lower () == \"possum party\" : return { \"easter_egg\" : \"possum_party\" , \"activate\" : True , \"special_mode\" : \"opossum_mode\" , \"animation\" : \"dancing_opossums\" , \"duration\" : \"session\" } return { \"activate\" : False }","title":"Technical Implementation"},{"location":"features/easter-eggs/#user-discovery","text":"This Easter egg is deliberately undocumented in the user-facing documentation to encourage discovery. However, subtle hints appear throughout the application starting on October 1: The 'O' in the Opossum Search logo subtly changes to have a small tail The loading animation occasionally shows a brief opossum silhouette Help documentation updated with the cryptic message \"Something special happens when opossums celebrate...\"","title":"User Discovery"},{"location":"features/easter-eggs/#history","text":"This Easter egg was added in version 0.8.3 (September 2024) and has been a beloved tradition ever since. Each year, new opossum animations and jokes are added to keep the experience fresh.","title":"History"},{"location":"features/easter-eggs/#additional-easter-eggs","text":"Opossum Search contains several other Easter eggs: April Fool's Day Reversal - All search results are delivered in reverse order Developer Mode - Activated by the Konami code Hidden Terminal - A simulated terminal accessed through a specific key combination","title":"Additional Easter Eggs"},{"location":"features/national-day/","text":"Opossum National Day \u00b6 Celebrating October 18th \u00b6 National Opossum Day is celebrated annually on October 18th, and Opossum Search embraces this occasion with special features, themed responses, and playful interactions across the platform. Special Features \u00b6 Automatic Activation \u00b6 On October 18th, Opossum Search automatically enters \"Celebration Mode\" with the following enhancements: Visual Elements \u00b6 Festive Background : The dynamic background text includes celebration-themed words and emoji Themed UI : Interface elements use a special color scheme inspired by opossum coloration Custom Animations : Chat responses appear with playful \"playing possum\" animations Day Recognition : A cookie-based system remembers if you've visited on National Opossum Day Functional Enhancements \u00b6 Special Response Formats : All chat responses include opossum facts and themed formatting Temporary SVG Visualizations : Special opossum-themed visualizations become available Easter Egg Hunt : Special hidden commands are activated only on this day Note: For detailed implementation specifics and code examples, please refer to the Easter Eggs documentation, which covers the technical aspects of all special event features. Special Commands \u00b6 The following special commands are only available on National Opossum Day: Command Description !opossum Triggers a special opossum animation in the background tell me about marsupial superheros Returns a creative story about \"Captain Opossum\" opossum facts Provides a list of all interesting opossum facts playing possum Activates the existing \"play dead\" animation opossum party Changes the background animation to a more celebratory pattern Implementation Details \u00b6 The National Day feature uses: localStorage to remember if a user has visited on October 18th CSS animations for visual effects Special GraphQL query parameters that activate only on the correct date Enhanced background text generation with celebration-themed content Opossum Facts \u00b6 During National Opossum Day, users learn interesting facts about opossums, including: Opossums have 50 teeth, more than any other North American mammal They are immune to most snake venom They have excellent memory, outperforming rats, rabbits, cats, and dogs in maze tests They have remarkable immune systems and rarely get sick A group of opossums is called a 'passel' Opossums consume up to 5,000 ticks per season, helping control Lyme disease They have opposable thumbs on their rear feet Baby opossums are called 'joeys', just like baby kangaroos Implementation Guidelines for Contributors \u00b6 If you're contributing to Opossum Search and want to enhance the National Opossum Day experience, follow these guidelines: All National Day features must be non-intrusive to normal operation Features should degrade gracefully if resources are constrained Keep special animations and visual elements subtle and professional Ensure all content is family-friendly and educational Use browser storage (localStorage or cookies) for remembering user participation","title":"National Opossum Day"},{"location":"features/national-day/#opossum-national-day","text":"","title":"Opossum National Day"},{"location":"features/national-day/#celebrating-october-18th","text":"National Opossum Day is celebrated annually on October 18th, and Opossum Search embraces this occasion with special features, themed responses, and playful interactions across the platform.","title":"Celebrating October 18th"},{"location":"features/national-day/#special-features","text":"","title":"Special Features"},{"location":"features/national-day/#automatic-activation","text":"On October 18th, Opossum Search automatically enters \"Celebration Mode\" with the following enhancements:","title":"Automatic Activation"},{"location":"features/national-day/#visual-elements","text":"Festive Background : The dynamic background text includes celebration-themed words and emoji Themed UI : Interface elements use a special color scheme inspired by opossum coloration Custom Animations : Chat responses appear with playful \"playing possum\" animations Day Recognition : A cookie-based system remembers if you've visited on National Opossum Day","title":"Visual Elements"},{"location":"features/national-day/#functional-enhancements","text":"Special Response Formats : All chat responses include opossum facts and themed formatting Temporary SVG Visualizations : Special opossum-themed visualizations become available Easter Egg Hunt : Special hidden commands are activated only on this day Note: For detailed implementation specifics and code examples, please refer to the Easter Eggs documentation, which covers the technical aspects of all special event features.","title":"Functional Enhancements"},{"location":"features/national-day/#special-commands","text":"The following special commands are only available on National Opossum Day: Command Description !opossum Triggers a special opossum animation in the background tell me about marsupial superheros Returns a creative story about \"Captain Opossum\" opossum facts Provides a list of all interesting opossum facts playing possum Activates the existing \"play dead\" animation opossum party Changes the background animation to a more celebratory pattern","title":"Special Commands"},{"location":"features/national-day/#implementation-details","text":"The National Day feature uses: localStorage to remember if a user has visited on October 18th CSS animations for visual effects Special GraphQL query parameters that activate only on the correct date Enhanced background text generation with celebration-themed content","title":"Implementation Details"},{"location":"features/national-day/#opossum-facts","text":"During National Opossum Day, users learn interesting facts about opossums, including: Opossums have 50 teeth, more than any other North American mammal They are immune to most snake venom They have excellent memory, outperforming rats, rabbits, cats, and dogs in maze tests They have remarkable immune systems and rarely get sick A group of opossums is called a 'passel' Opossums consume up to 5,000 ticks per season, helping control Lyme disease They have opposable thumbs on their rear feet Baby opossums are called 'joeys', just like baby kangaroos","title":"Opossum Facts"},{"location":"features/national-day/#implementation-guidelines-for-contributors","text":"If you're contributing to Opossum Search and want to enhance the National Opossum Day experience, follow these guidelines: All National Day features must be non-intrusive to normal operation Features should degrade gracefully if resources are constrained Keep special animations and visual elements subtle and professional Ensure all content is family-friendly and educational Use browser storage (localStorage or cookies) for remembering user participation","title":"Implementation Guidelines for Contributors"},{"location":"features/service-vizualizations/","text":"Service Visualizations \u00b6 Overview \u00b6 Opossum Search provides dynamic visualizations of system services, performance metrics, and operational status through its SVG generation capabilities. These visualizations help users and administrators understand system behavior, monitor performance, and diagnose issues through intuitive graphical representations. Visualization Types \u00b6 System Status Dashboard \u00b6 The System Status Dashboard provides a real-time overview of all Opossum Search services: Service Health Indicators : Color-coded status indicators for each component Response Time Metrics : Visual representation of current and historical response times Resource Utilization : CPU, memory, and network usage visualizations Error Rate Tracking : Error frequency and distribution charts query SystemStatusDashboard { systemStatus { serviceHealth { name status uptime lastIncident } performanceMetrics { responseTime throughput errorRate } resourceUtilization { cpu memory network } } } Model Selection Flow \u00b6 Visualizes the dynamic model selection process in real-time: Decision Tree Visualization : Shows how queries are routed to different models Capability Matching : Highlights which capabilities influenced model selection Fallback Chains : Visualizes primary and fallback paths for resilience Performance Comparison : Compares response times across different models query ModelSelectionFlow ( $query : String !) { modelSelection ( query : $query ) { visualizationSvg decisionPath { step criteriaEvaluated result } selectedModel { name capabilities responseTime } fallbackOptions { name priority } } } Caching Efficiency Map \u00b6 Provides insights into the multi-level caching system: Cache Hit/Miss Visualization : Visual representation of cache efficiency TTL Heatmap : Shows expiration patterns across different cache types Memory Usage Diagram : Displays Redis memory allocation by key type Invalidation Patterns : Visualizes cache invalidation frequency and triggers Request Journey Timeline \u00b6 Traces the complete lifecycle of a request through the system: Component Interaction Flow : Shows how a request moves through services Timing Breakdown : Visual representation of time spent in each component Bottleneck Identification : Highlights potential performance bottlenecks Parallel Processing Visualization : Shows concurrent operations Accessing Visualizations \u00b6 GraphQL Endpoint \u00b6 Visualizations are available through the GraphQL API using the visualization field: query ServiceVisualization ( $type : VisualizationType !, $parameters : VisualizationParameters ) { visualization ( type : $type , parameters : $parameters ) { svg interactiveElements { id type action } dataPoints { label value timestamp } metadata { generatedAt dataFreshness } } } Web Dashboard \u00b6 All visualizations are accessible through the administrative dashboard at /admin/visualizations : Filter visualizations by type, timeframe, and components Save custom visualization configurations Export SVGs for documentation or reporting Schedule automatic visualization generation Implementation Details \u00b6 SVG Generation Process \u00b6 graph TD A[Data Collection] --> B[Data Processing] B --> C[Template Selection] C --> D[SVG Generation] D --> E[Interactive Element Addition] E --> F[Optimization] F --> G[Delivery] classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,D,G primary; class B,C,E,F secondary; Data Collection : Telemetry data is gathered from OpenTelemetry and internal metrics Processing : Raw data is transformed into visualization-ready format Template Selection : Appropriate SVG template is chosen based on visualization type SVG Generation : Dynamic SVG is created with current data values Interactive Elements : Tooltips, clickable regions, and other interactive features are added Optimization : SVG is optimized for size and rendering performance Delivery : Final SVG is delivered via API or embedded in dashboard Rendering Engine \u00b6 Visualizations use a custom SVG rendering engine built on: Data-driven templates : Base SVG templates with data binding points Dynamic scaling : Automatic adjustment to display size Theme support : Light/dark mode compatibility Accessibility features : ARIA attributes and keyboard navigation Animation capabilities : Smooth transitions between data states Examples \u00b6 System Health Overview \u00b6 The system health visualization provides an at-a-glance view of all services: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Opossum Search System Health \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 API Gateway \u2502 Model Svc \u2502 Redis Cache \u2502 \u2502 \u25cf HEALTHY \u2502 \u25cf HEALTHY \u2502 \u25cf HEALTHY \u2502 \u2502 99.9% up \u2502 99.8% up \u2502 100% up \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Image Proc \u2502 GraphQL \u2502 Auth Service\u2502 \u2502 \u25cf HEALTHY \u2502 \u25cf DEGRADED \u2502 \u25cf HEALTHY \u2502 \u2502 99.7% up \u2502 97.2% up \u2502 99.9% up \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Last updated: 2025-03-23 14:32:05 UTC Response Time Histogram \u00b6 Visualizes response time distribution across percentiles: Response Time Distribution (ms) \u2502 900\u2524 \u2588 800\u2524 \u2588 700\u2524 \u2588 600\u2524 \u2584\u2584\u2588 500\u2524 \u2588\u2588\u2588 400\u2524 \u2584\u2584\u2588\u2588\u2588 300\u2524 \u2584\u2584\u2588\u2588\u2588\u2588\u2588 200\u2524 \u2584\u2584\u2584\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588 100\u2524 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 50 75 90 95 99 99.9 Percentile Customization Options \u00b6 Visualizations can be customized through the API: Time Range : Adjust the period for time-series data Color Scheme : Select from predefined themes or custom palettes Granularity : Control data resolution and aggregation level Components : Filter which services or components to include Layout : Choose between compact, detailed, or custom layouts Technical Requirements \u00b6 Browser Support : Modern browsers with SVG support (Chrome, Firefox, Safari, Edge) Minimum Resolution : 800x600 for optimal viewing Data Freshness : Visualizations reflect data no older than 60 seconds Rendering Performance : SVGs optimized to render in under 500ms Future Enhancements \u00b6 Upcoming visualization features include: 3D Service Maps : Three-dimensional visualization of service interactions Animated Traffic Flows : Visual representation of request volume between components Predictive Scaling Visualizations : Forecasting visualizations for resource needs Comparative Performance Views : Side-by-side comparison of different deployment configurations Service visualizations are designed to provide intuitive understanding of complex system behavior, helping both developers and administrators maintain optimal performance and quickly diagnose issues when they arise.","title":"Service Visualizations"},{"location":"features/service-vizualizations/#service-visualizations","text":"","title":"Service Visualizations"},{"location":"features/service-vizualizations/#overview","text":"Opossum Search provides dynamic visualizations of system services, performance metrics, and operational status through its SVG generation capabilities. These visualizations help users and administrators understand system behavior, monitor performance, and diagnose issues through intuitive graphical representations.","title":"Overview"},{"location":"features/service-vizualizations/#visualization-types","text":"","title":"Visualization Types"},{"location":"features/service-vizualizations/#system-status-dashboard","text":"The System Status Dashboard provides a real-time overview of all Opossum Search services: Service Health Indicators : Color-coded status indicators for each component Response Time Metrics : Visual representation of current and historical response times Resource Utilization : CPU, memory, and network usage visualizations Error Rate Tracking : Error frequency and distribution charts query SystemStatusDashboard { systemStatus { serviceHealth { name status uptime lastIncident } performanceMetrics { responseTime throughput errorRate } resourceUtilization { cpu memory network } } }","title":"System Status Dashboard"},{"location":"features/service-vizualizations/#model-selection-flow","text":"Visualizes the dynamic model selection process in real-time: Decision Tree Visualization : Shows how queries are routed to different models Capability Matching : Highlights which capabilities influenced model selection Fallback Chains : Visualizes primary and fallback paths for resilience Performance Comparison : Compares response times across different models query ModelSelectionFlow ( $query : String !) { modelSelection ( query : $query ) { visualizationSvg decisionPath { step criteriaEvaluated result } selectedModel { name capabilities responseTime } fallbackOptions { name priority } } }","title":"Model Selection Flow"},{"location":"features/service-vizualizations/#caching-efficiency-map","text":"Provides insights into the multi-level caching system: Cache Hit/Miss Visualization : Visual representation of cache efficiency TTL Heatmap : Shows expiration patterns across different cache types Memory Usage Diagram : Displays Redis memory allocation by key type Invalidation Patterns : Visualizes cache invalidation frequency and triggers","title":"Caching Efficiency Map"},{"location":"features/service-vizualizations/#request-journey-timeline","text":"Traces the complete lifecycle of a request through the system: Component Interaction Flow : Shows how a request moves through services Timing Breakdown : Visual representation of time spent in each component Bottleneck Identification : Highlights potential performance bottlenecks Parallel Processing Visualization : Shows concurrent operations","title":"Request Journey Timeline"},{"location":"features/service-vizualizations/#accessing-visualizations","text":"","title":"Accessing Visualizations"},{"location":"features/service-vizualizations/#graphql-endpoint","text":"Visualizations are available through the GraphQL API using the visualization field: query ServiceVisualization ( $type : VisualizationType !, $parameters : VisualizationParameters ) { visualization ( type : $type , parameters : $parameters ) { svg interactiveElements { id type action } dataPoints { label value timestamp } metadata { generatedAt dataFreshness } } }","title":"GraphQL Endpoint"},{"location":"features/service-vizualizations/#web-dashboard","text":"All visualizations are accessible through the administrative dashboard at /admin/visualizations : Filter visualizations by type, timeframe, and components Save custom visualization configurations Export SVGs for documentation or reporting Schedule automatic visualization generation","title":"Web Dashboard"},{"location":"features/service-vizualizations/#implementation-details","text":"","title":"Implementation Details"},{"location":"features/service-vizualizations/#svg-generation-process","text":"graph TD A[Data Collection] --> B[Data Processing] B --> C[Template Selection] C --> D[SVG Generation] D --> E[Interactive Element Addition] E --> F[Optimization] F --> G[Delivery] classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,D,G primary; class B,C,E,F secondary; Data Collection : Telemetry data is gathered from OpenTelemetry and internal metrics Processing : Raw data is transformed into visualization-ready format Template Selection : Appropriate SVG template is chosen based on visualization type SVG Generation : Dynamic SVG is created with current data values Interactive Elements : Tooltips, clickable regions, and other interactive features are added Optimization : SVG is optimized for size and rendering performance Delivery : Final SVG is delivered via API or embedded in dashboard","title":"SVG Generation Process"},{"location":"features/service-vizualizations/#rendering-engine","text":"Visualizations use a custom SVG rendering engine built on: Data-driven templates : Base SVG templates with data binding points Dynamic scaling : Automatic adjustment to display size Theme support : Light/dark mode compatibility Accessibility features : ARIA attributes and keyboard navigation Animation capabilities : Smooth transitions between data states","title":"Rendering Engine"},{"location":"features/service-vizualizations/#examples","text":"","title":"Examples"},{"location":"features/service-vizualizations/#system-health-overview","text":"The system health visualization provides an at-a-glance view of all services: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Opossum Search System Health \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 API Gateway \u2502 Model Svc \u2502 Redis Cache \u2502 \u2502 \u25cf HEALTHY \u2502 \u25cf HEALTHY \u2502 \u25cf HEALTHY \u2502 \u2502 99.9% up \u2502 99.8% up \u2502 100% up \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Image Proc \u2502 GraphQL \u2502 Auth Service\u2502 \u2502 \u25cf HEALTHY \u2502 \u25cf DEGRADED \u2502 \u25cf HEALTHY \u2502 \u2502 99.7% up \u2502 97.2% up \u2502 99.9% up \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Last updated: 2025-03-23 14:32:05 UTC","title":"System Health Overview"},{"location":"features/service-vizualizations/#response-time-histogram","text":"Visualizes response time distribution across percentiles: Response Time Distribution (ms) \u2502 900\u2524 \u2588 800\u2524 \u2588 700\u2524 \u2588 600\u2524 \u2584\u2584\u2588 500\u2524 \u2588\u2588\u2588 400\u2524 \u2584\u2584\u2588\u2588\u2588 300\u2524 \u2584\u2584\u2588\u2588\u2588\u2588\u2588 200\u2524 \u2584\u2584\u2584\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588 100\u2524 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 50 75 90 95 99 99.9 Percentile","title":"Response Time Histogram"},{"location":"features/service-vizualizations/#customization-options","text":"Visualizations can be customized through the API: Time Range : Adjust the period for time-series data Color Scheme : Select from predefined themes or custom palettes Granularity : Control data resolution and aggregation level Components : Filter which services or components to include Layout : Choose between compact, detailed, or custom layouts","title":"Customization Options"},{"location":"features/service-vizualizations/#technical-requirements","text":"Browser Support : Modern browsers with SVG support (Chrome, Firefox, Safari, Edge) Minimum Resolution : 800x600 for optimal viewing Data Freshness : Visualizations reflect data no older than 60 seconds Rendering Performance : SVGs optimized to render in under 500ms","title":"Technical Requirements"},{"location":"features/service-vizualizations/#future-enhancements","text":"Upcoming visualization features include: 3D Service Maps : Three-dimensional visualization of service interactions Animated Traffic Flows : Visual representation of request volume between components Predictive Scaling Visualizations : Forecasting visualizations for resource needs Comparative Performance Views : Side-by-side comparison of different deployment configurations Service visualizations are designed to provide intuitive understanding of complex system behavior, helping both developers and administrators maintain optimal performance and quickly diagnose issues when they arise.","title":"Future Enhancements"},{"location":"features/special-events/","text":"Special Events \u00b6 Note: These special events are planned features for future implementation. This document outlines the conceptual design and implementation possibilities. Seasonal Events \u00b6 Opossum Search celebrates various occasions throughout the year with temporary features and themed experiences. These events provide moments of delight for users while showcasing the system's flexibility. April Fool's Day (April 1) \u00b6 For 24 hours on April 1st, Opossum Search implements harmless pranks that surprise and amuse users: Reversed Results : Search results appear in reverse order Playful Responses : The system occasionally responds with intentionally silly or overly literal interpretations of queries Word Scramble : Some words in responses are comically scrambled \"Did you actually mean...\" : The system suggests absurd alternatives to simple search terms All April Fool's features can be disabled with the command no fooling for users who prefer standard functionality. Winter Holidays (December 20-31) \u00b6 During the winter holiday season, Opossum Search presents a festive experience: Snowy Animations : Gentle snowflakes fall in the background of the interface Decorated UI : Subtle holiday decorations appear in UI elements Warm Color Scheme : The interface shifts to warmer colors Holiday Greetings : Responses include seasonal well-wishes Gift Recommendations : Enhanced capabilities for gift suggestion queries System Birthday (June 15) \u00b6 Celebrating the anniversary of the first commit, Opossum Search's birthday includes: Development Statistics : Users can see fun facts about the system's growth \"Year in Review\" Visualization : A special SVG showing the system's journey Thank You Messages : Special acknowledgments to users and contributors Performance Boost : Temporary increase in resource allocation for faster responses Technical Community Events \u00b6 Programmer's Day (September 13/12) \u00b6 On the 256th day of the year, Opossum Search celebrates programmers with: Code-Friendly Responses : Enhanced code formatting and syntax highlighting Programming Jokes : Tech humor in responses Binary Easter Egg : Typing \"01101000 01101001\" (binary for \"hi\") triggers a special greeting Famous Algorithm Visualizations : Special SVG generation for classic algorithms System Administrator Appreciation Day (Last Friday in July) \u00b6 Honoring the unsung heroes of IT: Uptime Celebration : Special visualization showing system stability metrics IT Humor : Responses include classic sysadmin jokes and references Infrastructure Visualizations : Special diagrams of the Opossum Search architecture \"Have you tried turning it off and on again?\" : Easter egg response to certain troubleshooting queries Implementing Special Events \u00b6 Special events in Opossum Search follow these implementation principles: # Conceptual implementation approach def check_special_events ( request_date ): \"\"\"Check for active special events based on date\"\"\" events = [] # April Fool's Day if request_date . month == 4 and request_date . day == 1 : events . append ( \"april_fools\" ) # Winter Holidays if request_date . month == 12 and 20 <= request_date . day <= 31 : events . append ( \"winter_holidays\" ) # System Birthday if request_date . month == 6 and request_date . day == 15 : events . append ( \"system_birthday\" ) # Programmer's Day (256th day of the year) day_of_year = request_date . timetuple () . tm_yday if day_of_year == 256 : events . append ( \"programmers_day\" ) # System Administrator Appreciation Day (last Friday in July) if request_date . month == 7 : last_day = calendar . monthrange ( request_date . year , 7 )[ 1 ] last_friday = last_day - calendar . weekday ( request_date . year , 7 , last_day ) if request_date . day == last_friday : events . append ( \"sysadmin_day\" ) return events Feature Flags \u00b6 Each special event is controlled by a feature flag that can be globally enabled or disabled: # Example configuration special_events : april_fools : enabled : true intensity : medium # Options: subtle, medium, obvious winter_holidays : enabled : true intensity : subtle system_birthday : enabled : true intensity : medium programmers_day : enabled : true intensity : subtle sysadmin_day : enabled : true intensity : medium Event Assets \u00b6 Special events use dedicated asset collections: Themed SVG templates Special animation sequences Event-specific response modifiers Temporary UI element replacements Contributor Guidelines \u00b6 Proposing new special events: Events should be universally appealing and non-controversial Implementation should be lightweight with minimal performance impact All features must degrade gracefully if resources are constrained Events should have an opt-out mechanism for users who prefer standard functionality Plan for accessibility considerations with all visual or interactive elements Future Event Concepts \u00b6 We're considering the following events for future implementation: Pi Day (March 14): Mathematical visualizations and numerically-themed responses Earth Day (April 22): Environmentally focused visualizations and energy-efficiency metrics International Talk Like a Pirate Day (September 19): Arrr! Ye responses be in pirate speak! Halloween (October 31): Subtle spooky themes and \"trick or treat\" special command Interested in contributing to a special event implementation? Check our contributing guidelines and join the conversation!","title":"Special Events"},{"location":"features/special-events/#special-events","text":"Note: These special events are planned features for future implementation. This document outlines the conceptual design and implementation possibilities.","title":"Special Events"},{"location":"features/special-events/#seasonal-events","text":"Opossum Search celebrates various occasions throughout the year with temporary features and themed experiences. These events provide moments of delight for users while showcasing the system's flexibility.","title":"Seasonal Events"},{"location":"features/special-events/#april-fools-day-april-1","text":"For 24 hours on April 1st, Opossum Search implements harmless pranks that surprise and amuse users: Reversed Results : Search results appear in reverse order Playful Responses : The system occasionally responds with intentionally silly or overly literal interpretations of queries Word Scramble : Some words in responses are comically scrambled \"Did you actually mean...\" : The system suggests absurd alternatives to simple search terms All April Fool's features can be disabled with the command no fooling for users who prefer standard functionality.","title":"April Fool's Day (April 1)"},{"location":"features/special-events/#winter-holidays-december-20-31","text":"During the winter holiday season, Opossum Search presents a festive experience: Snowy Animations : Gentle snowflakes fall in the background of the interface Decorated UI : Subtle holiday decorations appear in UI elements Warm Color Scheme : The interface shifts to warmer colors Holiday Greetings : Responses include seasonal well-wishes Gift Recommendations : Enhanced capabilities for gift suggestion queries","title":"Winter Holidays (December 20-31)"},{"location":"features/special-events/#system-birthday-june-15","text":"Celebrating the anniversary of the first commit, Opossum Search's birthday includes: Development Statistics : Users can see fun facts about the system's growth \"Year in Review\" Visualization : A special SVG showing the system's journey Thank You Messages : Special acknowledgments to users and contributors Performance Boost : Temporary increase in resource allocation for faster responses","title":"System Birthday (June 15)"},{"location":"features/special-events/#technical-community-events","text":"","title":"Technical Community Events"},{"location":"features/special-events/#programmers-day-september-1312","text":"On the 256th day of the year, Opossum Search celebrates programmers with: Code-Friendly Responses : Enhanced code formatting and syntax highlighting Programming Jokes : Tech humor in responses Binary Easter Egg : Typing \"01101000 01101001\" (binary for \"hi\") triggers a special greeting Famous Algorithm Visualizations : Special SVG generation for classic algorithms","title":"Programmer's Day (September 13/12)"},{"location":"features/special-events/#system-administrator-appreciation-day-last-friday-in-july","text":"Honoring the unsung heroes of IT: Uptime Celebration : Special visualization showing system stability metrics IT Humor : Responses include classic sysadmin jokes and references Infrastructure Visualizations : Special diagrams of the Opossum Search architecture \"Have you tried turning it off and on again?\" : Easter egg response to certain troubleshooting queries","title":"System Administrator Appreciation Day (Last Friday in July)"},{"location":"features/special-events/#implementing-special-events","text":"Special events in Opossum Search follow these implementation principles: # Conceptual implementation approach def check_special_events ( request_date ): \"\"\"Check for active special events based on date\"\"\" events = [] # April Fool's Day if request_date . month == 4 and request_date . day == 1 : events . append ( \"april_fools\" ) # Winter Holidays if request_date . month == 12 and 20 <= request_date . day <= 31 : events . append ( \"winter_holidays\" ) # System Birthday if request_date . month == 6 and request_date . day == 15 : events . append ( \"system_birthday\" ) # Programmer's Day (256th day of the year) day_of_year = request_date . timetuple () . tm_yday if day_of_year == 256 : events . append ( \"programmers_day\" ) # System Administrator Appreciation Day (last Friday in July) if request_date . month == 7 : last_day = calendar . monthrange ( request_date . year , 7 )[ 1 ] last_friday = last_day - calendar . weekday ( request_date . year , 7 , last_day ) if request_date . day == last_friday : events . append ( \"sysadmin_day\" ) return events","title":"Implementing Special Events"},{"location":"features/special-events/#feature-flags","text":"Each special event is controlled by a feature flag that can be globally enabled or disabled: # Example configuration special_events : april_fools : enabled : true intensity : medium # Options: subtle, medium, obvious winter_holidays : enabled : true intensity : subtle system_birthday : enabled : true intensity : medium programmers_day : enabled : true intensity : subtle sysadmin_day : enabled : true intensity : medium","title":"Feature Flags"},{"location":"features/special-events/#event-assets","text":"Special events use dedicated asset collections: Themed SVG templates Special animation sequences Event-specific response modifiers Temporary UI element replacements","title":"Event Assets"},{"location":"features/special-events/#contributor-guidelines","text":"Proposing new special events: Events should be universally appealing and non-controversial Implementation should be lightweight with minimal performance impact All features must degrade gracefully if resources are constrained Events should have an opt-out mechanism for users who prefer standard functionality Plan for accessibility considerations with all visual or interactive elements","title":"Contributor Guidelines"},{"location":"features/special-events/#future-event-concepts","text":"We're considering the following events for future implementation: Pi Day (March 14): Mathematical visualizations and numerically-themed responses Earth Day (April 22): Environmentally focused visualizations and energy-efficiency metrics International Talk Like a Pirate Day (September 19): Arrr! Ye responses be in pirate speak! Halloween (October 31): Subtle spooky themes and \"trick or treat\" special command Interested in contributing to a special event implementation? Check our contributing guidelines and join the conversation!","title":"Future Event Concepts"},{"location":"image-processing/caching/","text":"Caching Strategy \u00b6 Overview \u00b6 The Opossum Search image processing system implements a sophisticated multi-tiered caching architecture to optimize performance, reduce resource consumption, and improve response times. This document details the caching strategies used throughout the image processing pipeline. Note Effective caching is crucial for maintaining high performance in image processing. This document outlines the caching architecture and strategies used in Opossum Search. Caching Architecture \u00b6 The caching system employs a layered approach to maximize efficiency across different types of operations and data: graph TD A[Client Request] --> B{Cache Hit?} B -->|Yes| C[Return Cached Result] B -->|No| D[Process Request] D --> E[Store in Cache] E --> F[Return Result] subgraph \"Cache Layers\" G[Memory Cache] H[Redis Cache] I[Disk Cache] J[CDN Cache] end B -.-> G B -.-> H B -.-> I B -.-> J E -.-> G E -.-> H E -.-> I E -.-> J Cache Tiers \u00b6 Cache Tier Storage Medium Access Speed Capacity Persistence Use Case L1: Memory RAM <1ms Limited None Frequently accessed small results L2: Redis Redis Cluster 1-5ms Medium Configurable Shared results across instances L3: Disk Local SSD 5-20ms Large Full Larger results, backup layer L4: CDN Distributed Variable Very Large Extended Public, static results Cache Categories \u00b6 The system implements specialized caching for different types of processed content: 1. Result Caching \u00b6 Stores complete processing results based on input parameters: def get_processed_image ( image_id , operations , options ): # Generate cache key from parameters cache_key = generate_cache_key ( image_id , operations , options ) # Check if result exists in cache cached_result = cache . get ( cache_key ) if cached_result : return cached_result # Process image if not in cache result = process_image ( image_id , operations , options ) # Store in cache with appropriate TTL cache . set ( key = cache_key , value = result , ttl = calculate_ttl ( operations , options ) ) return result 2. Intermediate Result Caching \u00b6 Caches partial results to optimize multi-step processing chains: def apply_filter_chain ( image , filters ): result = image last_cached_state = None for i , filter_op in enumerate ( filters ): # Generate intermediate cache key intermediate_key = f \" { image . id } :step { i } : { filter_op . hash } \" # Try to retrieve from cache cached_state = cache . get ( intermediate_key ) if cached_state : result = cached_state last_cached_state = i continue # Apply filter and cache result result = apply_filter ( result , filter_op ) cache . set ( key = intermediate_key , value = result , ttl = INTERMEDIATE_CACHE_TTL ) return result 3. Resource Caching \u00b6 Caches frequently used resources like filters, presets, and templates: def get_filter_preset ( preset_name , version = None ): # Generate cache key version = version or CURRENT_VERSION cache_key = f \"preset: { preset_name } :v { version } \" # Check cache cached_preset = resource_cache . get ( cache_key ) if cached_preset : return cached_preset # Load preset from storage preset = load_preset_from_storage ( preset_name , version ) # Cache with long TTL resource_cache . set ( key = cache_key , value = preset , ttl = RESOURCE_CACHE_TTL # Typically 24+ hours ) return preset 4. Thumbnail Caching \u00b6 Specialized caching for generated thumbnails and previews: def get_thumbnail ( image_id , width , height , format = \"webp\" ): cache_key = f \"thumb: { image_id } : { width } x { height } : { format } \" # Check cache cached_thumb = thumbnail_cache . get ( cache_key ) if cached_thumb : return cached_thumb # Generate thumbnail original = load_image ( image_id ) thumbnail = resize_image ( original , width , height ) if format != original . format : thumbnail = convert_format ( thumbnail , format ) # Cache with relatively long TTL thumbnail_cache . set ( key = cache_key , value = thumbnail , ttl = THUMBNAIL_CACHE_TTL # Typically 7+ days ) return thumbnail Cache Key Generation \u00b6 The system uses deterministic cache key generation to ensure consistency: def generate_cache_key ( image_id , operations , options ): \"\"\"Generate a deterministic cache key for image operations\"\"\" # Base components components = [ f \"img: { image_id } \" , f \"v: { IMAGE_PROCESSING_VERSION } \" ] # Add normalized operation signatures for op in operations : op_signature = normalize_operation ( op ) components . append ( f \"op: { op_signature } \" ) # Add normalized options for key , value in sorted ( options . items ()): components . append ( f \"opt: { key } : { normalize_value ( value ) } \" ) # Generate hash for the full key full_key = \":\" . join ( components ) hashed_key = hashlib . sha256 ( full_key . encode ()) . hexdigest () return f \"img_proc: { hashed_key } \" TTL Strategies \u00b6 Different cache items have different time-to-live (TTL) values based on their characteristics: Content Type Default TTL Rationale Final Results 24 hours Balance between freshness and efficiency Intermediate Results 1 hour Temporary computation savings Thumbnails 7 days Rarely change, high request frequency Resources 24-48 hours Static content with version checks Error Results 5 minutes Prevent repeated processing of problematic inputs TTLs are dynamically adjusted based on: Resource constraints (lower TTLs when storage is limited) Usage patterns (longer TTLs for frequently accessed items) Content type (images vs. generated SVGs) Processing complexity (longer TTLs for expensive operations) Cache Invalidation Strategies \u00b6 Time-Based Invalidation \u00b6 The primary invalidation mechanism is time-based expiration through TTLs. Explicit Invalidation \u00b6 def invalidate_image_caches ( image_id ): \"\"\"Invalidate all caches related to a specific image\"\"\" # Pattern-based invalidation in Redis redis_client . delete_by_pattern ( f \"img_proc:*:img: { image_id } :*\" ) # Clear from memory cache memory_cache . delete_by_pattern ( f \"* { image_id } *\" ) # Mark for disk cache cleanup disk_cache . mark_for_cleanup ( image_id ) # Purge from CDN if applicable if cdn_enabled : cdn_client . purge_object ( f \"images/ { image_id } /*\" ) Version-Based Invalidation \u00b6 Cache keys include version information to automatically invalidate when: Processing algorithms change New filters are added Quality improvements are implemented # When the algorithm version changes IMAGE_PROCESSING_VERSION = \"2.5.0\" # Included in all cache keys LRU/LFU Policies \u00b6 Different cache tiers implement appropriate eviction policies: Cache Tier Primary Policy Secondary Policy Memory LRU (Least Recently Used) Size limit Redis LFU (Least Frequently Used) TTL Disk FIFO with size limits Manual purge CDN TTL None Performance Metrics \u00b6 Cache Hit Rates \u00b6 Cache Type Target Hit Rate Observed Hit Rate Result Cache >60% 65-75% Intermediate Cache >40% 45-55% Thumbnail Cache >80% 85-95% Resource Cache >95% 98-99% Latency Improvements \u00b6 Operation Uncached Latency Cached Latency Improvement Basic Filter 100-200ms 5-15ms 93-95% Filter Chain 300-800ms 10-30ms 96-97% SVG Generation 500-2000ms 15-50ms 97-98% Image Analysis 1000-3000ms 20-70ms 98-99% Cache Storage Sizing \u00b6 Guidelines for allocating cache storage: Traffic Level Memory Cache Redis Cache Disk Cache Low (<100k req/day) 1-2GB 5-10GB 20-50GB Medium (100k-1M req/day) 4-8GB 20-50GB 100-200GB High (1M+ req/day) 16-32GB 100-200GB 500GB-1TB Integration with Redis \u00b6 The Redis cache implementation leverages specialized data structures: class RedisCacheManager : def __init__ ( self , redis_client , namespace = \"img_proc\" ): self . redis = redis_client self . namespace = namespace def get ( self , key ): \"\"\"Retrieve an item from cache\"\"\" full_key = f \" { self . namespace } : { key } \" # Get value and metadata pipeline = self . redis . pipeline () pipeline . get ( full_key ) pipeline . hgetall ( f \" { full_key } :meta\" ) value , metadata = pipeline . execute () if not value : return None # Update access metrics self . redis . hincrby ( f \" { full_key } :meta\" , \"access_count\" , 1 ) self . redis . hset ( f \" { full_key } :meta\" , \"last_access\" , time . time ()) # Deserialize value return pickle . loads ( value ) def set ( self , key , value , ttl = 86400 ): \"\"\"Store an item in cache with metadata\"\"\" full_key = f \" { self . namespace } : { key } \" # Serialize value serialized = pickle . dumps ( value ) # Store value and metadata pipeline = self . redis . pipeline () pipeline . setex ( full_key , ttl , serialized ) # Store metadata for analytics and management pipeline . hmset ( f \" { full_key } :meta\" , { \"created_at\" : time . time (), \"size_bytes\" : len ( serialized ), \"ttl\" : ttl , \"access_count\" : 0 , \"last_access\" : time . time () }) pipeline . expire ( f \" { full_key } :meta\" , ttl ) pipeline . execute () def delete_by_pattern ( self , pattern ): \"\"\"Delete all keys matching a pattern\"\"\" full_pattern = f \" { self . namespace } : { pattern } \" # Find all matching keys keys = self . redis . keys ( full_pattern ) meta_keys = [ f \" { key } :meta\" for key in keys ] # Delete keys and metadata if keys : pipeline = self . redis . pipeline () pipeline . delete ( * keys ) if meta_keys : pipeline . delete ( * meta_keys ) pipeline . execute () Cache Warming Strategies \u00b6 The system implements proactive cache warming to improve performance: Predictive Warming : Analyzes usage patterns to pre-cache likely-to-be-requested variants Background Warming : Fills caches during low-traffic periods Related Content Warming : When one variant is requested, preemptively caches related variants def warm_cache_for_popular_images ( top_n = 100 ): \"\"\"Warm cache for most popular images with common operations\"\"\" # Get most frequently accessed images popular_images = analytics . get_top_images ( limit = top_n ) # Common operations to pre-cache common_operations = [ { \"type\" : \"resize\" , \"width\" : 800 , \"height\" : 600 }, { \"type\" : \"thumbnail\" , \"size\" : 200 }, { \"type\" : \"grayscale\" }, # More common operations ] # Queue cache warming tasks for image_id in popular_images : for operation in common_operations : task_queue . enqueue ( warm_image_cache , image_id = image_id , operation = operation , priority = \"low\" # Don't interfere with user requests ) Cache Analytics and Monitoring \u00b6 The system collects detailed cache performance metrics: class CacheAnalytics : def collect_metrics ( self ): \"\"\"Collect and report cache performance metrics\"\"\" metrics = { \"memory_cache\" : { \"size\" : memory_cache . size (), \"item_count\" : memory_cache . count (), \"hit_rate\" : memory_cache . hit_rate (), \"miss_rate\" : memory_cache . miss_rate (), \"eviction_rate\" : memory_cache . eviction_rate (), \"avg_ttl\" : memory_cache . avg_ttl () }, \"redis_cache\" : { \"size\" : redis_cache . size (), \"item_count\" : redis_cache . count (), \"hit_rate\" : redis_cache . hit_rate (), \"miss_rate\" : redis_cache . miss_rate (), \"eviction_rate\" : redis_cache . eviction_rate (), \"avg_ttl\" : redis_cache . avg_ttl (), \"memory_usage\" : redis_cache . memory_usage () }, # Similar metrics for other cache tiers } # Report to monitoring system monitoring . report_metrics ( \"cache_performance\" , metrics ) # Log periodic summary logger . info ( f \"Cache performance summary: hit_rate= { metrics [ 'redis_cache' ][ 'hit_rate' ] } %, \" f \"size= { metrics [ 'redis_cache' ][ 'size' ] / 1024 / 1024 : .2f } MB\" ) Configuration Options \u00b6 The caching system is highly configurable: # Example configuration caching : # Global settings enabled : true log_level : \"info\" # Memory cache memory_cache : enabled : true max_size : \"2GB\" ttl : 600 # seconds # Redis cache redis_cache : enabled : true host : \"redis.example.com\" port : 6379 db : 0 ttl : 86400 # seconds max_memory : \"50GB\" eviction_policy : \"lfu\" # Disk cache disk_cache : enabled : true path : \"/var/cache/opossum_images\" max_size : \"200GB\" cleanup_interval : 3600 # seconds Tip Properly configuring the caching system is essential for achieving optimal performance. Adjust the cache sizes, TTLs, and eviction policies based on your specific workload and resource constraints. Related Documentation \u00b6 Image Processing Overview Effects and Filters SVG Generation Performance Optimization Infrastructure Optimization Strategies","title":"Caching Strategy"},{"location":"image-processing/caching/#caching-strategy","text":"","title":"Caching Strategy"},{"location":"image-processing/caching/#overview","text":"The Opossum Search image processing system implements a sophisticated multi-tiered caching architecture to optimize performance, reduce resource consumption, and improve response times. This document details the caching strategies used throughout the image processing pipeline. Note Effective caching is crucial for maintaining high performance in image processing. This document outlines the caching architecture and strategies used in Opossum Search.","title":"Overview"},{"location":"image-processing/caching/#caching-architecture","text":"The caching system employs a layered approach to maximize efficiency across different types of operations and data: graph TD A[Client Request] --> B{Cache Hit?} B -->|Yes| C[Return Cached Result] B -->|No| D[Process Request] D --> E[Store in Cache] E --> F[Return Result] subgraph \"Cache Layers\" G[Memory Cache] H[Redis Cache] I[Disk Cache] J[CDN Cache] end B -.-> G B -.-> H B -.-> I B -.-> J E -.-> G E -.-> H E -.-> I E -.-> J","title":"Caching Architecture"},{"location":"image-processing/caching/#cache-tiers","text":"Cache Tier Storage Medium Access Speed Capacity Persistence Use Case L1: Memory RAM <1ms Limited None Frequently accessed small results L2: Redis Redis Cluster 1-5ms Medium Configurable Shared results across instances L3: Disk Local SSD 5-20ms Large Full Larger results, backup layer L4: CDN Distributed Variable Very Large Extended Public, static results","title":"Cache Tiers"},{"location":"image-processing/caching/#cache-categories","text":"The system implements specialized caching for different types of processed content:","title":"Cache Categories"},{"location":"image-processing/caching/#1-result-caching","text":"Stores complete processing results based on input parameters: def get_processed_image ( image_id , operations , options ): # Generate cache key from parameters cache_key = generate_cache_key ( image_id , operations , options ) # Check if result exists in cache cached_result = cache . get ( cache_key ) if cached_result : return cached_result # Process image if not in cache result = process_image ( image_id , operations , options ) # Store in cache with appropriate TTL cache . set ( key = cache_key , value = result , ttl = calculate_ttl ( operations , options ) ) return result","title":"1. Result Caching"},{"location":"image-processing/caching/#2-intermediate-result-caching","text":"Caches partial results to optimize multi-step processing chains: def apply_filter_chain ( image , filters ): result = image last_cached_state = None for i , filter_op in enumerate ( filters ): # Generate intermediate cache key intermediate_key = f \" { image . id } :step { i } : { filter_op . hash } \" # Try to retrieve from cache cached_state = cache . get ( intermediate_key ) if cached_state : result = cached_state last_cached_state = i continue # Apply filter and cache result result = apply_filter ( result , filter_op ) cache . set ( key = intermediate_key , value = result , ttl = INTERMEDIATE_CACHE_TTL ) return result","title":"2. Intermediate Result Caching"},{"location":"image-processing/caching/#3-resource-caching","text":"Caches frequently used resources like filters, presets, and templates: def get_filter_preset ( preset_name , version = None ): # Generate cache key version = version or CURRENT_VERSION cache_key = f \"preset: { preset_name } :v { version } \" # Check cache cached_preset = resource_cache . get ( cache_key ) if cached_preset : return cached_preset # Load preset from storage preset = load_preset_from_storage ( preset_name , version ) # Cache with long TTL resource_cache . set ( key = cache_key , value = preset , ttl = RESOURCE_CACHE_TTL # Typically 24+ hours ) return preset","title":"3. Resource Caching"},{"location":"image-processing/caching/#4-thumbnail-caching","text":"Specialized caching for generated thumbnails and previews: def get_thumbnail ( image_id , width , height , format = \"webp\" ): cache_key = f \"thumb: { image_id } : { width } x { height } : { format } \" # Check cache cached_thumb = thumbnail_cache . get ( cache_key ) if cached_thumb : return cached_thumb # Generate thumbnail original = load_image ( image_id ) thumbnail = resize_image ( original , width , height ) if format != original . format : thumbnail = convert_format ( thumbnail , format ) # Cache with relatively long TTL thumbnail_cache . set ( key = cache_key , value = thumbnail , ttl = THUMBNAIL_CACHE_TTL # Typically 7+ days ) return thumbnail","title":"4. Thumbnail Caching"},{"location":"image-processing/caching/#cache-key-generation","text":"The system uses deterministic cache key generation to ensure consistency: def generate_cache_key ( image_id , operations , options ): \"\"\"Generate a deterministic cache key for image operations\"\"\" # Base components components = [ f \"img: { image_id } \" , f \"v: { IMAGE_PROCESSING_VERSION } \" ] # Add normalized operation signatures for op in operations : op_signature = normalize_operation ( op ) components . append ( f \"op: { op_signature } \" ) # Add normalized options for key , value in sorted ( options . items ()): components . append ( f \"opt: { key } : { normalize_value ( value ) } \" ) # Generate hash for the full key full_key = \":\" . join ( components ) hashed_key = hashlib . sha256 ( full_key . encode ()) . hexdigest () return f \"img_proc: { hashed_key } \"","title":"Cache Key Generation"},{"location":"image-processing/caching/#ttl-strategies","text":"Different cache items have different time-to-live (TTL) values based on their characteristics: Content Type Default TTL Rationale Final Results 24 hours Balance between freshness and efficiency Intermediate Results 1 hour Temporary computation savings Thumbnails 7 days Rarely change, high request frequency Resources 24-48 hours Static content with version checks Error Results 5 minutes Prevent repeated processing of problematic inputs TTLs are dynamically adjusted based on: Resource constraints (lower TTLs when storage is limited) Usage patterns (longer TTLs for frequently accessed items) Content type (images vs. generated SVGs) Processing complexity (longer TTLs for expensive operations)","title":"TTL Strategies"},{"location":"image-processing/caching/#cache-invalidation-strategies","text":"","title":"Cache Invalidation Strategies"},{"location":"image-processing/caching/#time-based-invalidation","text":"The primary invalidation mechanism is time-based expiration through TTLs.","title":"Time-Based Invalidation"},{"location":"image-processing/caching/#explicit-invalidation","text":"def invalidate_image_caches ( image_id ): \"\"\"Invalidate all caches related to a specific image\"\"\" # Pattern-based invalidation in Redis redis_client . delete_by_pattern ( f \"img_proc:*:img: { image_id } :*\" ) # Clear from memory cache memory_cache . delete_by_pattern ( f \"* { image_id } *\" ) # Mark for disk cache cleanup disk_cache . mark_for_cleanup ( image_id ) # Purge from CDN if applicable if cdn_enabled : cdn_client . purge_object ( f \"images/ { image_id } /*\" )","title":"Explicit Invalidation"},{"location":"image-processing/caching/#version-based-invalidation","text":"Cache keys include version information to automatically invalidate when: Processing algorithms change New filters are added Quality improvements are implemented # When the algorithm version changes IMAGE_PROCESSING_VERSION = \"2.5.0\" # Included in all cache keys","title":"Version-Based Invalidation"},{"location":"image-processing/caching/#lrulfu-policies","text":"Different cache tiers implement appropriate eviction policies: Cache Tier Primary Policy Secondary Policy Memory LRU (Least Recently Used) Size limit Redis LFU (Least Frequently Used) TTL Disk FIFO with size limits Manual purge CDN TTL None","title":"LRU/LFU Policies"},{"location":"image-processing/caching/#performance-metrics","text":"","title":"Performance Metrics"},{"location":"image-processing/caching/#cache-hit-rates","text":"Cache Type Target Hit Rate Observed Hit Rate Result Cache >60% 65-75% Intermediate Cache >40% 45-55% Thumbnail Cache >80% 85-95% Resource Cache >95% 98-99%","title":"Cache Hit Rates"},{"location":"image-processing/caching/#latency-improvements","text":"Operation Uncached Latency Cached Latency Improvement Basic Filter 100-200ms 5-15ms 93-95% Filter Chain 300-800ms 10-30ms 96-97% SVG Generation 500-2000ms 15-50ms 97-98% Image Analysis 1000-3000ms 20-70ms 98-99%","title":"Latency Improvements"},{"location":"image-processing/caching/#cache-storage-sizing","text":"Guidelines for allocating cache storage: Traffic Level Memory Cache Redis Cache Disk Cache Low (<100k req/day) 1-2GB 5-10GB 20-50GB Medium (100k-1M req/day) 4-8GB 20-50GB 100-200GB High (1M+ req/day) 16-32GB 100-200GB 500GB-1TB","title":"Cache Storage Sizing"},{"location":"image-processing/caching/#integration-with-redis","text":"The Redis cache implementation leverages specialized data structures: class RedisCacheManager : def __init__ ( self , redis_client , namespace = \"img_proc\" ): self . redis = redis_client self . namespace = namespace def get ( self , key ): \"\"\"Retrieve an item from cache\"\"\" full_key = f \" { self . namespace } : { key } \" # Get value and metadata pipeline = self . redis . pipeline () pipeline . get ( full_key ) pipeline . hgetall ( f \" { full_key } :meta\" ) value , metadata = pipeline . execute () if not value : return None # Update access metrics self . redis . hincrby ( f \" { full_key } :meta\" , \"access_count\" , 1 ) self . redis . hset ( f \" { full_key } :meta\" , \"last_access\" , time . time ()) # Deserialize value return pickle . loads ( value ) def set ( self , key , value , ttl = 86400 ): \"\"\"Store an item in cache with metadata\"\"\" full_key = f \" { self . namespace } : { key } \" # Serialize value serialized = pickle . dumps ( value ) # Store value and metadata pipeline = self . redis . pipeline () pipeline . setex ( full_key , ttl , serialized ) # Store metadata for analytics and management pipeline . hmset ( f \" { full_key } :meta\" , { \"created_at\" : time . time (), \"size_bytes\" : len ( serialized ), \"ttl\" : ttl , \"access_count\" : 0 , \"last_access\" : time . time () }) pipeline . expire ( f \" { full_key } :meta\" , ttl ) pipeline . execute () def delete_by_pattern ( self , pattern ): \"\"\"Delete all keys matching a pattern\"\"\" full_pattern = f \" { self . namespace } : { pattern } \" # Find all matching keys keys = self . redis . keys ( full_pattern ) meta_keys = [ f \" { key } :meta\" for key in keys ] # Delete keys and metadata if keys : pipeline = self . redis . pipeline () pipeline . delete ( * keys ) if meta_keys : pipeline . delete ( * meta_keys ) pipeline . execute ()","title":"Integration with Redis"},{"location":"image-processing/caching/#cache-warming-strategies","text":"The system implements proactive cache warming to improve performance: Predictive Warming : Analyzes usage patterns to pre-cache likely-to-be-requested variants Background Warming : Fills caches during low-traffic periods Related Content Warming : When one variant is requested, preemptively caches related variants def warm_cache_for_popular_images ( top_n = 100 ): \"\"\"Warm cache for most popular images with common operations\"\"\" # Get most frequently accessed images popular_images = analytics . get_top_images ( limit = top_n ) # Common operations to pre-cache common_operations = [ { \"type\" : \"resize\" , \"width\" : 800 , \"height\" : 600 }, { \"type\" : \"thumbnail\" , \"size\" : 200 }, { \"type\" : \"grayscale\" }, # More common operations ] # Queue cache warming tasks for image_id in popular_images : for operation in common_operations : task_queue . enqueue ( warm_image_cache , image_id = image_id , operation = operation , priority = \"low\" # Don't interfere with user requests )","title":"Cache Warming Strategies"},{"location":"image-processing/caching/#cache-analytics-and-monitoring","text":"The system collects detailed cache performance metrics: class CacheAnalytics : def collect_metrics ( self ): \"\"\"Collect and report cache performance metrics\"\"\" metrics = { \"memory_cache\" : { \"size\" : memory_cache . size (), \"item_count\" : memory_cache . count (), \"hit_rate\" : memory_cache . hit_rate (), \"miss_rate\" : memory_cache . miss_rate (), \"eviction_rate\" : memory_cache . eviction_rate (), \"avg_ttl\" : memory_cache . avg_ttl () }, \"redis_cache\" : { \"size\" : redis_cache . size (), \"item_count\" : redis_cache . count (), \"hit_rate\" : redis_cache . hit_rate (), \"miss_rate\" : redis_cache . miss_rate (), \"eviction_rate\" : redis_cache . eviction_rate (), \"avg_ttl\" : redis_cache . avg_ttl (), \"memory_usage\" : redis_cache . memory_usage () }, # Similar metrics for other cache tiers } # Report to monitoring system monitoring . report_metrics ( \"cache_performance\" , metrics ) # Log periodic summary logger . info ( f \"Cache performance summary: hit_rate= { metrics [ 'redis_cache' ][ 'hit_rate' ] } %, \" f \"size= { metrics [ 'redis_cache' ][ 'size' ] / 1024 / 1024 : .2f } MB\" )","title":"Cache Analytics and Monitoring"},{"location":"image-processing/caching/#configuration-options","text":"The caching system is highly configurable: # Example configuration caching : # Global settings enabled : true log_level : \"info\" # Memory cache memory_cache : enabled : true max_size : \"2GB\" ttl : 600 # seconds # Redis cache redis_cache : enabled : true host : \"redis.example.com\" port : 6379 db : 0 ttl : 86400 # seconds max_memory : \"50GB\" eviction_policy : \"lfu\" # Disk cache disk_cache : enabled : true path : \"/var/cache/opossum_images\" max_size : \"200GB\" cleanup_interval : 3600 # seconds Tip Properly configuring the caching system is essential for achieving optimal performance. Adjust the cache sizes, TTLs, and eviction policies based on your specific workload and resource constraints.","title":"Configuration Options"},{"location":"image-processing/caching/#related-documentation","text":"Image Processing Overview Effects and Filters SVG Generation Performance Optimization Infrastructure Optimization Strategies","title":"Related Documentation"},{"location":"image-processing/effects/","text":"Image Effects and Filters \u00b6 Overview \u00b6 Opossum Search provides a comprehensive suite of image effects and filters to enhance, transform, and stylize images through its processing pipeline. This document outlines the available effects, their parameters, usage patterns, and implementation details. Available Filters \u00b6 Basic Adjustments \u00b6 Filter Name Description Parameters Performance Impact brightness Adjusts image brightness level (-100 to 100) Low contrast Modifies image contrast level (-100 to 100) Low saturation Alters color intensity level (-100 to 100) Low sharpness Enhances or softens details level (-100 to 100) Medium exposure Adjusts light exposure stops (-5.0 to 5.0) Low gamma Applies gamma correction value (0.1 to 5.0) Low Color Transformations \u00b6 Filter Name Description Parameters Performance Impact grayscale Converts to black and white method ('average', 'luminosity', 'lightness') Low sepia Applies sepia tone intensity (0.0 to 1.0) Low invert Inverts all colors strength (0.0 to 1.0) Low colorize Applies color tint color (hex), strength (0.0 to 1.0) Low temperature Adjusts color temperature kelvin (2000 to 12000) Low channel_mixer Modifies RGB channels red , green , blue (-2.0 to 2.0 each) Medium Stylistic Effects \u00b6 Filter Name Description Parameters Performance Impact vignette Darkens image corners size (0.0 to 1.0), darkness (0.0 to 1.0) Low noise Adds grain to image amount (0.0 to 1.0), type ('gaussian', 'salt', 'poisson') Medium bloom Creates light glow effect intensity (0.0 to 1.0), threshold (0.0 to 1.0) High pixelate Creates pixel art style size (1 to 100) Medium posterize Reduces color palette levels (2 to 16) Low duotone Two-color gradient mapping highlight (hex), shadow (hex) Medium Artistic Filters \u00b6 Filter Name Description Parameters Performance Impact watercolor Mimics watercolor painting strength (0.0 to 1.0), detail (0.0 to 1.0) Very High oil_painting Simulates oil painting brushSize (1 to 20), coarseness (1 to 10) Very High sketch Creates pencil sketch effect style ('simple', 'detailed'), strength (0.0 to 1.0) High vintage Retro film look era ('50s', '60s', '70s', '80s') Medium comic Comic book style detail (0.0 to 1.0), colors (2 to 8) High halftone Newspaper print style size (1 to 50), angle (0 to 360) Medium Spatial Transformations \u00b6 Filter Name Description Parameters Performance Impact blur Gaussian blur effect radius (0.0 to 50.0) Medium-High (radius dependent) motion_blur Directional blur angle (0 to 360), distance (0 to 200) High radial_blur Circular blur pattern center_x , center_y (0.0 to 1.0), amount (0.0 to 1.0) High emboss Creates 3D embossed look strength (0.0 to 5.0), angle (0 to 360) Medium wave Applies wave distortion amplitude (0 to 50), wavelength (0 to 200) Medium fisheye Fisheye lens distortion strength (0.0 to 1.0) Medium Usage Patterns \u00b6 Basic Implementation \u00b6 from opossum.image import filters # Apply a single filter result = filters . apply_filter ( image_path = 'input.jpg' , filter_name = 'grayscale' , params = { 'method' : 'luminosity' } ) # Save the result result . save ( 'output.jpg' ) Filter Chaining \u00b6 from opossum.image import filters # Create a filter chain filter_chain = [ { 'name' : 'brightness' , 'params' : { 'level' : 15 }}, { 'name' : 'contrast' , 'params' : { 'level' : 10 }}, { 'name' : 'vignette' , 'params' : { 'size' : 0.2 , 'darkness' : 0.3 }} ] # Apply the chain result = filters . apply_filter_chain ( image_path = 'input.jpg' , filter_chain = filter_chain ) # Save the result result . save ( 'output.jpg' ) API Usage \u00b6 mutation { processImage ( inputUrl : \"https://example.com/image.jpg\" , filters : [ { name : \"vintage\" , params : { era : \"70s\" } }, { name : \"vignette\" , params : { size : 0.3 , darkness : 0.4 } } ] ) { resultUrl processingTime dimensions { width height } } } Presets \u00b6 The system includes several predefined filter combinations for common effects: Preset Name Description Included Filters nostalgic Warm vintage look temperature(7500) + vignette + grain cinematic Film-like color grading contrast + color_lut(cinematic) + letterbox clarity Enhanced details sharpness + clarity + local_contrast dramatic High-contrast look contrast + clarity + duotone fresh Bright and vibrant brightness + saturation + vibrance faded Subtle muted look fade + grain + curve_adjustment Implementation Details \u00b6 Processing Pipeline \u00b6 graph LR A[Image Input] --> B[Preprocessing] B --> C[Filter Application] C --> D[Post-processing] D --> E[Result Output] subgraph \"Filter Application\" F[Parameter Validation] --> G[Filter Selection] G --> H[Effect Processing] H --> I[Result Compositing] end Performance Optimization \u00b6 The filter system implements several optimizations: Progressive Processing : Applies filters at reduced resolution first for preview Caching Strategy : Caches intermediate results for filter chains Adaptive Sampling : Adjusts processing resolution based on filter complexity Hardware Acceleration : Uses GPU acceleration when available Parallel Processing : Applies independent filters concurrently Technical Considerations \u00b6 Supported Image Formats \u00b6 Input formats: JPEG, PNG, GIF, WEBP, HEIC, BMP, TIFF Output formats: JPEG, PNG, WEBP, GIF Size Limitations \u00b6 Maximum input dimensions: 8192 \u00d7 8192 pixels Maximum file size: 20MB Recommended dimensions for optimal performance: 1024 \u00d7 1024 pixels Memory Usage \u00b6 Filter Complexity Memory Overhead Recommended Image Size Low 1-2\u00d7 image size No limitation Medium 2-4\u00d7 image size \u2264 4096 \u00d7 4096 High 4-8\u00d7 image size \u2264 2048 \u00d7 2048 Very High 8-16\u00d7 image size \u2264 1024 \u00d7 1024 Best Practices \u00b6 Chain Order Matters : The sequence of filters can significantly affect the result Preview First : Use lower resolution for previews before applying to full resolution Selective Application : Consider using masks for applying effects to specific areas Parameter Restraint : Subtle effects (0.2-0.5 range) often produce better results Resolution Matching : Process at the final needed resolution to save resources Examples \u00b6 Before/After Gallery \u00b6 Effect Before After Parameters Vintage Original Processed era: '70s', strength: 0.7 Duotone Original Processed highlight: #FF9500, shadow: #004A7F Sketch Original Processed style: 'detailed', strength: 0.8 Custom Filter Development \u00b6 For developers who want to create custom filters, the system provides an extension API: from opossum.image import FilterBase class MyCustomFilter ( FilterBase ): \"\"\"Custom rainbow gradient overlay filter\"\"\" def __init__ ( self , intensity = 0.5 , orientation = 'horizontal' ): self . intensity = float ( intensity ) self . orientation = orientation def process ( self , image ): # Implementation details # ... return processed_image @property def performance_impact ( self ): return \"medium\" # Register custom filter from opossum.image import register_filter register_filter ( 'rainbow' , MyCustomFilter ) Future Development \u00b6 Upcoming filter enhancements include: AI-powered style transfer filters Real-time video filter application User-created filter marketplace Image component selective filtering Platform-specific optimizations Related Documentation \u00b6 Image Processing Overview SVG Generation Performance Optimization Caching Strategy","title":"Effects and Filters"},{"location":"image-processing/effects/#image-effects-and-filters","text":"","title":"Image Effects and Filters"},{"location":"image-processing/effects/#overview","text":"Opossum Search provides a comprehensive suite of image effects and filters to enhance, transform, and stylize images through its processing pipeline. This document outlines the available effects, their parameters, usage patterns, and implementation details.","title":"Overview"},{"location":"image-processing/effects/#available-filters","text":"","title":"Available Filters"},{"location":"image-processing/effects/#basic-adjustments","text":"Filter Name Description Parameters Performance Impact brightness Adjusts image brightness level (-100 to 100) Low contrast Modifies image contrast level (-100 to 100) Low saturation Alters color intensity level (-100 to 100) Low sharpness Enhances or softens details level (-100 to 100) Medium exposure Adjusts light exposure stops (-5.0 to 5.0) Low gamma Applies gamma correction value (0.1 to 5.0) Low","title":"Basic Adjustments"},{"location":"image-processing/effects/#color-transformations","text":"Filter Name Description Parameters Performance Impact grayscale Converts to black and white method ('average', 'luminosity', 'lightness') Low sepia Applies sepia tone intensity (0.0 to 1.0) Low invert Inverts all colors strength (0.0 to 1.0) Low colorize Applies color tint color (hex), strength (0.0 to 1.0) Low temperature Adjusts color temperature kelvin (2000 to 12000) Low channel_mixer Modifies RGB channels red , green , blue (-2.0 to 2.0 each) Medium","title":"Color Transformations"},{"location":"image-processing/effects/#stylistic-effects","text":"Filter Name Description Parameters Performance Impact vignette Darkens image corners size (0.0 to 1.0), darkness (0.0 to 1.0) Low noise Adds grain to image amount (0.0 to 1.0), type ('gaussian', 'salt', 'poisson') Medium bloom Creates light glow effect intensity (0.0 to 1.0), threshold (0.0 to 1.0) High pixelate Creates pixel art style size (1 to 100) Medium posterize Reduces color palette levels (2 to 16) Low duotone Two-color gradient mapping highlight (hex), shadow (hex) Medium","title":"Stylistic Effects"},{"location":"image-processing/effects/#artistic-filters","text":"Filter Name Description Parameters Performance Impact watercolor Mimics watercolor painting strength (0.0 to 1.0), detail (0.0 to 1.0) Very High oil_painting Simulates oil painting brushSize (1 to 20), coarseness (1 to 10) Very High sketch Creates pencil sketch effect style ('simple', 'detailed'), strength (0.0 to 1.0) High vintage Retro film look era ('50s', '60s', '70s', '80s') Medium comic Comic book style detail (0.0 to 1.0), colors (2 to 8) High halftone Newspaper print style size (1 to 50), angle (0 to 360) Medium","title":"Artistic Filters"},{"location":"image-processing/effects/#spatial-transformations","text":"Filter Name Description Parameters Performance Impact blur Gaussian blur effect radius (0.0 to 50.0) Medium-High (radius dependent) motion_blur Directional blur angle (0 to 360), distance (0 to 200) High radial_blur Circular blur pattern center_x , center_y (0.0 to 1.0), amount (0.0 to 1.0) High emboss Creates 3D embossed look strength (0.0 to 5.0), angle (0 to 360) Medium wave Applies wave distortion amplitude (0 to 50), wavelength (0 to 200) Medium fisheye Fisheye lens distortion strength (0.0 to 1.0) Medium","title":"Spatial Transformations"},{"location":"image-processing/effects/#usage-patterns","text":"","title":"Usage Patterns"},{"location":"image-processing/effects/#basic-implementation","text":"from opossum.image import filters # Apply a single filter result = filters . apply_filter ( image_path = 'input.jpg' , filter_name = 'grayscale' , params = { 'method' : 'luminosity' } ) # Save the result result . save ( 'output.jpg' )","title":"Basic Implementation"},{"location":"image-processing/effects/#filter-chaining","text":"from opossum.image import filters # Create a filter chain filter_chain = [ { 'name' : 'brightness' , 'params' : { 'level' : 15 }}, { 'name' : 'contrast' , 'params' : { 'level' : 10 }}, { 'name' : 'vignette' , 'params' : { 'size' : 0.2 , 'darkness' : 0.3 }} ] # Apply the chain result = filters . apply_filter_chain ( image_path = 'input.jpg' , filter_chain = filter_chain ) # Save the result result . save ( 'output.jpg' )","title":"Filter Chaining"},{"location":"image-processing/effects/#api-usage","text":"mutation { processImage ( inputUrl : \"https://example.com/image.jpg\" , filters : [ { name : \"vintage\" , params : { era : \"70s\" } }, { name : \"vignette\" , params : { size : 0.3 , darkness : 0.4 } } ] ) { resultUrl processingTime dimensions { width height } } }","title":"API Usage"},{"location":"image-processing/effects/#presets","text":"The system includes several predefined filter combinations for common effects: Preset Name Description Included Filters nostalgic Warm vintage look temperature(7500) + vignette + grain cinematic Film-like color grading contrast + color_lut(cinematic) + letterbox clarity Enhanced details sharpness + clarity + local_contrast dramatic High-contrast look contrast + clarity + duotone fresh Bright and vibrant brightness + saturation + vibrance faded Subtle muted look fade + grain + curve_adjustment","title":"Presets"},{"location":"image-processing/effects/#implementation-details","text":"","title":"Implementation Details"},{"location":"image-processing/effects/#processing-pipeline","text":"graph LR A[Image Input] --> B[Preprocessing] B --> C[Filter Application] C --> D[Post-processing] D --> E[Result Output] subgraph \"Filter Application\" F[Parameter Validation] --> G[Filter Selection] G --> H[Effect Processing] H --> I[Result Compositing] end","title":"Processing Pipeline"},{"location":"image-processing/effects/#performance-optimization","text":"The filter system implements several optimizations: Progressive Processing : Applies filters at reduced resolution first for preview Caching Strategy : Caches intermediate results for filter chains Adaptive Sampling : Adjusts processing resolution based on filter complexity Hardware Acceleration : Uses GPU acceleration when available Parallel Processing : Applies independent filters concurrently","title":"Performance Optimization"},{"location":"image-processing/effects/#technical-considerations","text":"","title":"Technical Considerations"},{"location":"image-processing/effects/#supported-image-formats","text":"Input formats: JPEG, PNG, GIF, WEBP, HEIC, BMP, TIFF Output formats: JPEG, PNG, WEBP, GIF","title":"Supported Image Formats"},{"location":"image-processing/effects/#size-limitations","text":"Maximum input dimensions: 8192 \u00d7 8192 pixels Maximum file size: 20MB Recommended dimensions for optimal performance: 1024 \u00d7 1024 pixels","title":"Size Limitations"},{"location":"image-processing/effects/#memory-usage","text":"Filter Complexity Memory Overhead Recommended Image Size Low 1-2\u00d7 image size No limitation Medium 2-4\u00d7 image size \u2264 4096 \u00d7 4096 High 4-8\u00d7 image size \u2264 2048 \u00d7 2048 Very High 8-16\u00d7 image size \u2264 1024 \u00d7 1024","title":"Memory Usage"},{"location":"image-processing/effects/#best-practices","text":"Chain Order Matters : The sequence of filters can significantly affect the result Preview First : Use lower resolution for previews before applying to full resolution Selective Application : Consider using masks for applying effects to specific areas Parameter Restraint : Subtle effects (0.2-0.5 range) often produce better results Resolution Matching : Process at the final needed resolution to save resources","title":"Best Practices"},{"location":"image-processing/effects/#examples","text":"","title":"Examples"},{"location":"image-processing/effects/#beforeafter-gallery","text":"Effect Before After Parameters Vintage Original Processed era: '70s', strength: 0.7 Duotone Original Processed highlight: #FF9500, shadow: #004A7F Sketch Original Processed style: 'detailed', strength: 0.8","title":"Before/After Gallery"},{"location":"image-processing/effects/#custom-filter-development","text":"For developers who want to create custom filters, the system provides an extension API: from opossum.image import FilterBase class MyCustomFilter ( FilterBase ): \"\"\"Custom rainbow gradient overlay filter\"\"\" def __init__ ( self , intensity = 0.5 , orientation = 'horizontal' ): self . intensity = float ( intensity ) self . orientation = orientation def process ( self , image ): # Implementation details # ... return processed_image @property def performance_impact ( self ): return \"medium\" # Register custom filter from opossum.image import register_filter register_filter ( 'rainbow' , MyCustomFilter )","title":"Custom Filter Development"},{"location":"image-processing/effects/#future-development","text":"Upcoming filter enhancements include: AI-powered style transfer filters Real-time video filter application User-created filter marketplace Image component selective filtering Platform-specific optimizations","title":"Future Development"},{"location":"image-processing/effects/#related-documentation","text":"Image Processing Overview SVG Generation Performance Optimization Caching Strategy","title":"Related Documentation"},{"location":"image-processing/optimization/","text":"Performance Optimization \u00b6 Overview \u00b6 Opossum Search implements a comprehensive set of performance optimization strategies for its image processing pipeline. This document outlines the techniques used to maximize throughput, minimize latency, and efficiently utilize system resources while maintaining high-quality output. Performance Challenges \u00b6 Image processing presents several unique performance challenges: Challenge Description Impact Memory Consumption Image data requires significant memory, especially at high resolutions Resource constraints, potential OOM errors CPU Intensity Many operations require intensive computation Processing latency, system load I/O Bottlenecks Reading/writing large image files can create bottlenecks Throughput limitations, increased latency Scaling Complexity Performance costs often scale non-linearly with image dimensions Unpredictable processing times Resource Contention Multiple concurrent operations compete for system resources Reduced throughput, increased latency variance Optimization Strategies \u00b6 Resolution-Aware Processing \u00b6 graph TD A[Input Image] --> B{Size Analysis} B -->|Small| C[Direct Processing] B -->|Medium| D[Scaled Processing] B -->|Large| E[Multi-Resolution Approach] E --> F[Process at Low Resolution] F --> G[Apply Results to Medium Resolution] G --> H[Finalize at Target Resolution] C --> I[Output Result] D --> I H --> I The system dynamically adjusts processing based on image dimensions: Image Size Strategy Performance Gain <800px Direct processing at original resolution Baseline 800-2000px Scaled processing with selective upsampling 40-60% faster >2000px Multi-resolution pyramid approach 70-90% faster Parallel Processing Architecture \u00b6 The pipeline implements parallelism at multiple levels: Inter-image parallelism : Processing multiple images concurrently Intra-image parallelism : Dividing a single image into tiles for parallel processing Pipeline parallelism : Executing sequential operations in parallel where dependencies allow Operation parallelism : Utilizing SIMD and multi-threading for individual operations # Example of parallel processing implementation from opossum.image import ParallelProcessor def process_image_batch ( image_paths , operations , max_workers = None ): \"\"\"Process multiple images in parallel with automatic resource management\"\"\" with ParallelProcessor ( max_workers = max_workers ) as processor : results = processor . map ( function = apply_operations , items = image_paths , operations = operations , chunk_size = get_optimal_chunk_size () ) return results Memory Management \u00b6 Sophisticated memory management techniques minimize footprint: Technique Description Memory Reduction Progressive Loading Loads only necessary image parts into memory 40-70% In-place Operations Modifies images in-place when possible 30-50% Image Tiling Processes large images in smaller tiles 60-80% Format Optimization Uses memory-efficient pixel formats 20-40% Reference Counting Tracks image data usage for timely deallocation 20-30% Garbage Collection Scheduled cleanup of unused resources 10-20% Hardware Acceleration \u00b6 The system leverages available hardware: GPU Acceleration : Uses GPU for compatible operations (CUDA, OpenCL) SIMD Instructions : Vectorized processing with AVX2/NEON Memory Hierarchy Optimization : Cache-aware algorithms Heterogeneous Computing : Distributes work across CPU, GPU, and specialized processors Performance gains from hardware acceleration: Operation CPU Only With Acceleration Speedup Gaussian Blur 100ms 15ms 6.7\u00d7 Color Conversion 45ms 8ms 5.6\u00d7 Edge Detection 120ms 18ms 6.7\u00d7 Neural Vectorization 8,500ms 950ms 8.9\u00d7 Algorithm Selection \u00b6 The system dynamically selects optimal algorithms based on image characteristics: def select_optimal_algorithm ( image , operation , quality_target ): \"\"\"Selects the most efficient algorithm based on image characteristics\"\"\" # Analyze image complexity complexity = analyze_image_complexity ( image ) # Select algorithm based on complexity, operation, and quality target if complexity < 0.3 : # Low complexity image if quality_target == \"high\" : return PreciseAlgorithm ( operation ) else : return FastApproximationAlgorithm ( operation ) elif complexity < 0.7 : # Medium complexity if quality_target == \"high\" : return AdaptiveAlgorithm ( operation ) else : return OptimizedApproximationAlgorithm ( operation ) else : # High complexity if quality_target == \"high\" : return MultiStageAlgorithm ( operation ) else : return AcceleratedApproximationAlgorithm ( operation ) Caching System \u00b6 The multi-level caching system significantly improves performance: Cache Level Description Hit Rate Latency Reduction In-memory Result Cache Caches complete results for repeat requests 15-25% 95-100% Intermediate Result Cache Stores partial processing results for reuse 20-30% 40-60% Thumbnail Cache Caches scaled versions for preview and visualization 30-40% 70-90% Disk Cache Persists frequently accessed items to disk 10-15% 80-95% CDN Integration External caching of public results 5-10% 95-100% For detailed caching implementation, see Caching Strategy. Benchmarks and Performance Metrics \u00b6 Operation Performance by Image Size \u00b6 Operation 512\u00d7512 1024\u00d71024 2048\u00d72048 4096\u00d74096 Gaussian Blur 12ms 45ms 170ms 650ms Color Adjustment 8ms 30ms 110ms 420ms Edge Detection 15ms 55ms 200ms 780ms Neural Vectorization 950ms 3,200ms 12,500ms 48,000ms SVG Generation 180ms 720ms 2,900ms 11,000ms Measured on reference hardware: 8-core CPU @ 3.5GHz, 16GB RAM, NVIDIA RTX 3060 Throughput Testing \u00b6 Concurrency Images/Second (512\u00d7512) Images/Second (1024\u00d71024) Memory Usage 1 45 12 0.8GB 4 160 42 2.2GB 8 280 75 4.5GB 16 320 85 9.0GB 32 340 90 17.5GB Test conditions: Basic image processing chain (resize, color adjustment, sharpening) Memory Efficiency Improvements \u00b6 graph LR A[Original] --> B[v1.0] B -- \"-35%\" --> C[v1.2] C -- \"-25%\" --> D[v1.5] D -- \"-20%\" --> E[Current] style B fill:#f9a,stroke:#333 style C fill:#fd7,stroke:#333 style D fill:#df7,stroke:#333 style E fill:#9f9,stroke:#333 Memory usage reduction over software versions: v1.0: Baseline v1.2: 35% reduction through tiled processing v1.5: Additional 25% reduction through format optimization Current: Further 20% reduction through algorithm refinement Configuration Options \u00b6 The performance optimization system provides several configuration options: Resource Management \u00b6 # Example configuration section performance : # Hardware utilization max_cpu_cores : 0 # 0 = automatic detection gpu_enabled : true gpu_memory_limit : \"2GB\" # Memory management max_memory_usage : \"4GB\" tile_size : 512 enable_memory_monitoring : true # Processing strategy parallel_images : 4 quality_vs_speed : 0.7 # 0=fastest, 1=highest quality # Caching enable_result_cache : true enable_intermediate_cache : true cache_ttl : 3600 # seconds persistent_cache_size : \"10GB\" Quality vs. Performance Balance \u00b6 The system offers configurable quality-speed tradeoffs: Quality Setting Description Performance Impact Visual Quality ultra Maximum quality, no compromises Baseline Reference high Visually indistinguishable from ultra 2-3\u00d7 faster 99% of reference balanced Default setting, good tradeoff 5-8\u00d7 faster 95% of reference fast Prioritizes performance 10-15\u00d7 faster 90% of reference draft Maximum performance 20-30\u00d7 faster 80% of reference Automatic Profiling and Tuning \u00b6 The system includes automatic performance optimization: Runtime Profiling : Monitors execution time and resource usage Bottleneck Detection : Identifies performance bottlenecks Parameter Tuning : Automatically adjusts processing parameters Resource Allocation : Dynamically allocates resources to operations Feedback Loop : Incorporates execution history into future decisions Optimization for Specific Use Cases \u00b6 Batch Processing \u00b6 For processing large numbers of images: from opossum.image import BatchProcessor , OperationChain # Define operation chain chain = OperationChain () chain . add_operation ( \"resize\" , width = 800 , height = 600 ) chain . add_operation ( \"adjust_colors\" , brightness = 1.1 , contrast = 1.2 ) chain . add_operation ( \"watermark\" , text = \"\u00a9 Opossum Search\" ) # Configure batch processor processor = BatchProcessor ( operation_chain = chain , concurrency = 8 , input_dir = \"input_images\" , output_dir = \"processed_images\" , output_format = \"webp\" , quality = 85 , error_handling = \"continue\" # Skip errors and continue processing ) # Process batch results = processor . process () print ( f \"Processed { results . success_count } images, { results . error_count } errors\" ) print ( f \"Average processing time: { results . avg_time_ms } ms per image\" ) Real-time Processing \u00b6 For low-latency requirements: Simplified Pipeline : Uses faster approximation algorithms Warm-up Phase : Pre-loads and initializes components Persistent Workers : Maintains worker processes for immediate availability Priority Queue : Processes urgent requests first Deadline-aware Scheduling : Adapts quality based on time constraints Best Practices \u00b6 General Recommendations \u00b6 Process at Target Resolution : Avoid unnecessary upscaling/downscaling Batch Similar Operations : Group similar images for better resource utilization Consider Output Format : WebP often provides the best quality/size ratio Monitor Resource Usage : Adjust concurrency based on system capabilities Implement Timeouts : Prevent resource exhaustion from problematic images Hardware Recommendations \u00b6 Use Case Recommended Hardware Notes Development 4+ cores, 16GB+ RAM GPU optional but helpful Small Production 8+ cores, 32GB+ RAM, SSD Entry-level GPU recommended Medium Production 16+ cores, 64GB+ RAM, NVMe Mid-range GPU (RTX 3060+) Large Production 32+ cores, 128GB+ RAM, RAID High-end GPU or multiple GPUs Enterprise Distributed processing cluster Multiple specialized nodes Code Examples \u00b6 Optimized Filter Chain \u00b6 from opossum.image import OptimizedProcessor # Create optimized processor processor = OptimizedProcessor ( quality_target = \"balanced\" , hardware_acceleration = True , memory_limit = \"4GB\" ) # Process image with optimized chain result = processor . process ( image_path = \"input.jpg\" , operations = [ # Operations are automatically reordered for optimal performance { \"type\" : \"resize\" , \"width\" : 1200 , \"height\" : 800 }, { \"type\" : \"sharpen\" , \"amount\" : 0.3 }, { \"type\" : \"adjust_colors\" , \"brightness\" : 1.1 , \"contrast\" : 1.05 }, { \"type\" : \"add_border\" , \"size\" : 5 , \"color\" : \"#000000\" } ], output_format = \"webp\" , output_quality = 85 ) Performance Monitoring \u00b6 from opossum.image import PerformanceMonitor , process_image # Create performance monitor monitor = PerformanceMonitor () # Process with monitoring with monitor . track ( \"main_processing\" ): # Track overall processing with monitor . track ( \"preprocessing\" ): # Preprocessing steps image = load_image ( \"input.jpg\" ) image = resize_image ( image , width = 1200 ) with monitor . track ( \"filtering\" ): # Apply filters image = apply_filter ( image , \"sharpen\" ) image = adjust_colors ( image , brightness = 1.1 ) with monitor . track ( \"output_generation\" ): # Generate output save_image ( image , \"output.webp\" ) # Print performance report print ( monitor . report ()) Performance Testing Tools \u00b6 The codebase includes several tools for performance testing: Benchmark Suite : Comprehensive performance testing across operations Profiling Tools : Memory and CPU profiling for optimization Load Testing : Simulates high-concurrency scenarios Regression Testing : Tracks performance changes between versions Hardware Comparison : Tests performance across different hardware configurations Future Optimizations \u00b6 Planned performance improvements include: Neural Compute Optimization : Specialized neural processing for appropriate operations Distributed Processing : Handling extremely large workloads across multiple machines Predictive Resource Allocation : Anticipating resource needs based on input characteristics Custom CUDA Kernels : Highly optimized GPU implementations of critical operations Adaptive Quality Control : Dynamic quality adjustment based on perceptual importance Related Documentation \u00b6 Image Processing Overview Effects and Filters SVG Generation Caching Strategy Infrastructure Optimization Strategies","title":"Performance Optimization"},{"location":"image-processing/optimization/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"image-processing/optimization/#overview","text":"Opossum Search implements a comprehensive set of performance optimization strategies for its image processing pipeline. This document outlines the techniques used to maximize throughput, minimize latency, and efficiently utilize system resources while maintaining high-quality output.","title":"Overview"},{"location":"image-processing/optimization/#performance-challenges","text":"Image processing presents several unique performance challenges: Challenge Description Impact Memory Consumption Image data requires significant memory, especially at high resolutions Resource constraints, potential OOM errors CPU Intensity Many operations require intensive computation Processing latency, system load I/O Bottlenecks Reading/writing large image files can create bottlenecks Throughput limitations, increased latency Scaling Complexity Performance costs often scale non-linearly with image dimensions Unpredictable processing times Resource Contention Multiple concurrent operations compete for system resources Reduced throughput, increased latency variance","title":"Performance Challenges"},{"location":"image-processing/optimization/#optimization-strategies","text":"","title":"Optimization Strategies"},{"location":"image-processing/optimization/#resolution-aware-processing","text":"graph TD A[Input Image] --> B{Size Analysis} B -->|Small| C[Direct Processing] B -->|Medium| D[Scaled Processing] B -->|Large| E[Multi-Resolution Approach] E --> F[Process at Low Resolution] F --> G[Apply Results to Medium Resolution] G --> H[Finalize at Target Resolution] C --> I[Output Result] D --> I H --> I The system dynamically adjusts processing based on image dimensions: Image Size Strategy Performance Gain <800px Direct processing at original resolution Baseline 800-2000px Scaled processing with selective upsampling 40-60% faster >2000px Multi-resolution pyramid approach 70-90% faster","title":"Resolution-Aware Processing"},{"location":"image-processing/optimization/#parallel-processing-architecture","text":"The pipeline implements parallelism at multiple levels: Inter-image parallelism : Processing multiple images concurrently Intra-image parallelism : Dividing a single image into tiles for parallel processing Pipeline parallelism : Executing sequential operations in parallel where dependencies allow Operation parallelism : Utilizing SIMD and multi-threading for individual operations # Example of parallel processing implementation from opossum.image import ParallelProcessor def process_image_batch ( image_paths , operations , max_workers = None ): \"\"\"Process multiple images in parallel with automatic resource management\"\"\" with ParallelProcessor ( max_workers = max_workers ) as processor : results = processor . map ( function = apply_operations , items = image_paths , operations = operations , chunk_size = get_optimal_chunk_size () ) return results","title":"Parallel Processing Architecture"},{"location":"image-processing/optimization/#memory-management","text":"Sophisticated memory management techniques minimize footprint: Technique Description Memory Reduction Progressive Loading Loads only necessary image parts into memory 40-70% In-place Operations Modifies images in-place when possible 30-50% Image Tiling Processes large images in smaller tiles 60-80% Format Optimization Uses memory-efficient pixel formats 20-40% Reference Counting Tracks image data usage for timely deallocation 20-30% Garbage Collection Scheduled cleanup of unused resources 10-20%","title":"Memory Management"},{"location":"image-processing/optimization/#hardware-acceleration","text":"The system leverages available hardware: GPU Acceleration : Uses GPU for compatible operations (CUDA, OpenCL) SIMD Instructions : Vectorized processing with AVX2/NEON Memory Hierarchy Optimization : Cache-aware algorithms Heterogeneous Computing : Distributes work across CPU, GPU, and specialized processors Performance gains from hardware acceleration: Operation CPU Only With Acceleration Speedup Gaussian Blur 100ms 15ms 6.7\u00d7 Color Conversion 45ms 8ms 5.6\u00d7 Edge Detection 120ms 18ms 6.7\u00d7 Neural Vectorization 8,500ms 950ms 8.9\u00d7","title":"Hardware Acceleration"},{"location":"image-processing/optimization/#algorithm-selection","text":"The system dynamically selects optimal algorithms based on image characteristics: def select_optimal_algorithm ( image , operation , quality_target ): \"\"\"Selects the most efficient algorithm based on image characteristics\"\"\" # Analyze image complexity complexity = analyze_image_complexity ( image ) # Select algorithm based on complexity, operation, and quality target if complexity < 0.3 : # Low complexity image if quality_target == \"high\" : return PreciseAlgorithm ( operation ) else : return FastApproximationAlgorithm ( operation ) elif complexity < 0.7 : # Medium complexity if quality_target == \"high\" : return AdaptiveAlgorithm ( operation ) else : return OptimizedApproximationAlgorithm ( operation ) else : # High complexity if quality_target == \"high\" : return MultiStageAlgorithm ( operation ) else : return AcceleratedApproximationAlgorithm ( operation )","title":"Algorithm Selection"},{"location":"image-processing/optimization/#caching-system","text":"The multi-level caching system significantly improves performance: Cache Level Description Hit Rate Latency Reduction In-memory Result Cache Caches complete results for repeat requests 15-25% 95-100% Intermediate Result Cache Stores partial processing results for reuse 20-30% 40-60% Thumbnail Cache Caches scaled versions for preview and visualization 30-40% 70-90% Disk Cache Persists frequently accessed items to disk 10-15% 80-95% CDN Integration External caching of public results 5-10% 95-100% For detailed caching implementation, see Caching Strategy.","title":"Caching System"},{"location":"image-processing/optimization/#benchmarks-and-performance-metrics","text":"","title":"Benchmarks and Performance Metrics"},{"location":"image-processing/optimization/#operation-performance-by-image-size","text":"Operation 512\u00d7512 1024\u00d71024 2048\u00d72048 4096\u00d74096 Gaussian Blur 12ms 45ms 170ms 650ms Color Adjustment 8ms 30ms 110ms 420ms Edge Detection 15ms 55ms 200ms 780ms Neural Vectorization 950ms 3,200ms 12,500ms 48,000ms SVG Generation 180ms 720ms 2,900ms 11,000ms Measured on reference hardware: 8-core CPU @ 3.5GHz, 16GB RAM, NVIDIA RTX 3060","title":"Operation Performance by Image Size"},{"location":"image-processing/optimization/#throughput-testing","text":"Concurrency Images/Second (512\u00d7512) Images/Second (1024\u00d71024) Memory Usage 1 45 12 0.8GB 4 160 42 2.2GB 8 280 75 4.5GB 16 320 85 9.0GB 32 340 90 17.5GB Test conditions: Basic image processing chain (resize, color adjustment, sharpening)","title":"Throughput Testing"},{"location":"image-processing/optimization/#memory-efficiency-improvements","text":"graph LR A[Original] --> B[v1.0] B -- \"-35%\" --> C[v1.2] C -- \"-25%\" --> D[v1.5] D -- \"-20%\" --> E[Current] style B fill:#f9a,stroke:#333 style C fill:#fd7,stroke:#333 style D fill:#df7,stroke:#333 style E fill:#9f9,stroke:#333 Memory usage reduction over software versions: v1.0: Baseline v1.2: 35% reduction through tiled processing v1.5: Additional 25% reduction through format optimization Current: Further 20% reduction through algorithm refinement","title":"Memory Efficiency Improvements"},{"location":"image-processing/optimization/#configuration-options","text":"The performance optimization system provides several configuration options:","title":"Configuration Options"},{"location":"image-processing/optimization/#resource-management","text":"# Example configuration section performance : # Hardware utilization max_cpu_cores : 0 # 0 = automatic detection gpu_enabled : true gpu_memory_limit : \"2GB\" # Memory management max_memory_usage : \"4GB\" tile_size : 512 enable_memory_monitoring : true # Processing strategy parallel_images : 4 quality_vs_speed : 0.7 # 0=fastest, 1=highest quality # Caching enable_result_cache : true enable_intermediate_cache : true cache_ttl : 3600 # seconds persistent_cache_size : \"10GB\"","title":"Resource Management"},{"location":"image-processing/optimization/#quality-vs-performance-balance","text":"The system offers configurable quality-speed tradeoffs: Quality Setting Description Performance Impact Visual Quality ultra Maximum quality, no compromises Baseline Reference high Visually indistinguishable from ultra 2-3\u00d7 faster 99% of reference balanced Default setting, good tradeoff 5-8\u00d7 faster 95% of reference fast Prioritizes performance 10-15\u00d7 faster 90% of reference draft Maximum performance 20-30\u00d7 faster 80% of reference","title":"Quality vs. Performance Balance"},{"location":"image-processing/optimization/#automatic-profiling-and-tuning","text":"The system includes automatic performance optimization: Runtime Profiling : Monitors execution time and resource usage Bottleneck Detection : Identifies performance bottlenecks Parameter Tuning : Automatically adjusts processing parameters Resource Allocation : Dynamically allocates resources to operations Feedback Loop : Incorporates execution history into future decisions","title":"Automatic Profiling and Tuning"},{"location":"image-processing/optimization/#optimization-for-specific-use-cases","text":"","title":"Optimization for Specific Use Cases"},{"location":"image-processing/optimization/#batch-processing","text":"For processing large numbers of images: from opossum.image import BatchProcessor , OperationChain # Define operation chain chain = OperationChain () chain . add_operation ( \"resize\" , width = 800 , height = 600 ) chain . add_operation ( \"adjust_colors\" , brightness = 1.1 , contrast = 1.2 ) chain . add_operation ( \"watermark\" , text = \"\u00a9 Opossum Search\" ) # Configure batch processor processor = BatchProcessor ( operation_chain = chain , concurrency = 8 , input_dir = \"input_images\" , output_dir = \"processed_images\" , output_format = \"webp\" , quality = 85 , error_handling = \"continue\" # Skip errors and continue processing ) # Process batch results = processor . process () print ( f \"Processed { results . success_count } images, { results . error_count } errors\" ) print ( f \"Average processing time: { results . avg_time_ms } ms per image\" )","title":"Batch Processing"},{"location":"image-processing/optimization/#real-time-processing","text":"For low-latency requirements: Simplified Pipeline : Uses faster approximation algorithms Warm-up Phase : Pre-loads and initializes components Persistent Workers : Maintains worker processes for immediate availability Priority Queue : Processes urgent requests first Deadline-aware Scheduling : Adapts quality based on time constraints","title":"Real-time Processing"},{"location":"image-processing/optimization/#best-practices","text":"","title":"Best Practices"},{"location":"image-processing/optimization/#general-recommendations","text":"Process at Target Resolution : Avoid unnecessary upscaling/downscaling Batch Similar Operations : Group similar images for better resource utilization Consider Output Format : WebP often provides the best quality/size ratio Monitor Resource Usage : Adjust concurrency based on system capabilities Implement Timeouts : Prevent resource exhaustion from problematic images","title":"General Recommendations"},{"location":"image-processing/optimization/#hardware-recommendations","text":"Use Case Recommended Hardware Notes Development 4+ cores, 16GB+ RAM GPU optional but helpful Small Production 8+ cores, 32GB+ RAM, SSD Entry-level GPU recommended Medium Production 16+ cores, 64GB+ RAM, NVMe Mid-range GPU (RTX 3060+) Large Production 32+ cores, 128GB+ RAM, RAID High-end GPU or multiple GPUs Enterprise Distributed processing cluster Multiple specialized nodes","title":"Hardware Recommendations"},{"location":"image-processing/optimization/#code-examples","text":"","title":"Code Examples"},{"location":"image-processing/optimization/#optimized-filter-chain","text":"from opossum.image import OptimizedProcessor # Create optimized processor processor = OptimizedProcessor ( quality_target = \"balanced\" , hardware_acceleration = True , memory_limit = \"4GB\" ) # Process image with optimized chain result = processor . process ( image_path = \"input.jpg\" , operations = [ # Operations are automatically reordered for optimal performance { \"type\" : \"resize\" , \"width\" : 1200 , \"height\" : 800 }, { \"type\" : \"sharpen\" , \"amount\" : 0.3 }, { \"type\" : \"adjust_colors\" , \"brightness\" : 1.1 , \"contrast\" : 1.05 }, { \"type\" : \"add_border\" , \"size\" : 5 , \"color\" : \"#000000\" } ], output_format = \"webp\" , output_quality = 85 )","title":"Optimized Filter Chain"},{"location":"image-processing/optimization/#performance-monitoring","text":"from opossum.image import PerformanceMonitor , process_image # Create performance monitor monitor = PerformanceMonitor () # Process with monitoring with monitor . track ( \"main_processing\" ): # Track overall processing with monitor . track ( \"preprocessing\" ): # Preprocessing steps image = load_image ( \"input.jpg\" ) image = resize_image ( image , width = 1200 ) with monitor . track ( \"filtering\" ): # Apply filters image = apply_filter ( image , \"sharpen\" ) image = adjust_colors ( image , brightness = 1.1 ) with monitor . track ( \"output_generation\" ): # Generate output save_image ( image , \"output.webp\" ) # Print performance report print ( monitor . report ())","title":"Performance Monitoring"},{"location":"image-processing/optimization/#performance-testing-tools","text":"The codebase includes several tools for performance testing: Benchmark Suite : Comprehensive performance testing across operations Profiling Tools : Memory and CPU profiling for optimization Load Testing : Simulates high-concurrency scenarios Regression Testing : Tracks performance changes between versions Hardware Comparison : Tests performance across different hardware configurations","title":"Performance Testing Tools"},{"location":"image-processing/optimization/#future-optimizations","text":"Planned performance improvements include: Neural Compute Optimization : Specialized neural processing for appropriate operations Distributed Processing : Handling extremely large workloads across multiple machines Predictive Resource Allocation : Anticipating resource needs based on input characteristics Custom CUDA Kernels : Highly optimized GPU implementations of critical operations Adaptive Quality Control : Dynamic quality adjustment based on perceptual importance","title":"Future Optimizations"},{"location":"image-processing/optimization/#related-documentation","text":"Image Processing Overview Effects and Filters SVG Generation Caching Strategy Infrastructure Optimization Strategies","title":"Related Documentation"},{"location":"image-processing/overview/","text":"Image Processing Overview \u00b6 Introduction \u00b6 Opossum Search features a robust image processing pipeline designed to handle a wide range of operations from basic image manipulation to complex analysis and SVG generation. This system forms an integral part of Opossum's multimodal capabilities, allowing seamless integration of visual content within the search and response framework. Architecture \u00b6 The image processing pipeline is built on a modular architecture that enables efficient resource utilization and high throughput: graph TD A[Image Input] --> B[Preprocessor] B --> C{Analysis Type} C -->|Object Detection| D[Vision Model] C -->|Filter Application| E[Effect Processor] C -->|Vector Conversion| F[SVG Generator] D --> G[Result Aggregator] E --> G F --> G G --> H[Cache Manager] G --> I[Response Formatter] H <-.-> G Key Components \u00b6 Preprocessor : Handles image validation, normalization, and preparation Analysis Router : Directs the image to appropriate processing modules Vision Model Interface : Connects to Gemini and other vision models Effect Processor : Applies filters and transforms to images SVG Generator : Converts raster images to vector graphics Result Aggregator : Combines outputs from multiple processing steps Cache Manager : Implements intelligent caching for processed results Core Capabilities \u00b6 Image Analysis \u00b6 Object Detection : Identifies and locates objects within images Scene Classification : Categorizes image content and context Text Extraction : Detects and extracts text from images (OCR) Facial Recognition : Identifies facial features (with privacy controls) Content Moderation : Flags potentially inappropriate content Image Manipulation \u00b6 Filtering : Applies visual filters (grayscale, sepia, blur, etc.) Transformations : Resizes, crops, rotates, and adjusts images Composition : Combines multiple images or adds overlays Enhancement : Improves clarity, brightness, contrast, and color balance Special Effects : Adds artistic effects and stylized transformations SVG Processing \u00b6 Raster to Vector Conversion : Transforms bitmap images to scalable vectors Path Optimization : Simplifies vector paths while preserving quality Dynamic SVG Generation : Creates SVGs from data inputs Interactive Elements : Embeds interactive components in SVGs Animation Support : Implements SMIL and CSS animations in SVGs Integration Points \u00b6 The image processing pipeline integrates with other Opossum components: Conversation Management : Enables image-related discussions Model Selection : Routes to appropriate vision models based on task requirements Redis Caching : Leverages the caching system for processed images GraphQL API : Exposes image processing capabilities through the API Service Availability : Implements resilience patterns for reliable operation Performance Characteristics \u00b6 Operation Average Processing Time Cache Impact Basic Analysis 300-500ms 90% reduction Advanced Detection 800-1200ms 85% reduction Filter Application 100-200ms 95% reduction SVG Generation 500-1500ms 80% reduction Combined Operations 1000-2000ms 85% reduction Times measured on standard hardware with 1024\u00d7768 images Resource Management \u00b6 The pipeline implements several strategies to optimize resource usage: Tiered Processing : Scales complexity based on requirements Parallel Execution : Processes compatible operations concurrently Dynamic Scaling : Adjusts resource allocation based on system load Selective Computation : Processes only necessary image regions Result Caching : Stores and reuses processed outputs Common Use Cases \u00b6 Content Discovery : Analyzing images to enhance search relevance Data Visualization : Generating SVG charts and diagrams Visual Response Enhancement : Adding images to conversation responses Content Moderation : Screening for inappropriate imagery Accessibility Improvements : Converting visual content to accessible formats Getting Started \u00b6 To use the image processing capabilities: from opossum.image import ImageProcessor # Initialize processor processor = ImageProcessor ( config = { 'cache_enabled' : True , 'default_model' : 'gemini-vision' , 'fallback_model' : 'local-vision' }) # Process an image result = processor . analyze ( image_path = 'path/to/image.jpg' , operations = [ 'object_detection' , 'text_extraction' ], output_format = 'json' ) # Apply filters filtered = processor . apply_filter ( image_path = 'path/to/image.jpg' , filter_name = 'nostalgic' , intensity = 0.7 ) # Generate SVG svg = processor . to_svg ( image_path = 'path/to/image.jpg' , optimization_level = 'medium' , include_interactive = True ) Related Documentation \u00b6 For more detailed information, refer to: Effects and Filters: Detailed documentation on available filters SVG Generation: In-depth guide to SVG capabilities Performance Optimization: Strategies for maximizing performance Caching Strategy: How caching is implemented for image processing Vision Model Integration: Details on vision model providers Technical Requirements \u00b6 Supported input formats: JPEG, PNG, GIF, WEBP, HEIC, BMP Maximum input size: 20MB (configurable) Recommended minimum resolution: 300\u00d7300 pixels Optimal resolution range: 800\u00d7600 to 1920\u00d71080 pixels SVG output complies with SVG 1.1 specifications Future Developments \u00b6 Upcoming enhancements to the image processing pipeline include: Extended video frame processing capabilities Real-time image stream analysis Advanced style transfer algorithms 3D model rendering from 2D images Expanded medical and scientific image analysis Note The image processing capabilities of Opossum Search continue to evolve, with new features and optimizations regularly added to enhance the system's multimodal capabilities. Related Documentation \u00b6 Effects and Filters SVG Generation Performance Optimization Caching Strategy Infrastructure Optimization Strategies","title":"Overview"},{"location":"image-processing/overview/#image-processing-overview","text":"","title":"Image Processing Overview"},{"location":"image-processing/overview/#introduction","text":"Opossum Search features a robust image processing pipeline designed to handle a wide range of operations from basic image manipulation to complex analysis and SVG generation. This system forms an integral part of Opossum's multimodal capabilities, allowing seamless integration of visual content within the search and response framework.","title":"Introduction"},{"location":"image-processing/overview/#architecture","text":"The image processing pipeline is built on a modular architecture that enables efficient resource utilization and high throughput: graph TD A[Image Input] --> B[Preprocessor] B --> C{Analysis Type} C -->|Object Detection| D[Vision Model] C -->|Filter Application| E[Effect Processor] C -->|Vector Conversion| F[SVG Generator] D --> G[Result Aggregator] E --> G F --> G G --> H[Cache Manager] G --> I[Response Formatter] H <-.-> G","title":"Architecture"},{"location":"image-processing/overview/#key-components","text":"Preprocessor : Handles image validation, normalization, and preparation Analysis Router : Directs the image to appropriate processing modules Vision Model Interface : Connects to Gemini and other vision models Effect Processor : Applies filters and transforms to images SVG Generator : Converts raster images to vector graphics Result Aggregator : Combines outputs from multiple processing steps Cache Manager : Implements intelligent caching for processed results","title":"Key Components"},{"location":"image-processing/overview/#core-capabilities","text":"","title":"Core Capabilities"},{"location":"image-processing/overview/#image-analysis","text":"Object Detection : Identifies and locates objects within images Scene Classification : Categorizes image content and context Text Extraction : Detects and extracts text from images (OCR) Facial Recognition : Identifies facial features (with privacy controls) Content Moderation : Flags potentially inappropriate content","title":"Image Analysis"},{"location":"image-processing/overview/#image-manipulation","text":"Filtering : Applies visual filters (grayscale, sepia, blur, etc.) Transformations : Resizes, crops, rotates, and adjusts images Composition : Combines multiple images or adds overlays Enhancement : Improves clarity, brightness, contrast, and color balance Special Effects : Adds artistic effects and stylized transformations","title":"Image Manipulation"},{"location":"image-processing/overview/#svg-processing","text":"Raster to Vector Conversion : Transforms bitmap images to scalable vectors Path Optimization : Simplifies vector paths while preserving quality Dynamic SVG Generation : Creates SVGs from data inputs Interactive Elements : Embeds interactive components in SVGs Animation Support : Implements SMIL and CSS animations in SVGs","title":"SVG Processing"},{"location":"image-processing/overview/#integration-points","text":"The image processing pipeline integrates with other Opossum components: Conversation Management : Enables image-related discussions Model Selection : Routes to appropriate vision models based on task requirements Redis Caching : Leverages the caching system for processed images GraphQL API : Exposes image processing capabilities through the API Service Availability : Implements resilience patterns for reliable operation","title":"Integration Points"},{"location":"image-processing/overview/#performance-characteristics","text":"Operation Average Processing Time Cache Impact Basic Analysis 300-500ms 90% reduction Advanced Detection 800-1200ms 85% reduction Filter Application 100-200ms 95% reduction SVG Generation 500-1500ms 80% reduction Combined Operations 1000-2000ms 85% reduction Times measured on standard hardware with 1024\u00d7768 images","title":"Performance Characteristics"},{"location":"image-processing/overview/#resource-management","text":"The pipeline implements several strategies to optimize resource usage: Tiered Processing : Scales complexity based on requirements Parallel Execution : Processes compatible operations concurrently Dynamic Scaling : Adjusts resource allocation based on system load Selective Computation : Processes only necessary image regions Result Caching : Stores and reuses processed outputs","title":"Resource Management"},{"location":"image-processing/overview/#common-use-cases","text":"Content Discovery : Analyzing images to enhance search relevance Data Visualization : Generating SVG charts and diagrams Visual Response Enhancement : Adding images to conversation responses Content Moderation : Screening for inappropriate imagery Accessibility Improvements : Converting visual content to accessible formats","title":"Common Use Cases"},{"location":"image-processing/overview/#getting-started","text":"To use the image processing capabilities: from opossum.image import ImageProcessor # Initialize processor processor = ImageProcessor ( config = { 'cache_enabled' : True , 'default_model' : 'gemini-vision' , 'fallback_model' : 'local-vision' }) # Process an image result = processor . analyze ( image_path = 'path/to/image.jpg' , operations = [ 'object_detection' , 'text_extraction' ], output_format = 'json' ) # Apply filters filtered = processor . apply_filter ( image_path = 'path/to/image.jpg' , filter_name = 'nostalgic' , intensity = 0.7 ) # Generate SVG svg = processor . to_svg ( image_path = 'path/to/image.jpg' , optimization_level = 'medium' , include_interactive = True )","title":"Getting Started"},{"location":"image-processing/overview/#related-documentation","text":"For more detailed information, refer to: Effects and Filters: Detailed documentation on available filters SVG Generation: In-depth guide to SVG capabilities Performance Optimization: Strategies for maximizing performance Caching Strategy: How caching is implemented for image processing Vision Model Integration: Details on vision model providers","title":"Related Documentation"},{"location":"image-processing/overview/#technical-requirements","text":"Supported input formats: JPEG, PNG, GIF, WEBP, HEIC, BMP Maximum input size: 20MB (configurable) Recommended minimum resolution: 300\u00d7300 pixels Optimal resolution range: 800\u00d7600 to 1920\u00d71080 pixels SVG output complies with SVG 1.1 specifications","title":"Technical Requirements"},{"location":"image-processing/overview/#future-developments","text":"Upcoming enhancements to the image processing pipeline include: Extended video frame processing capabilities Real-time image stream analysis Advanced style transfer algorithms 3D model rendering from 2D images Expanded medical and scientific image analysis Note The image processing capabilities of Opossum Search continue to evolve, with new features and optimizations regularly added to enhance the system's multimodal capabilities.","title":"Future Developments"},{"location":"image-processing/overview/#related-documentation_1","text":"Effects and Filters SVG Generation Performance Optimization Caching Strategy Infrastructure Optimization Strategies","title":"Related Documentation"},{"location":"image-processing/svg-generation/","text":"SVG Generation \u00b6 Opossum Search provides a sophisticated SVG (Scalable Vector Graphics) generation system that combines two complementary approaches: a native template-based system and an AI-powered Chat2SVG integration. This document explains both systems, their technical capabilities, and guidelines for when to use each approach. Overview of SVG Generation Approaches \u00b6 Feature Native Template System Chat2SVG Integration Primary Use Case Operational visualizations & dashboards Creative, custom imagery Source of Intelligence Predefined templates with data binding Three-stage AI pipeline with LLMs and diffusion models Response Time Fast (milliseconds) Slower (seconds) Resource Usage Very low Higher (especially with detail enhancement) Customization Limited to template parameters Open-ended text-to-image generation Offline Capability Full support Depends on configuration Reliability Very high Depends on model availability Core Capabilities \u00b6 Raster to Vector Conversion \u00b6 The system transforms bitmap images into scalable vector graphics through multiple techniques: Conversion Method Best For Accuracy Performance Cost Contour Tracing Simple shapes, logos High Low-Medium Color Quantization Illustrations, cartoons Medium-High Medium Path Simplification Complex photographs Medium High Neural Vectorization Photographs, complex images Very High Extreme Data Visualization Generation \u00b6 The SVG generator creates various data visualizations from structured input: Charts : Bar, line, area, pie, radar, and scatter plots Diagrams : Flow charts, network diagrams, tree structures Maps : Choropleth, bubble, and heatmap visualizations Custom Visualizations : Domain-specific visual representations Dashboards : Multi-component interactive displays Interactive Elements \u00b6 SVG output can include interactive capabilities: Hover Effects : Visual feedback on element hovering Click Actions : Interaction with individual SVG components Animations : SMIL and CSS-based transitions and effects Zoom and Pan : Navigation within complex visualizations Tooltips : Contextual information displays Implementation Architecture \u00b6 graph TB A[Input Source] --> B{Source Type} B -->|Raster Image| C[Image Processor] B -->|Data Structure| D[Visualization Engine] B -->|SVG Template| E[Template Processor] B -->|Text Prompt| F[Chat2SVG] C --> G[Trace Engine] G --> H[Path Optimizer] D --> I[Layout Engine] I --> J[Element Generator] E --> K[Binding Engine] K --> L[Template Renderer] F --> M[Template Generation] M --> N[Detail Enhancement] N --> O[SVG Optimization] H --> P[SVG Composer] J --> P L --> P O --> P P --> Q[Optimizer] Q --> R[Output Generator] 1. Native Template System \u00b6 The native SVG generation system uses predefined templates with data binding to create operational visualizations. Available Visualization Types \u00b6 Service Status - Real-time dashboard of service availability Failover Process - Diagram showing the failover flow when services become unavailable Capability Degradation - Visualization of capability levels across different service states Implementation \u00b6 The native system uses a simple, efficient approach: // Example of generating a service status visualization async function generateSVGVisualization ( svgType ) { const response = await fetch ( '/generate-svg' , { method : 'POST' , headers : { 'Content-Type' : 'application/json' }, body : JSON . stringify ({ type : svgType , data : {} // Populated with real-time data }) }); const data = await response . json (); addSVGMessage ( data . svg_content , data . base64_image ); } Benefits \u00b6 Speed - Generates visualizations in milliseconds Reliability - No dependency on external AI services Consistency - Predictable output format every time Resource Efficiency - Minimal CPU and memory usage Use Cases \u00b6 Real-time operational dashboards System status visualizations Process flow diagrams Error state illustrations Easter egg visuals for special features Data Visualization Example \u00b6 from opossum.svg import ChartGenerator # Create chart data data = [ { 'month' : 'Jan' , 'value' : 42 }, { 'month' : 'Feb' , 'value' : 56 }, { 'month' : 'Mar' , 'value' : 35 }, { 'month' : 'Apr' , 'value' : 68 }, { 'month' : 'May' , 'value' : 89 } ] # Generate bar chart chart = ChartGenerator . bar_chart ( data = data , x_key = 'month' , y_key = 'value' , width = 800 , height = 400 , colors = [ '#4285F4' , '#34A853' , '#FBBC05' , '#EA4335' , '#8142F4' ], title = 'Monthly Performance' , animate = True ) # Get SVG string svg_content = chart . to_svg () Template-Based Generation \u00b6 from opossum.svg import TemplateRenderer # Define data for template data = { 'title' : 'System Status' , 'components' : [ { 'name' : 'API' , 'status' : 'healthy' , 'uptime' : 99.98 }, { 'name' : 'Database' , 'status' : 'degraded' , 'uptime' : 98.5 }, { 'name' : 'Cache' , 'status' : 'healthy' , 'uptime' : 100.0 } ], 'last_updated' : '2023-03-15T14:32:45Z' } # Render template with data renderer = TemplateRenderer () svg_content = renderer . render ( template_name = 'system_status' , data = data , options = { 'width' : 600 , 'height' : 300 } ) 2. Chat2SVG Integration \u00b6 The Chat2SVG integration uses a sophisticated AI pipeline to generate custom vector graphics from text descriptions. How Chat2SVG Works \u00b6 The system uses a three-stage pipeline: Template Generation - Uses large language models to create initial SVG templates Detail Enhancement - Leverages image diffusion models to add visual details (optional) SVG Optimization - Optimizes vector paths using VAE models Model Quantization \u00b6 To optimize performance and memory usage, Opossum Search supports model quantization for Chat2SVG: Quantization Options \u00b6 Precision Description Memory Reduction Quality Impact fp32 Full precision (32-bit) 1.0x (baseline) None fp16 Half precision (16-bit) 0.5x (50% reduction) Very Low int8 8-bit integer quantization 0.25x (75% reduction) Low Configuration \u00b6 Model quantization can be configured through environment variables or the config file: # Production configuration example (in config.py) MODEL_QUANTIZATION_ENABLED = True MODEL_QUANTIZATION_PRECISION = \"int8\" # fp32, fp16, int8 CHAT2SVG_QUANTIZE_MODELS = True CHAT2SVG_QUANTIZATION_PRECISION = \"fp16\" Quantization Process \u00b6 The system includes a dedicated script ( scripts/quantize_models.py ) that automatically: Converts Transformers models to optimized ONNX format Quantizes Chat2SVG PyTorch models to FP16 precision Validates the quantized models Configures the application to use the optimized models This process runs during container startup or can be triggered manually: # Run quantization manually python scripts/quantize_models.py --precision int8 Benefits of Quantization \u00b6 Reduced Memory Usage : Up to 75% memory reduction with INT8 quantization Faster Inference : Potential speedup, especially on hardware with INT8 support Container Optimization : Smaller memory footprint in containerized deployments Consistent Performance : Predictable memory usage across different environments Implementation \u00b6 async function generateTextToSVG ( prompt ) { const mutation = ` mutation GenerateSVG($input: SVGGenerationInput!) { generate_svg(input: $input) { svg_content base64_image metadata } } ` ; const variables = { input : { prompt : prompt , style : \"default\" } }; const data = await graphqlRequest ( mutation , variables ); addSVGMessage ( data . generate_svg . svg_content , data . generate_svg . base64_image ); } Configuration Options \u00b6 Configure Chat2SVG integration through environment variables: CHAT2SVG_ENABLED - Enable/disable the Chat2SVG integration (default: true) CHAT2SVG_DETAIL_ENHANCEMENT - Enable/disable the detail enhancement stage (default: false) CHAT2SVG_PATH - Path to Chat2SVG installation CHAT2SVG_QUANTIZE_MODELS - Enable model quantization (default: true) CHAT2SVG_QUANTIZATION_PRECISION - Quantization precision (fp32, fp16, int8) (default: fp16) Benefits \u00b6 Creative Freedom - Generate any concept described in natural language Custom Imagery - Create unique, tailored visualizations Artistic Styles - Support for various visual styles Complex Scenes - Ability to generate intricate visual compositions Resource-Aware - Adaptive resource usage with model quantization Use Cases \u00b6 Custom user-requested visualizations Decorative elements and illustrations Creative images for special events Visual explanations of complex concepts National Opossum Day special visualizations GraphQL API Usage \u00b6 mutation { generateSVG ( input : { prompt : \"An opossum hanging from a tree branch under a full moon\" , style : \"minimalist\" } ) { svg_content base64_image metadata { generation_time resource_level quantization_level } } } Decision Guidelines \u00b6 Use the Native Template System when: \u00b6 You need operational visualizations with real-time data Speed and reliability are critical You need consistent, predictable output You're working with limited resources The visualization is part of a critical system feature You need high performance at scale Use Chat2SVG when: \u00b6 You need creative, novel visualizations The user has requested a specific visual concept You want to generate illustrative examples You're enhancing special features like National Opossum Day Resource constraints are not a concern The request falls outside existing templates Hybrid Approach \u00b6 For optimal results, consider a hybrid approach: Use native templates for operational data visualization Use Chat2SVG for supplementary illustrations and unique requests Cache frequently requested Chat2SVG generations for performance Technical Details \u00b6 Advanced Techniques \u00b6 Image Vectorization Options \u00b6 The image vectorization engine provides several parameters to control the output: Parameter Description Values Default max_colors Maximum colors in output 2-256 16 detail_level Level of detail to preserve 'low', 'medium', 'high', 'ultra' 'medium' smoothing Path smoothing factor 0.0-1.0 0.5 min_path_length Minimum path length to include 0-100 pixels 5 corner_threshold Angle for detecting corners 10-170 degrees 120 Animation Options \u00b6 SVG animations can be configured with these parameters: Parameter Description Values Default animation_type Type of animation 'fade', 'grow', 'slide', 'bounce', 'custom' 'fade' duration Animation duration 100-10000ms 500 delay Delay before animation starts 0-10000ms 0 easing Easing function 'linear', 'ease', 'ease-in', 'ease-out', 'ease-in-out' 'ease' repeat_count Number of repetitions 0-infinite 1 Optimization Techniques \u00b6 The SVG optimization engine improves output quality and file size: Size Optimization \u00b6 Path Simplification : Reduces point count while maintaining shape Decimal Precision : Truncates coordinates to necessary precision SVGO Integration : Applies comprehensive optimization techniques Attribute Minimization : Removes redundant or default attributes ID Shortening : Reduces the length of internal IDs Rendering Optimization \u00b6 Paint Order Adjustment : Optimizes rendering order for performance Text to Path Conversion : Ensures consistent display without fonts Gradient Simplification : Simplifies complex gradients Filter Effect Optimization : Streamlines filter definitions View Box Normalization : Standardizes coordinate systems Fallback Mechanism \u00b6 If Chat2SVG fails or is unavailable, the system automatically falls back to a simpler template: def _fallback_svg ( self , prompt : str ) -> str : \"\"\"Generate a simple fallback SVG when the main pipeline fails.\"\"\" return f \"\"\" <svg viewBox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"> <!-- Fallback SVG for: { prompt } --> <rect x=\"50\" y=\"50\" width=\"412\" height=\"412\" fill=\"#f8f8f8\" stroke=\"#ccc\" stroke-width=\"2\" /> <text x=\"256\" y=\"236\" text-anchor=\"middle\" font-size=\"24\" fill=\"#333\">SVG Generation</text> <text x=\"256\" y=\"276\" text-anchor=\"middle\" font-size=\"18\" fill=\"#666\">\" { prompt } \"</text> </svg> \"\"\" Caching \u00b6 Both systems implement caching to improve performance: Native templates cache visualizations for 5 minutes Chat2SVG generations are cached for 1 hour Performance Considerations \u00b6 Operation Small Image (<500px) Medium Image (500-1500px) Large Image (>1500px) Native Template Rendering 0.1-0.2s 0.2-0.5s 0.5-1.5s Chart Generation 0.1-0.3s 0.3-0.8s 0.8-2s Chat2SVG Basic 1-3s 3-8s 8-15s Chat2SVG with Detail Enhancement 5-10s 10-30s 30-90s File Size Implications \u00b6 Content Type Raster Equivalent (PNG) SVG Output Reduction Factor Simple Logo 50-100KB 5-15KB 5-10\u00d7 Chart/Diagram 100-500KB 10-50KB 8-12\u00d7 Illustration 500KB-2MB 50-200KB 10-15\u00d7 Chat2SVG Output 1-5MB 50-300KB 10-20\u00d7 Browser Compatibility \u00b6 The generated SVGs are tested for compatibility with: Chrome 60+ Firefox 60+ Safari 10+ Edge 16+ Opera 50+ iOS Safari 10+ Android Browser 67+ Special Feature: National Opossum Day \u00b6 On October 18th (National Opossum Day), the system uses Chat2SVG to enhance the celebration with special opossum-themed visualizations: # Example usage for National Opossum Day if is_national_opossum_day (): prompt = \"A celebratory opossum wearing a party hat for National Opossum Day\" svg_content = await chat2svg_generator . generate_svg_from_prompt ( prompt ) # Use the SVG in the celebration features This integration enhances the existing Easter egg functionality, providing users with delightful custom visualizations during the special event. Future Enhancements \u00b6 Style control parameters for guided Chat2SVG generation Additional template categories for the native system Integration of chat context for more relevant visualizations Performance optimizations for Chat2SVG pipeline Animation Sequences : Multi-step animation capabilities 3D Perspective : 3D-like effects in SVG Pattern Recognition : Improved pattern detection for vectorization Collaborative Editing : Real-time multi-user SVG editing Appendix: SVG Optimization Results \u00b6 Sample optimization results for different input types: Input Type Original SVG Size Optimized Size Reduction Rendering Performance Improvement Logo 45.2KB 12.8KB 71.7% 22% faster Chart 128.5KB 36.2KB 71.8% 35% faster Map 876.3KB 215.9KB 75.4% 48% faster Chat2SVG Output 520KB 180KB 65.4% 38% faster Related Documentation \u00b6 Image Processing Overview Effects and Filters Performance Optimization Caching Strategy National Opossum Day","title":"SVG Generation"},{"location":"image-processing/svg-generation/#svg-generation","text":"Opossum Search provides a sophisticated SVG (Scalable Vector Graphics) generation system that combines two complementary approaches: a native template-based system and an AI-powered Chat2SVG integration. This document explains both systems, their technical capabilities, and guidelines for when to use each approach.","title":"SVG Generation"},{"location":"image-processing/svg-generation/#overview-of-svg-generation-approaches","text":"Feature Native Template System Chat2SVG Integration Primary Use Case Operational visualizations & dashboards Creative, custom imagery Source of Intelligence Predefined templates with data binding Three-stage AI pipeline with LLMs and diffusion models Response Time Fast (milliseconds) Slower (seconds) Resource Usage Very low Higher (especially with detail enhancement) Customization Limited to template parameters Open-ended text-to-image generation Offline Capability Full support Depends on configuration Reliability Very high Depends on model availability","title":"Overview of SVG Generation Approaches"},{"location":"image-processing/svg-generation/#core-capabilities","text":"","title":"Core Capabilities"},{"location":"image-processing/svg-generation/#raster-to-vector-conversion","text":"The system transforms bitmap images into scalable vector graphics through multiple techniques: Conversion Method Best For Accuracy Performance Cost Contour Tracing Simple shapes, logos High Low-Medium Color Quantization Illustrations, cartoons Medium-High Medium Path Simplification Complex photographs Medium High Neural Vectorization Photographs, complex images Very High Extreme","title":"Raster to Vector Conversion"},{"location":"image-processing/svg-generation/#data-visualization-generation","text":"The SVG generator creates various data visualizations from structured input: Charts : Bar, line, area, pie, radar, and scatter plots Diagrams : Flow charts, network diagrams, tree structures Maps : Choropleth, bubble, and heatmap visualizations Custom Visualizations : Domain-specific visual representations Dashboards : Multi-component interactive displays","title":"Data Visualization Generation"},{"location":"image-processing/svg-generation/#interactive-elements","text":"SVG output can include interactive capabilities: Hover Effects : Visual feedback on element hovering Click Actions : Interaction with individual SVG components Animations : SMIL and CSS-based transitions and effects Zoom and Pan : Navigation within complex visualizations Tooltips : Contextual information displays","title":"Interactive Elements"},{"location":"image-processing/svg-generation/#implementation-architecture","text":"graph TB A[Input Source] --> B{Source Type} B -->|Raster Image| C[Image Processor] B -->|Data Structure| D[Visualization Engine] B -->|SVG Template| E[Template Processor] B -->|Text Prompt| F[Chat2SVG] C --> G[Trace Engine] G --> H[Path Optimizer] D --> I[Layout Engine] I --> J[Element Generator] E --> K[Binding Engine] K --> L[Template Renderer] F --> M[Template Generation] M --> N[Detail Enhancement] N --> O[SVG Optimization] H --> P[SVG Composer] J --> P L --> P O --> P P --> Q[Optimizer] Q --> R[Output Generator]","title":"Implementation Architecture"},{"location":"image-processing/svg-generation/#1-native-template-system","text":"The native SVG generation system uses predefined templates with data binding to create operational visualizations.","title":"1. Native Template System"},{"location":"image-processing/svg-generation/#available-visualization-types","text":"Service Status - Real-time dashboard of service availability Failover Process - Diagram showing the failover flow when services become unavailable Capability Degradation - Visualization of capability levels across different service states","title":"Available Visualization Types"},{"location":"image-processing/svg-generation/#implementation","text":"The native system uses a simple, efficient approach: // Example of generating a service status visualization async function generateSVGVisualization ( svgType ) { const response = await fetch ( '/generate-svg' , { method : 'POST' , headers : { 'Content-Type' : 'application/json' }, body : JSON . stringify ({ type : svgType , data : {} // Populated with real-time data }) }); const data = await response . json (); addSVGMessage ( data . svg_content , data . base64_image ); }","title":"Implementation"},{"location":"image-processing/svg-generation/#benefits","text":"Speed - Generates visualizations in milliseconds Reliability - No dependency on external AI services Consistency - Predictable output format every time Resource Efficiency - Minimal CPU and memory usage","title":"Benefits"},{"location":"image-processing/svg-generation/#use-cases","text":"Real-time operational dashboards System status visualizations Process flow diagrams Error state illustrations Easter egg visuals for special features","title":"Use Cases"},{"location":"image-processing/svg-generation/#data-visualization-example","text":"from opossum.svg import ChartGenerator # Create chart data data = [ { 'month' : 'Jan' , 'value' : 42 }, { 'month' : 'Feb' , 'value' : 56 }, { 'month' : 'Mar' , 'value' : 35 }, { 'month' : 'Apr' , 'value' : 68 }, { 'month' : 'May' , 'value' : 89 } ] # Generate bar chart chart = ChartGenerator . bar_chart ( data = data , x_key = 'month' , y_key = 'value' , width = 800 , height = 400 , colors = [ '#4285F4' , '#34A853' , '#FBBC05' , '#EA4335' , '#8142F4' ], title = 'Monthly Performance' , animate = True ) # Get SVG string svg_content = chart . to_svg ()","title":"Data Visualization Example"},{"location":"image-processing/svg-generation/#template-based-generation","text":"from opossum.svg import TemplateRenderer # Define data for template data = { 'title' : 'System Status' , 'components' : [ { 'name' : 'API' , 'status' : 'healthy' , 'uptime' : 99.98 }, { 'name' : 'Database' , 'status' : 'degraded' , 'uptime' : 98.5 }, { 'name' : 'Cache' , 'status' : 'healthy' , 'uptime' : 100.0 } ], 'last_updated' : '2023-03-15T14:32:45Z' } # Render template with data renderer = TemplateRenderer () svg_content = renderer . render ( template_name = 'system_status' , data = data , options = { 'width' : 600 , 'height' : 300 } )","title":"Template-Based Generation"},{"location":"image-processing/svg-generation/#2-chat2svg-integration","text":"The Chat2SVG integration uses a sophisticated AI pipeline to generate custom vector graphics from text descriptions.","title":"2. Chat2SVG Integration"},{"location":"image-processing/svg-generation/#how-chat2svg-works","text":"The system uses a three-stage pipeline: Template Generation - Uses large language models to create initial SVG templates Detail Enhancement - Leverages image diffusion models to add visual details (optional) SVG Optimization - Optimizes vector paths using VAE models","title":"How Chat2SVG Works"},{"location":"image-processing/svg-generation/#model-quantization","text":"To optimize performance and memory usage, Opossum Search supports model quantization for Chat2SVG:","title":"Model Quantization"},{"location":"image-processing/svg-generation/#quantization-options","text":"Precision Description Memory Reduction Quality Impact fp32 Full precision (32-bit) 1.0x (baseline) None fp16 Half precision (16-bit) 0.5x (50% reduction) Very Low int8 8-bit integer quantization 0.25x (75% reduction) Low","title":"Quantization Options"},{"location":"image-processing/svg-generation/#configuration","text":"Model quantization can be configured through environment variables or the config file: # Production configuration example (in config.py) MODEL_QUANTIZATION_ENABLED = True MODEL_QUANTIZATION_PRECISION = \"int8\" # fp32, fp16, int8 CHAT2SVG_QUANTIZE_MODELS = True CHAT2SVG_QUANTIZATION_PRECISION = \"fp16\"","title":"Configuration"},{"location":"image-processing/svg-generation/#quantization-process","text":"The system includes a dedicated script ( scripts/quantize_models.py ) that automatically: Converts Transformers models to optimized ONNX format Quantizes Chat2SVG PyTorch models to FP16 precision Validates the quantized models Configures the application to use the optimized models This process runs during container startup or can be triggered manually: # Run quantization manually python scripts/quantize_models.py --precision int8","title":"Quantization Process"},{"location":"image-processing/svg-generation/#benefits-of-quantization","text":"Reduced Memory Usage : Up to 75% memory reduction with INT8 quantization Faster Inference : Potential speedup, especially on hardware with INT8 support Container Optimization : Smaller memory footprint in containerized deployments Consistent Performance : Predictable memory usage across different environments","title":"Benefits of Quantization"},{"location":"image-processing/svg-generation/#implementation_1","text":"async function generateTextToSVG ( prompt ) { const mutation = ` mutation GenerateSVG($input: SVGGenerationInput!) { generate_svg(input: $input) { svg_content base64_image metadata } } ` ; const variables = { input : { prompt : prompt , style : \"default\" } }; const data = await graphqlRequest ( mutation , variables ); addSVGMessage ( data . generate_svg . svg_content , data . generate_svg . base64_image ); }","title":"Implementation"},{"location":"image-processing/svg-generation/#configuration-options","text":"Configure Chat2SVG integration through environment variables: CHAT2SVG_ENABLED - Enable/disable the Chat2SVG integration (default: true) CHAT2SVG_DETAIL_ENHANCEMENT - Enable/disable the detail enhancement stage (default: false) CHAT2SVG_PATH - Path to Chat2SVG installation CHAT2SVG_QUANTIZE_MODELS - Enable model quantization (default: true) CHAT2SVG_QUANTIZATION_PRECISION - Quantization precision (fp32, fp16, int8) (default: fp16)","title":"Configuration Options"},{"location":"image-processing/svg-generation/#benefits_1","text":"Creative Freedom - Generate any concept described in natural language Custom Imagery - Create unique, tailored visualizations Artistic Styles - Support for various visual styles Complex Scenes - Ability to generate intricate visual compositions Resource-Aware - Adaptive resource usage with model quantization","title":"Benefits"},{"location":"image-processing/svg-generation/#use-cases_1","text":"Custom user-requested visualizations Decorative elements and illustrations Creative images for special events Visual explanations of complex concepts National Opossum Day special visualizations","title":"Use Cases"},{"location":"image-processing/svg-generation/#graphql-api-usage","text":"mutation { generateSVG ( input : { prompt : \"An opossum hanging from a tree branch under a full moon\" , style : \"minimalist\" } ) { svg_content base64_image metadata { generation_time resource_level quantization_level } } }","title":"GraphQL API Usage"},{"location":"image-processing/svg-generation/#decision-guidelines","text":"","title":"Decision Guidelines"},{"location":"image-processing/svg-generation/#use-the-native-template-system-when","text":"You need operational visualizations with real-time data Speed and reliability are critical You need consistent, predictable output You're working with limited resources The visualization is part of a critical system feature You need high performance at scale","title":"Use the Native Template System when:"},{"location":"image-processing/svg-generation/#use-chat2svg-when","text":"You need creative, novel visualizations The user has requested a specific visual concept You want to generate illustrative examples You're enhancing special features like National Opossum Day Resource constraints are not a concern The request falls outside existing templates","title":"Use Chat2SVG when:"},{"location":"image-processing/svg-generation/#hybrid-approach","text":"For optimal results, consider a hybrid approach: Use native templates for operational data visualization Use Chat2SVG for supplementary illustrations and unique requests Cache frequently requested Chat2SVG generations for performance","title":"Hybrid Approach"},{"location":"image-processing/svg-generation/#technical-details","text":"","title":"Technical Details"},{"location":"image-processing/svg-generation/#advanced-techniques","text":"","title":"Advanced Techniques"},{"location":"image-processing/svg-generation/#image-vectorization-options","text":"The image vectorization engine provides several parameters to control the output: Parameter Description Values Default max_colors Maximum colors in output 2-256 16 detail_level Level of detail to preserve 'low', 'medium', 'high', 'ultra' 'medium' smoothing Path smoothing factor 0.0-1.0 0.5 min_path_length Minimum path length to include 0-100 pixels 5 corner_threshold Angle for detecting corners 10-170 degrees 120","title":"Image Vectorization Options"},{"location":"image-processing/svg-generation/#animation-options","text":"SVG animations can be configured with these parameters: Parameter Description Values Default animation_type Type of animation 'fade', 'grow', 'slide', 'bounce', 'custom' 'fade' duration Animation duration 100-10000ms 500 delay Delay before animation starts 0-10000ms 0 easing Easing function 'linear', 'ease', 'ease-in', 'ease-out', 'ease-in-out' 'ease' repeat_count Number of repetitions 0-infinite 1","title":"Animation Options"},{"location":"image-processing/svg-generation/#optimization-techniques","text":"The SVG optimization engine improves output quality and file size:","title":"Optimization Techniques"},{"location":"image-processing/svg-generation/#size-optimization","text":"Path Simplification : Reduces point count while maintaining shape Decimal Precision : Truncates coordinates to necessary precision SVGO Integration : Applies comprehensive optimization techniques Attribute Minimization : Removes redundant or default attributes ID Shortening : Reduces the length of internal IDs","title":"Size Optimization"},{"location":"image-processing/svg-generation/#rendering-optimization","text":"Paint Order Adjustment : Optimizes rendering order for performance Text to Path Conversion : Ensures consistent display without fonts Gradient Simplification : Simplifies complex gradients Filter Effect Optimization : Streamlines filter definitions View Box Normalization : Standardizes coordinate systems","title":"Rendering Optimization"},{"location":"image-processing/svg-generation/#fallback-mechanism","text":"If Chat2SVG fails or is unavailable, the system automatically falls back to a simpler template: def _fallback_svg ( self , prompt : str ) -> str : \"\"\"Generate a simple fallback SVG when the main pipeline fails.\"\"\" return f \"\"\" <svg viewBox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"> <!-- Fallback SVG for: { prompt } --> <rect x=\"50\" y=\"50\" width=\"412\" height=\"412\" fill=\"#f8f8f8\" stroke=\"#ccc\" stroke-width=\"2\" /> <text x=\"256\" y=\"236\" text-anchor=\"middle\" font-size=\"24\" fill=\"#333\">SVG Generation</text> <text x=\"256\" y=\"276\" text-anchor=\"middle\" font-size=\"18\" fill=\"#666\">\" { prompt } \"</text> </svg> \"\"\"","title":"Fallback Mechanism"},{"location":"image-processing/svg-generation/#caching","text":"Both systems implement caching to improve performance: Native templates cache visualizations for 5 minutes Chat2SVG generations are cached for 1 hour","title":"Caching"},{"location":"image-processing/svg-generation/#performance-considerations","text":"Operation Small Image (<500px) Medium Image (500-1500px) Large Image (>1500px) Native Template Rendering 0.1-0.2s 0.2-0.5s 0.5-1.5s Chart Generation 0.1-0.3s 0.3-0.8s 0.8-2s Chat2SVG Basic 1-3s 3-8s 8-15s Chat2SVG with Detail Enhancement 5-10s 10-30s 30-90s","title":"Performance Considerations"},{"location":"image-processing/svg-generation/#file-size-implications","text":"Content Type Raster Equivalent (PNG) SVG Output Reduction Factor Simple Logo 50-100KB 5-15KB 5-10\u00d7 Chart/Diagram 100-500KB 10-50KB 8-12\u00d7 Illustration 500KB-2MB 50-200KB 10-15\u00d7 Chat2SVG Output 1-5MB 50-300KB 10-20\u00d7","title":"File Size Implications"},{"location":"image-processing/svg-generation/#browser-compatibility","text":"The generated SVGs are tested for compatibility with: Chrome 60+ Firefox 60+ Safari 10+ Edge 16+ Opera 50+ iOS Safari 10+ Android Browser 67+","title":"Browser Compatibility"},{"location":"image-processing/svg-generation/#special-feature-national-opossum-day","text":"On October 18th (National Opossum Day), the system uses Chat2SVG to enhance the celebration with special opossum-themed visualizations: # Example usage for National Opossum Day if is_national_opossum_day (): prompt = \"A celebratory opossum wearing a party hat for National Opossum Day\" svg_content = await chat2svg_generator . generate_svg_from_prompt ( prompt ) # Use the SVG in the celebration features This integration enhances the existing Easter egg functionality, providing users with delightful custom visualizations during the special event.","title":"Special Feature: National Opossum Day"},{"location":"image-processing/svg-generation/#future-enhancements","text":"Style control parameters for guided Chat2SVG generation Additional template categories for the native system Integration of chat context for more relevant visualizations Performance optimizations for Chat2SVG pipeline Animation Sequences : Multi-step animation capabilities 3D Perspective : 3D-like effects in SVG Pattern Recognition : Improved pattern detection for vectorization Collaborative Editing : Real-time multi-user SVG editing","title":"Future Enhancements"},{"location":"image-processing/svg-generation/#appendix-svg-optimization-results","text":"Sample optimization results for different input types: Input Type Original SVG Size Optimized Size Reduction Rendering Performance Improvement Logo 45.2KB 12.8KB 71.7% 22% faster Chart 128.5KB 36.2KB 71.8% 35% faster Map 876.3KB 215.9KB 75.4% 48% faster Chat2SVG Output 520KB 180KB 65.4% 38% faster","title":"Appendix: SVG Optimization Results"},{"location":"image-processing/svg-generation/#related-documentation","text":"Image Processing Overview Effects and Filters Performance Optimization Caching Strategy National Opossum Day","title":"Related Documentation"},{"location":"infrastructure/constraint-mapping/","text":"Chat2SVG Pipeline Constraint Mapping \u00b6 This document provides a comprehensive mapping of constraints in the Chat2SVG pipeline, highlighting boundaries, challenges, and optimization opportunities. This mapping is critical for understanding the system's limitations and informing design decisions for the optimized implementation. 1. Resource Constraints \u00b6 Constraint Description Impact on Chat2SVG Optimization Strategy CPU Processing power for non-GPU tasks \u2022 Limits subprocess management \u2022 Bottleneck for Stage 1 (LLM template generation) \u2022 Controls overall orchestration speed \u2022 Async subprocess management \u2022 Resource-aware scheduling \u2022 Batch processing when possible System RAM Memory for intermediate results and processes \u2022 Limits concurrent requests \u2022 SVG/PNG data storage needs \u2022 Risk of OOM errors during peak loads \u2022 In-memory streaming between stages \u2022 Progressive garbage collection \u2022 Adaptive resource monitoring GPU Compute ML processing power \u2022 Critical bottleneck for Stage 2 (diffusion) \u2022 Impacts Stage 3 VAE optimization speed \u2022 Dictates queue length and wait times \u2022 Model quantization (FP16/INT8) \u2022 Dynamic parameter scaling \u2022 Fine-tuned batch sizing GPU VRAM Memory on GPU \u2022 Primary bottleneck - limits model loading \u2022 Forces model swapping between requests \u2022 May cause OOM errors with large batches \u2022 Aggressive model quantization \u2022 \"Just enough\" parameter settings \u2022 Shared memory access patterns Disk I/O File read/write performance \u2022 Major bottleneck in unoptimized pipeline \u2022 Current design writes 5-12 files per request \u2022 Adds 0.5-2s of overhead per stage \u2022 Replace file I/O with memory streaming \u2022 Unified temp directory structure \u2022 Minimize model loading/unloading Network API connectivity and bandwidth \u2022 Impacts LLM API calls in Stage 1 \u2022 Download time for initial model setup \u2022 Response caching \u2022 Offline operation mode \u2022 Progressive enhancement for slow connections 2. Pipeline Flow Constraints \u00b6 Constraint Description Impact on Chat2SVG Optimization Strategy Stage Dependencies S1 \u2192 S2 \u2192 S3 sequential flow \u2022 Total latency = sum(stage_times) \u2022 Single stage failure breaks entire pipeline \u2022 Debugging complexity increases with each stage \u2022 Unified pipeline controller \u2022 Circuit breaker pattern \u2022 Intelligent fallbacks between stages Data Handover Output format compatibility between stages \u2022 Current implementation uses filesystem \u2022 Directory/file path coordination needed \u2022 Inconsistent naming patterns \u2022 Standardized memory-based data structures \u2022 Clear data validation between stages \u2022 Type checking/conversion where needed Optional Stages Stages 2 & 3 can be skipped \u2022 Quality vs. performance trade-off \u2022 Resource availability influences decisions \u2022 Different failure modes per stage \u2022 Dynamic stage configuration matrix \u2022 Resource-aware stage activation \u2022 Progressive enhancement approach Subprocess Management Python subprocess calls for each stage \u2022 Overhead for process creation \u2022 Error propagation challenges \u2022 Environment variable coordination \u2022 Process pooling where possible \u2022 Improved error handling \u2022 Standardized environment management 3. Performance Constraints \u00b6 Constraint Description Impact on Chat2SVG Optimization Strategy Latency Time from request to response \u2022 5-90s total process time \u2022 Stage 2 (detail) is slowest: 3-30s \u2022 I/O adds ~20-25% overhead \u2022 Memory streaming (40-55% improvement) \u2022 Parallel preparation of next stages \u2022 Early response with progressive enhancement Throughput Requests processed per unit time \u2022 Limited by slowest stage \u2022 GPU becomes shared resource bottleneck \u2022 Concurrent request handling is suboptimal \u2022 Resource-aware scheduling \u2022 Request prioritization framework \u2022 Max-flow optimization for queue management Memory Growth RAM/VRAM usage over time \u2022 Current design shows memory growth patterns \u2022 File handles may leak under load \u2022 VRAM fragmentation during long sessions \u2022 Explicit resource cleanup \u2022 Strategic model unloading \u2022 Memory usage monitoring Cold Start Time for first request processing \u2022 Model loading time: 5-15s \u2022 Pipeline setup overhead: 1-3s \u2022 Subprocess initialization: ~0.5s \u2022 Model preloading for common cases \u2022 Warm standby processes \u2022 Cached initial responses 4. Quality Constraints \u00b6 Constraint Description Impact on Chat2SVG Optimization Strategy Stage Quality Impact Contribution of each stage \u2022 Stage 1 (Template): Basic structure (40%) \u2022 Stage 2 (Detail): Visual fidelity (40%) \u2022 Stage 3 (Optimization): Path quality (20%) \u2022 Stage-specific resource allocation \u2022 Quality impact weighting \u2022 Quality-aware stage skipping Parameter Settings Configuration options \u2022 Diffusion steps (20-30): Major quality factor \u2022 SAM points_per_side (24-64): Detail level \u2022 VAE iterations (500-1000): Path smoothness \u2022 Dynamic parameter selection matrix \u2022 Resource-adaptive settings \u2022 User preference integration Style Consistency Visual coherence \u2022 Stage transitions can create inconsistencies \u2022 Template style may conflict with enhancements \u2022 Parameter tuning affects style fidelity \u2022 Style preservation signals \u2022 Consistent enhancement algorithms \u2022 Style validation between stages SVG Specification W3C SVG compliance \u2022 Generated SVG must be valid/renderable \u2022 Browser compatibility considerations \u2022 Efficient path representation \u2022 Post-processing validation \u2022 Compatibility testing \u2022 Path optimization without quality loss 5. External Constraints \u00b6 Constraint Description Impact on Chat2SVG Optimization Strategy API Limits Rate limits on external services \u2022 LLM API throttling for Stage 1 \u2022 Potential cost considerations \u2022 Response time variation \u2022 Request throttling \u2022 Exponential backoff \u2022 Local model fallbacks Model Availability Required models must be downloaded \u2022 SAM model: ~2.4GB \u2022 Diffusion models: 5-7GB \u2022 VAE models: ~200MB \u2022 Progressive model download \u2022 Quantization to reduce size \u2022 Fallback to simpler models Environment Compatibility OS/hardware requirements \u2022 CUDA dependency for GPU acceleration \u2022 Library version compatibilities \u2022 Path handling differences (Windows/Unix) \u2022 Container-based deployment \u2022 Environment validation \u2022 Cross-platform path handling Deployment Constraints Production deployment limitations \u2022 Docker container size limits \u2022 Cloud resource quotas \u2022 API gateway timeouts (often <30s) \u2022 Model pruning \u2022 Asynchronous processing patterns \u2022 Serverless-compatible architecture Resource Interaction Diagram \u00b6 graph TB subgraph \"Resource Flows\" CPU[CPU] -->|Controls| PROC[Process Management] RAM[System RAM] -->|Stores| DATA[SVG/PNG Data] GPU[GPU] -->|Processes| ML[ML Models] VRAM[GPU VRAM] -->|Limits| MODEL_SIZE[Model Size/Batch] DISK[Disk I/O] -->|Impacts| IO_SPEED[Data Transfer Speed] end subgraph \"Pipeline Stages\" S1[Stage 1: Template] S2[Stage 2: Detail] S3[Stage 3: Optimize] S1 -->|SVG Template| S2 S2 -->|Enhanced SVG + PNG| S3 end subgraph \"Critical Bottlenecks\" VRAM -->|Primary Constraint| S2 GPU -->|Speed Constraint| S2 DISK -->|Overhead| S1 DISK -->|Overhead| S2 DISK -->|Overhead| S3 end Constraint Relationship Matrix \u00b6 Resource Type Stage 1 (Template) Stage 2 (Detail) Stage 3 (Optimize) CPU Critical (4/5) Medium (2/5) High (3/5) RAM Low (1/5) High (4/5) Medium (2/5) GPU Optional (1/5) Critical (5/5) High (4/5) VRAM Optional (1/5) Critical (5/5) High (4/5) Disk I/O Medium (3/5) High (4/5) Medium (3/5) Network High (4/5)* Low (1/5) Low (1/5) *If using external LLM API; low if using local model Sensitivity Analysis \u00b6 The pipeline uses advanced sensitivity analysis based on Google OR-Tools to optimize resource allocation and improve resilience: Analysis Capabilities \u00b6 Variable Impact Analysis : Quantifies the contribution of each pipeline stage to overall performance Resource Bottleneck Detection : Identifies which resources (CPU, memory, GPU, VRAM) are limiting factors Objective Coefficient Ranging : Determines how changes in stage importance affect solution quality What-If Scenario Modeling : Tests hypothetical resource configurations without service disruption Automated Recommendations : Suggests targeted improvements based on operational data Implementation Details \u00b6 The SensitivityAnalyzer class provides: Complete Analysis Workflow : ```python analyzer = SensitivityAnalyzer() model = analyzer.create_optimization_model(requests, resources) status = model.Solve() analysis = analyzer.analyze_model(model) recommendations = analyzer.generate_recommendations(analysis) Fallback Mechanisms : Graceful degradation when advanced analysis features aren't available in the selected solver Telemetry Integration : Performance monitoring for analysis operations This analysis enables data-driven decisions about pipeline configuration and hardware resource allocation. Key Observations & Optimization Priorities \u00b6 Memory Streaming Priority : The current file I/O between stages adds 20-25% overhead. Moving to memory streaming could yield the most immediate performance gains. GPU Resource Management : Stage 2 (detail enhancement) is the heaviest consumer of GPU resources. A dynamic scheduler should prioritize GPU allocation to this stage. Consolidated Resource Allocation : Instead of individual resource checks per stage, implementing a unified resource assessment at the pipeline level allows for optimal allocation across all stages. Progressive Enhancement Strategy : When resources are constrained, implementing quality reduction (fewer diffusion steps, lower SAM resolution) is better than complete stage skipping. Parallel Optimization Opportunities : While the core S1\u2192S2\u2192S3 flow is sequential for a single request, preparation work for later stages can happen in parallel with execution of earlier stages. Quantization Impact : Model quantization provides substantial resource savings with minimal quality impact. Reducing VRAM usage by 50-75% enables higher throughput and concurrent processing. Error Handling Optimization : Current implementation has cascading failures. The circuit breaker pattern with stage-specific fallbacks would significantly improve reliability. Practical Implementation Notes \u00b6 Current implementation creates multiple temp directories that could be consolidated Error handling currently doesn't distinguish between different failure modes Model quantization appears to be configured but not fully implemented in the integration Pipeline treats each request independently, missing opportunities for batch processing Resource detection happens multiple times instead of once with monitoring","title":"Constraint Mapping"},{"location":"infrastructure/constraint-mapping/#chat2svg-pipeline-constraint-mapping","text":"This document provides a comprehensive mapping of constraints in the Chat2SVG pipeline, highlighting boundaries, challenges, and optimization opportunities. This mapping is critical for understanding the system's limitations and informing design decisions for the optimized implementation.","title":"Chat2SVG Pipeline Constraint Mapping"},{"location":"infrastructure/constraint-mapping/#1-resource-constraints","text":"Constraint Description Impact on Chat2SVG Optimization Strategy CPU Processing power for non-GPU tasks \u2022 Limits subprocess management \u2022 Bottleneck for Stage 1 (LLM template generation) \u2022 Controls overall orchestration speed \u2022 Async subprocess management \u2022 Resource-aware scheduling \u2022 Batch processing when possible System RAM Memory for intermediate results and processes \u2022 Limits concurrent requests \u2022 SVG/PNG data storage needs \u2022 Risk of OOM errors during peak loads \u2022 In-memory streaming between stages \u2022 Progressive garbage collection \u2022 Adaptive resource monitoring GPU Compute ML processing power \u2022 Critical bottleneck for Stage 2 (diffusion) \u2022 Impacts Stage 3 VAE optimization speed \u2022 Dictates queue length and wait times \u2022 Model quantization (FP16/INT8) \u2022 Dynamic parameter scaling \u2022 Fine-tuned batch sizing GPU VRAM Memory on GPU \u2022 Primary bottleneck - limits model loading \u2022 Forces model swapping between requests \u2022 May cause OOM errors with large batches \u2022 Aggressive model quantization \u2022 \"Just enough\" parameter settings \u2022 Shared memory access patterns Disk I/O File read/write performance \u2022 Major bottleneck in unoptimized pipeline \u2022 Current design writes 5-12 files per request \u2022 Adds 0.5-2s of overhead per stage \u2022 Replace file I/O with memory streaming \u2022 Unified temp directory structure \u2022 Minimize model loading/unloading Network API connectivity and bandwidth \u2022 Impacts LLM API calls in Stage 1 \u2022 Download time for initial model setup \u2022 Response caching \u2022 Offline operation mode \u2022 Progressive enhancement for slow connections","title":"1. Resource Constraints"},{"location":"infrastructure/constraint-mapping/#2-pipeline-flow-constraints","text":"Constraint Description Impact on Chat2SVG Optimization Strategy Stage Dependencies S1 \u2192 S2 \u2192 S3 sequential flow \u2022 Total latency = sum(stage_times) \u2022 Single stage failure breaks entire pipeline \u2022 Debugging complexity increases with each stage \u2022 Unified pipeline controller \u2022 Circuit breaker pattern \u2022 Intelligent fallbacks between stages Data Handover Output format compatibility between stages \u2022 Current implementation uses filesystem \u2022 Directory/file path coordination needed \u2022 Inconsistent naming patterns \u2022 Standardized memory-based data structures \u2022 Clear data validation between stages \u2022 Type checking/conversion where needed Optional Stages Stages 2 & 3 can be skipped \u2022 Quality vs. performance trade-off \u2022 Resource availability influences decisions \u2022 Different failure modes per stage \u2022 Dynamic stage configuration matrix \u2022 Resource-aware stage activation \u2022 Progressive enhancement approach Subprocess Management Python subprocess calls for each stage \u2022 Overhead for process creation \u2022 Error propagation challenges \u2022 Environment variable coordination \u2022 Process pooling where possible \u2022 Improved error handling \u2022 Standardized environment management","title":"2. Pipeline Flow Constraints"},{"location":"infrastructure/constraint-mapping/#3-performance-constraints","text":"Constraint Description Impact on Chat2SVG Optimization Strategy Latency Time from request to response \u2022 5-90s total process time \u2022 Stage 2 (detail) is slowest: 3-30s \u2022 I/O adds ~20-25% overhead \u2022 Memory streaming (40-55% improvement) \u2022 Parallel preparation of next stages \u2022 Early response with progressive enhancement Throughput Requests processed per unit time \u2022 Limited by slowest stage \u2022 GPU becomes shared resource bottleneck \u2022 Concurrent request handling is suboptimal \u2022 Resource-aware scheduling \u2022 Request prioritization framework \u2022 Max-flow optimization for queue management Memory Growth RAM/VRAM usage over time \u2022 Current design shows memory growth patterns \u2022 File handles may leak under load \u2022 VRAM fragmentation during long sessions \u2022 Explicit resource cleanup \u2022 Strategic model unloading \u2022 Memory usage monitoring Cold Start Time for first request processing \u2022 Model loading time: 5-15s \u2022 Pipeline setup overhead: 1-3s \u2022 Subprocess initialization: ~0.5s \u2022 Model preloading for common cases \u2022 Warm standby processes \u2022 Cached initial responses","title":"3. Performance Constraints"},{"location":"infrastructure/constraint-mapping/#4-quality-constraints","text":"Constraint Description Impact on Chat2SVG Optimization Strategy Stage Quality Impact Contribution of each stage \u2022 Stage 1 (Template): Basic structure (40%) \u2022 Stage 2 (Detail): Visual fidelity (40%) \u2022 Stage 3 (Optimization): Path quality (20%) \u2022 Stage-specific resource allocation \u2022 Quality impact weighting \u2022 Quality-aware stage skipping Parameter Settings Configuration options \u2022 Diffusion steps (20-30): Major quality factor \u2022 SAM points_per_side (24-64): Detail level \u2022 VAE iterations (500-1000): Path smoothness \u2022 Dynamic parameter selection matrix \u2022 Resource-adaptive settings \u2022 User preference integration Style Consistency Visual coherence \u2022 Stage transitions can create inconsistencies \u2022 Template style may conflict with enhancements \u2022 Parameter tuning affects style fidelity \u2022 Style preservation signals \u2022 Consistent enhancement algorithms \u2022 Style validation between stages SVG Specification W3C SVG compliance \u2022 Generated SVG must be valid/renderable \u2022 Browser compatibility considerations \u2022 Efficient path representation \u2022 Post-processing validation \u2022 Compatibility testing \u2022 Path optimization without quality loss","title":"4. Quality Constraints"},{"location":"infrastructure/constraint-mapping/#5-external-constraints","text":"Constraint Description Impact on Chat2SVG Optimization Strategy API Limits Rate limits on external services \u2022 LLM API throttling for Stage 1 \u2022 Potential cost considerations \u2022 Response time variation \u2022 Request throttling \u2022 Exponential backoff \u2022 Local model fallbacks Model Availability Required models must be downloaded \u2022 SAM model: ~2.4GB \u2022 Diffusion models: 5-7GB \u2022 VAE models: ~200MB \u2022 Progressive model download \u2022 Quantization to reduce size \u2022 Fallback to simpler models Environment Compatibility OS/hardware requirements \u2022 CUDA dependency for GPU acceleration \u2022 Library version compatibilities \u2022 Path handling differences (Windows/Unix) \u2022 Container-based deployment \u2022 Environment validation \u2022 Cross-platform path handling Deployment Constraints Production deployment limitations \u2022 Docker container size limits \u2022 Cloud resource quotas \u2022 API gateway timeouts (often <30s) \u2022 Model pruning \u2022 Asynchronous processing patterns \u2022 Serverless-compatible architecture","title":"5. External Constraints"},{"location":"infrastructure/constraint-mapping/#resource-interaction-diagram","text":"graph TB subgraph \"Resource Flows\" CPU[CPU] -->|Controls| PROC[Process Management] RAM[System RAM] -->|Stores| DATA[SVG/PNG Data] GPU[GPU] -->|Processes| ML[ML Models] VRAM[GPU VRAM] -->|Limits| MODEL_SIZE[Model Size/Batch] DISK[Disk I/O] -->|Impacts| IO_SPEED[Data Transfer Speed] end subgraph \"Pipeline Stages\" S1[Stage 1: Template] S2[Stage 2: Detail] S3[Stage 3: Optimize] S1 -->|SVG Template| S2 S2 -->|Enhanced SVG + PNG| S3 end subgraph \"Critical Bottlenecks\" VRAM -->|Primary Constraint| S2 GPU -->|Speed Constraint| S2 DISK -->|Overhead| S1 DISK -->|Overhead| S2 DISK -->|Overhead| S3 end","title":"Resource Interaction Diagram"},{"location":"infrastructure/constraint-mapping/#constraint-relationship-matrix","text":"Resource Type Stage 1 (Template) Stage 2 (Detail) Stage 3 (Optimize) CPU Critical (4/5) Medium (2/5) High (3/5) RAM Low (1/5) High (4/5) Medium (2/5) GPU Optional (1/5) Critical (5/5) High (4/5) VRAM Optional (1/5) Critical (5/5) High (4/5) Disk I/O Medium (3/5) High (4/5) Medium (3/5) Network High (4/5)* Low (1/5) Low (1/5) *If using external LLM API; low if using local model","title":"Constraint Relationship Matrix"},{"location":"infrastructure/constraint-mapping/#sensitivity-analysis","text":"The pipeline uses advanced sensitivity analysis based on Google OR-Tools to optimize resource allocation and improve resilience:","title":"Sensitivity Analysis"},{"location":"infrastructure/constraint-mapping/#analysis-capabilities","text":"Variable Impact Analysis : Quantifies the contribution of each pipeline stage to overall performance Resource Bottleneck Detection : Identifies which resources (CPU, memory, GPU, VRAM) are limiting factors Objective Coefficient Ranging : Determines how changes in stage importance affect solution quality What-If Scenario Modeling : Tests hypothetical resource configurations without service disruption Automated Recommendations : Suggests targeted improvements based on operational data","title":"Analysis Capabilities"},{"location":"infrastructure/constraint-mapping/#implementation-details","text":"The SensitivityAnalyzer class provides: Complete Analysis Workflow : ```python analyzer = SensitivityAnalyzer() model = analyzer.create_optimization_model(requests, resources) status = model.Solve() analysis = analyzer.analyze_model(model) recommendations = analyzer.generate_recommendations(analysis) Fallback Mechanisms : Graceful degradation when advanced analysis features aren't available in the selected solver Telemetry Integration : Performance monitoring for analysis operations This analysis enables data-driven decisions about pipeline configuration and hardware resource allocation.","title":"Implementation Details"},{"location":"infrastructure/constraint-mapping/#key-observations-optimization-priorities","text":"Memory Streaming Priority : The current file I/O between stages adds 20-25% overhead. Moving to memory streaming could yield the most immediate performance gains. GPU Resource Management : Stage 2 (detail enhancement) is the heaviest consumer of GPU resources. A dynamic scheduler should prioritize GPU allocation to this stage. Consolidated Resource Allocation : Instead of individual resource checks per stage, implementing a unified resource assessment at the pipeline level allows for optimal allocation across all stages. Progressive Enhancement Strategy : When resources are constrained, implementing quality reduction (fewer diffusion steps, lower SAM resolution) is better than complete stage skipping. Parallel Optimization Opportunities : While the core S1\u2192S2\u2192S3 flow is sequential for a single request, preparation work for later stages can happen in parallel with execution of earlier stages. Quantization Impact : Model quantization provides substantial resource savings with minimal quality impact. Reducing VRAM usage by 50-75% enables higher throughput and concurrent processing. Error Handling Optimization : Current implementation has cascading failures. The circuit breaker pattern with stage-specific fallbacks would significantly improve reliability.","title":"Key Observations &amp; Optimization Priorities"},{"location":"infrastructure/constraint-mapping/#practical-implementation-notes","text":"Current implementation creates multiple temp directories that could be consolidated Error handling currently doesn't distinguish between different failure modes Model quantization appears to be configured but not fully implemented in the integration Pipeline treats each request independently, missing opportunities for batch processing Resource detection happens multiple times instead of once with monitoring","title":"Practical Implementation Notes"},{"location":"infrastructure/optimization-strategies/","text":"Optimization Strategies \u00b6 This document outlines key optimization opportunities for the Opossum Search platform across various components. These strategies enhance performance, reliability, and resource efficiency without compromising the system's hybrid architecture. Model Selection Optimization \u00b6 Weighted Selection Caching \u00b6 # Cache model selection decisions in Redis async def get_optimal_backend ( self , query_type , has_image = False ): # Generate cache key from request parameters cache_key = f \"model_selection: { query_type } : { has_image } \" # Check cache first cached_result = await self . redis_client . get ( cache_key ) if cached_result : return cached_result . decode ( 'utf-8' ) # Calculate optimal backend when not cached selected_backend = await self . _calculate_backend_scores ( query_type , has_image ) # Cache result for future requests await self . redis_client . setex ( cache_key , Config . MODEL_SELECTION_CACHE_TTL , selected_backend ) return selected_backend Fast Path Selection \u00b6 # Implement fast paths for common scenarios def get_fast_path ( self , query_type , has_image = False ): \"\"\"Fast selection paths to avoid full scoring calculation\"\"\" # Multimodal requests prioritize Gemini when available if has_image and self . services . is_gemini_available : return \"gemini-thinking\" # Simple greetings use lightweight models if query_type == \"greeting\" and self . services . is_ollama_available : return \"gemma\" # Complex reasoning favors Gemini if query_type == \"reasoning\" and self . services . is_gemini_available : return \"gemini-thinking\" # Fall through to standard weighted selection return None Redis Caching Strategies \u00b6 Multi-Level Response Caching \u00b6 async def get_cached_response ( self , query , context ): \"\"\"Implement multi-level caching with context awareness\"\"\" # Try exact match (highest priority, short TTL) exact_key = f \"response:exact: { hashlib . md5 ( query . encode ()) . hexdigest () } \" exact_match = await self . redis . get ( exact_key ) if exact_match : return json . loads ( exact_match ) # For non-critical contexts, try semantic matching (longer TTL) if context != \"critical_response\" : embedding = await self . get_embedding ( query ) similar_queries = await self . find_similar_queries ( embedding ) for similar_key in similar_queries : cached = await self . redis . get ( f \"response:exact: { similar_key } \" ) if cached : return json . loads ( cached ) # Cache miss - will need to generate new response return None Availability Cache Refresh \u00b6 class ServiceMonitor : async def start_refresh_task ( self ): \"\"\"Start background refresh of service availability\"\"\" async def _refresh_loop (): while True : try : # Check all services await self . check_all_services () # Update Redis cache with latest status for service , status in self . services . items (): await self . redis . setex ( f \"service:status: { service } \" , Config . AVAILABILITY_CACHE_TTL , \"1\" if status else \"0\" ) # Wait before next check await asyncio . sleep ( Config . AVAILABILITY_CHECK_INTERVAL ) except Exception as e : logger . error ( f \"Error in availability refresh: { e } \" ) await asyncio . sleep ( 5 ) # Shorter retry on error self . refresh_task = asyncio . create_task ( _refresh_loop ()) return self . refresh_task Parallel Processing \u00b6 Concurrent Service Checks \u00b6 async def check_all_services ( self ): \"\"\"Check all services concurrently\"\"\" tasks = [ self . check_service ( \"gemini\" ), self . check_service ( \"ollama\" ), self . check_service ( \"transformers\" ) ] # Use gather to run all checks in parallel results = await asyncio . gather ( * tasks , return_exceptions = True ) # Process results, handling any exceptions status_updates = {} for i , result in enumerate ( results ): service_name = [ \"gemini\" , \"ollama\" , \"transformers\" ][ i ] if isinstance ( result , Exception ): logger . error ( f \"Error checking { service_name } : { result } \" ) status_updates [ service_name ] = False else : status_updates [ service_name ] = result # Update global service status self . update_service_status ( status_updates ) Parallel Image Processing \u00b6 async def process_image ( self , image_data , target_size = 512 ): \"\"\"Process image with parallel operations\"\"\" async def resize_image (): with wand . image . Image ( blob = image_data ) as img : img . resize ( target_size , target_size ) return img . make_blob () async def extract_metadata (): with wand . image . Image ( blob = image_data ) as img : return { \"format\" : img . format , \"width\" : img . width , \"height\" : img . height , \"colorspace\" : str ( img . colorspace ), \"depth\" : img . depth } async def create_thumbnail (): with wand . image . Image ( blob = image_data ) as img : img . resize ( 128 , 128 ) return img . make_blob ( format = 'webp' ) # Run all processing tasks concurrently resized , metadata , thumbnail = await asyncio . gather ( resize_image (), extract_metadata (), create_thumbnail () ) return { \"processed\" : resized , \"metadata\" : metadata , \"thumbnail\" : thumbnail } Memory Optimization \u00b6 Efficient Transformers Loading \u00b6 def initialize_transformers_model (): \"\"\"Load transformers model with optimal memory settings\"\"\" model_name = Config . MODEL_CONFIGS [ \"gemma\" ][ \"transformers_name\" ] # Configure quantization based on available hardware if torch . cuda . is_available (): # Use 8-bit quantization on GPU return AutoModelForCausalLM . from_pretrained ( model_name , device_map = \"auto\" , load_in_8bit = True , torch_dtype = torch . float16 ) else : # CPU optimization return AutoModelForCausalLM . from_pretrained ( model_name , device_map = { \"\" : \"cpu\" }, low_cpu_mem_usage = True ) Connection Pooling \u00b6 class HttpClientPool : \"\"\"Manage persistent HTTP connections for external services\"\"\" def __init__ ( self ): # Shared connection pools by service self . clients = { \"ollama\" : httpx . AsyncClient ( base_url = Config . OLLAMA_BASE_URL , timeout = 30.0 , limits = httpx . Limits ( max_connections = 20 ), http2 = True ), \"telemetry\" : httpx . AsyncClient ( base_url = Config . OTEL_EXPORTER_OTLP_ENDPOINT , timeout = 5.0 , limits = httpx . Limits ( max_connections = 5 ) ) } def get_client ( self , service_name ): \"\"\"Get client for specific service\"\"\" return self . clients . get ( service_name ) async def close_all ( self ): \"\"\"Close all connections when shutting down\"\"\" for client in self . clients . values (): await client . aclose () API Optimization \u00b6 Rate Limiting & Queueing \u00b6 class GeminiRequestQueue : \"\"\"Queue for rate-limited Gemini API requests\"\"\" def __init__ ( self ): self . queue = asyncio . Queue () self . worker_task = None self . daily_tokens = 0 self . daily_requests = 0 self . tokens_lock = asyncio . Lock () async def start_worker ( self ): \"\"\"Process queued requests at controlled rate\"\"\" async def _worker (): while True : # Get next request from queue request_data , future = await self . queue . get () try : # Process with rate limiting result = await self . _process_request ( request_data ) future . set_result ( result ) except Exception as e : future . set_exception ( e ) finally : # Rate limit - ensure minimum time between requests await asyncio . sleep ( 60 / Config . GEMINI_RPM_LIMIT ) self . queue . task_done () self . worker_task = asyncio . create_task ( _worker ()) async def enqueue_request ( self , request_data ): \"\"\"Add request to processing queue\"\"\" # Check daily limits async with self . tokens_lock : if self . daily_requests >= Config . GEMINI_DAILY_LIMIT : raise Exception ( \"Daily request limit exceeded\" ) # Create future for async result future = asyncio . Future () await self . queue . put (( request_data , future )) return await future Smart Retries \u00b6 async def request_with_retry ( self , func , * args , max_retries = 3 , ** kwargs ): \"\"\"Generic retry wrapper with exponential backoff\"\"\" retry = 0 last_exception = None while retry <= max_retries : try : return await func ( * args , ** kwargs ) except Exception as e : last_exception = e retry += 1 # Don't retry certain errors if isinstance ( e , ( AuthenticationError , RateLimitExceeded )): raise # Calculate exponential backoff with jitter backoff = min ( 30 , 0.5 * ( 2 ** ( retry - 1 )) * ( 1 + random . random ())) logger . warning ( f \"Attempt { retry } / { max_retries } failed: { e } . Retrying in { backoff : .2f } s\" ) await asyncio . sleep ( backoff ) # All retries failed logger . error ( f \"All { max_retries } retry attempts failed\" ) raise last_exception Monitoring & Diagnostics \u00b6 Performance Tracing \u00b6 async def process_query ( self , query , context = None ): \"\"\"Process user query with performance tracking\"\"\" with tracer . start_as_current_span ( \"process_query\" ) as span : # Add context to span span . set_attribute ( \"query.length\" , len ( query )) span . set_attribute ( \"query.context\" , context or \"none\" ) start_time = time . time () # Get optimal backend backend_start = time . time () backend = await self . get_optimal_backend ( query , context ) backend_time = time . time () - backend_start span . set_attribute ( \"backend.selection.time_ms\" , backend_time * 1000 ) span . set_attribute ( \"backend.selected\" , backend ) # Generate response generate_start = time . time () response = await self . generate_response ( query , backend ) generate_time = time . time () - generate_start span . set_attribute ( \"response.generation.time_ms\" , generate_time * 1000 ) # Record total processing time total_time = time . time () - start_time span . set_attribute ( \"query.total_time_ms\" , total_time * 1000 ) return response Adaptive Quality Control \u00b6 async def monitor_response_quality ( self , query , response , backend ): \"\"\"Track response quality metrics for adaptive improvement\"\"\" # Calculate basic metrics response_length = len ( response ) query_length = len ( query ) ratio = response_length / max ( 1 , query_length ) # Calculate response time response_time = self . response_times . get ( backend , []) if response_time : avg_time = sum ( response_time ) / len ( response_time ) else : avg_time = 0 # Update quality metrics in Redis await self . redis . hset ( f \"quality:backend: { backend } \" , mapping = { \"avg_response_ratio\" : ( self . quality_metrics . get ( \"ratio\" , 0 ) + ratio ) / 2 , \"avg_response_time\" : avg_time , \"total_requests\" : self . quality_metrics . get ( \"requests\" , 0 ) + 1 } ) # If quality falling below threshold, adjust selection weights if ratio < 0.5 and avg_time > 2.0 : # Reduce preference for this backend await self . adjust_backend_weights ( backend , factor = 0.8 ) Configuration Management \u00b6 Environment-Based Settings \u00b6 # Example production settings for .env file FLASK_ENV = production API_KEY_REQUIRED = true GRAPHQL_GRAPHIQL = false VOYAGER_ENABLED = false GEMINI_DAILY_LIMIT = 1000 REDIS_PASSWORD = secure_password_here CACHE_TTL = 3600 CORS_ALLOWED_ORIGINS = https : // app . example . com , https : // studio . apollographql . com By implementing these optimizations, you'll significantly enhance your system's performance and reliability while maintaining its flexible architecture.","title":"Optimization Strategies"},{"location":"infrastructure/optimization-strategies/#optimization-strategies","text":"This document outlines key optimization opportunities for the Opossum Search platform across various components. These strategies enhance performance, reliability, and resource efficiency without compromising the system's hybrid architecture.","title":"Optimization Strategies"},{"location":"infrastructure/optimization-strategies/#model-selection-optimization","text":"","title":"Model Selection Optimization"},{"location":"infrastructure/optimization-strategies/#weighted-selection-caching","text":"# Cache model selection decisions in Redis async def get_optimal_backend ( self , query_type , has_image = False ): # Generate cache key from request parameters cache_key = f \"model_selection: { query_type } : { has_image } \" # Check cache first cached_result = await self . redis_client . get ( cache_key ) if cached_result : return cached_result . decode ( 'utf-8' ) # Calculate optimal backend when not cached selected_backend = await self . _calculate_backend_scores ( query_type , has_image ) # Cache result for future requests await self . redis_client . setex ( cache_key , Config . MODEL_SELECTION_CACHE_TTL , selected_backend ) return selected_backend","title":"Weighted Selection Caching"},{"location":"infrastructure/optimization-strategies/#fast-path-selection","text":"# Implement fast paths for common scenarios def get_fast_path ( self , query_type , has_image = False ): \"\"\"Fast selection paths to avoid full scoring calculation\"\"\" # Multimodal requests prioritize Gemini when available if has_image and self . services . is_gemini_available : return \"gemini-thinking\" # Simple greetings use lightweight models if query_type == \"greeting\" and self . services . is_ollama_available : return \"gemma\" # Complex reasoning favors Gemini if query_type == \"reasoning\" and self . services . is_gemini_available : return \"gemini-thinking\" # Fall through to standard weighted selection return None","title":"Fast Path Selection"},{"location":"infrastructure/optimization-strategies/#redis-caching-strategies","text":"","title":"Redis Caching Strategies"},{"location":"infrastructure/optimization-strategies/#multi-level-response-caching","text":"async def get_cached_response ( self , query , context ): \"\"\"Implement multi-level caching with context awareness\"\"\" # Try exact match (highest priority, short TTL) exact_key = f \"response:exact: { hashlib . md5 ( query . encode ()) . hexdigest () } \" exact_match = await self . redis . get ( exact_key ) if exact_match : return json . loads ( exact_match ) # For non-critical contexts, try semantic matching (longer TTL) if context != \"critical_response\" : embedding = await self . get_embedding ( query ) similar_queries = await self . find_similar_queries ( embedding ) for similar_key in similar_queries : cached = await self . redis . get ( f \"response:exact: { similar_key } \" ) if cached : return json . loads ( cached ) # Cache miss - will need to generate new response return None","title":"Multi-Level Response Caching"},{"location":"infrastructure/optimization-strategies/#availability-cache-refresh","text":"class ServiceMonitor : async def start_refresh_task ( self ): \"\"\"Start background refresh of service availability\"\"\" async def _refresh_loop (): while True : try : # Check all services await self . check_all_services () # Update Redis cache with latest status for service , status in self . services . items (): await self . redis . setex ( f \"service:status: { service } \" , Config . AVAILABILITY_CACHE_TTL , \"1\" if status else \"0\" ) # Wait before next check await asyncio . sleep ( Config . AVAILABILITY_CHECK_INTERVAL ) except Exception as e : logger . error ( f \"Error in availability refresh: { e } \" ) await asyncio . sleep ( 5 ) # Shorter retry on error self . refresh_task = asyncio . create_task ( _refresh_loop ()) return self . refresh_task","title":"Availability Cache Refresh"},{"location":"infrastructure/optimization-strategies/#parallel-processing","text":"","title":"Parallel Processing"},{"location":"infrastructure/optimization-strategies/#concurrent-service-checks","text":"async def check_all_services ( self ): \"\"\"Check all services concurrently\"\"\" tasks = [ self . check_service ( \"gemini\" ), self . check_service ( \"ollama\" ), self . check_service ( \"transformers\" ) ] # Use gather to run all checks in parallel results = await asyncio . gather ( * tasks , return_exceptions = True ) # Process results, handling any exceptions status_updates = {} for i , result in enumerate ( results ): service_name = [ \"gemini\" , \"ollama\" , \"transformers\" ][ i ] if isinstance ( result , Exception ): logger . error ( f \"Error checking { service_name } : { result } \" ) status_updates [ service_name ] = False else : status_updates [ service_name ] = result # Update global service status self . update_service_status ( status_updates )","title":"Concurrent Service Checks"},{"location":"infrastructure/optimization-strategies/#parallel-image-processing","text":"async def process_image ( self , image_data , target_size = 512 ): \"\"\"Process image with parallel operations\"\"\" async def resize_image (): with wand . image . Image ( blob = image_data ) as img : img . resize ( target_size , target_size ) return img . make_blob () async def extract_metadata (): with wand . image . Image ( blob = image_data ) as img : return { \"format\" : img . format , \"width\" : img . width , \"height\" : img . height , \"colorspace\" : str ( img . colorspace ), \"depth\" : img . depth } async def create_thumbnail (): with wand . image . Image ( blob = image_data ) as img : img . resize ( 128 , 128 ) return img . make_blob ( format = 'webp' ) # Run all processing tasks concurrently resized , metadata , thumbnail = await asyncio . gather ( resize_image (), extract_metadata (), create_thumbnail () ) return { \"processed\" : resized , \"metadata\" : metadata , \"thumbnail\" : thumbnail }","title":"Parallel Image Processing"},{"location":"infrastructure/optimization-strategies/#memory-optimization","text":"","title":"Memory Optimization"},{"location":"infrastructure/optimization-strategies/#efficient-transformers-loading","text":"def initialize_transformers_model (): \"\"\"Load transformers model with optimal memory settings\"\"\" model_name = Config . MODEL_CONFIGS [ \"gemma\" ][ \"transformers_name\" ] # Configure quantization based on available hardware if torch . cuda . is_available (): # Use 8-bit quantization on GPU return AutoModelForCausalLM . from_pretrained ( model_name , device_map = \"auto\" , load_in_8bit = True , torch_dtype = torch . float16 ) else : # CPU optimization return AutoModelForCausalLM . from_pretrained ( model_name , device_map = { \"\" : \"cpu\" }, low_cpu_mem_usage = True )","title":"Efficient Transformers Loading"},{"location":"infrastructure/optimization-strategies/#connection-pooling","text":"class HttpClientPool : \"\"\"Manage persistent HTTP connections for external services\"\"\" def __init__ ( self ): # Shared connection pools by service self . clients = { \"ollama\" : httpx . AsyncClient ( base_url = Config . OLLAMA_BASE_URL , timeout = 30.0 , limits = httpx . Limits ( max_connections = 20 ), http2 = True ), \"telemetry\" : httpx . AsyncClient ( base_url = Config . OTEL_EXPORTER_OTLP_ENDPOINT , timeout = 5.0 , limits = httpx . Limits ( max_connections = 5 ) ) } def get_client ( self , service_name ): \"\"\"Get client for specific service\"\"\" return self . clients . get ( service_name ) async def close_all ( self ): \"\"\"Close all connections when shutting down\"\"\" for client in self . clients . values (): await client . aclose ()","title":"Connection Pooling"},{"location":"infrastructure/optimization-strategies/#api-optimization","text":"","title":"API Optimization"},{"location":"infrastructure/optimization-strategies/#rate-limiting-queueing","text":"class GeminiRequestQueue : \"\"\"Queue for rate-limited Gemini API requests\"\"\" def __init__ ( self ): self . queue = asyncio . Queue () self . worker_task = None self . daily_tokens = 0 self . daily_requests = 0 self . tokens_lock = asyncio . Lock () async def start_worker ( self ): \"\"\"Process queued requests at controlled rate\"\"\" async def _worker (): while True : # Get next request from queue request_data , future = await self . queue . get () try : # Process with rate limiting result = await self . _process_request ( request_data ) future . set_result ( result ) except Exception as e : future . set_exception ( e ) finally : # Rate limit - ensure minimum time between requests await asyncio . sleep ( 60 / Config . GEMINI_RPM_LIMIT ) self . queue . task_done () self . worker_task = asyncio . create_task ( _worker ()) async def enqueue_request ( self , request_data ): \"\"\"Add request to processing queue\"\"\" # Check daily limits async with self . tokens_lock : if self . daily_requests >= Config . GEMINI_DAILY_LIMIT : raise Exception ( \"Daily request limit exceeded\" ) # Create future for async result future = asyncio . Future () await self . queue . put (( request_data , future )) return await future","title":"Rate Limiting &amp; Queueing"},{"location":"infrastructure/optimization-strategies/#smart-retries","text":"async def request_with_retry ( self , func , * args , max_retries = 3 , ** kwargs ): \"\"\"Generic retry wrapper with exponential backoff\"\"\" retry = 0 last_exception = None while retry <= max_retries : try : return await func ( * args , ** kwargs ) except Exception as e : last_exception = e retry += 1 # Don't retry certain errors if isinstance ( e , ( AuthenticationError , RateLimitExceeded )): raise # Calculate exponential backoff with jitter backoff = min ( 30 , 0.5 * ( 2 ** ( retry - 1 )) * ( 1 + random . random ())) logger . warning ( f \"Attempt { retry } / { max_retries } failed: { e } . Retrying in { backoff : .2f } s\" ) await asyncio . sleep ( backoff ) # All retries failed logger . error ( f \"All { max_retries } retry attempts failed\" ) raise last_exception","title":"Smart Retries"},{"location":"infrastructure/optimization-strategies/#monitoring-diagnostics","text":"","title":"Monitoring &amp; Diagnostics"},{"location":"infrastructure/optimization-strategies/#performance-tracing","text":"async def process_query ( self , query , context = None ): \"\"\"Process user query with performance tracking\"\"\" with tracer . start_as_current_span ( \"process_query\" ) as span : # Add context to span span . set_attribute ( \"query.length\" , len ( query )) span . set_attribute ( \"query.context\" , context or \"none\" ) start_time = time . time () # Get optimal backend backend_start = time . time () backend = await self . get_optimal_backend ( query , context ) backend_time = time . time () - backend_start span . set_attribute ( \"backend.selection.time_ms\" , backend_time * 1000 ) span . set_attribute ( \"backend.selected\" , backend ) # Generate response generate_start = time . time () response = await self . generate_response ( query , backend ) generate_time = time . time () - generate_start span . set_attribute ( \"response.generation.time_ms\" , generate_time * 1000 ) # Record total processing time total_time = time . time () - start_time span . set_attribute ( \"query.total_time_ms\" , total_time * 1000 ) return response","title":"Performance Tracing"},{"location":"infrastructure/optimization-strategies/#adaptive-quality-control","text":"async def monitor_response_quality ( self , query , response , backend ): \"\"\"Track response quality metrics for adaptive improvement\"\"\" # Calculate basic metrics response_length = len ( response ) query_length = len ( query ) ratio = response_length / max ( 1 , query_length ) # Calculate response time response_time = self . response_times . get ( backend , []) if response_time : avg_time = sum ( response_time ) / len ( response_time ) else : avg_time = 0 # Update quality metrics in Redis await self . redis . hset ( f \"quality:backend: { backend } \" , mapping = { \"avg_response_ratio\" : ( self . quality_metrics . get ( \"ratio\" , 0 ) + ratio ) / 2 , \"avg_response_time\" : avg_time , \"total_requests\" : self . quality_metrics . get ( \"requests\" , 0 ) + 1 } ) # If quality falling below threshold, adjust selection weights if ratio < 0.5 and avg_time > 2.0 : # Reduce preference for this backend await self . adjust_backend_weights ( backend , factor = 0.8 )","title":"Adaptive Quality Control"},{"location":"infrastructure/optimization-strategies/#configuration-management","text":"","title":"Configuration Management"},{"location":"infrastructure/optimization-strategies/#environment-based-settings","text":"# Example production settings for .env file FLASK_ENV = production API_KEY_REQUIRED = true GRAPHQL_GRAPHIQL = false VOYAGER_ENABLED = false GEMINI_DAILY_LIMIT = 1000 REDIS_PASSWORD = secure_password_here CACHE_TTL = 3600 CORS_ALLOWED_ORIGINS = https : // app . example . com , https : // studio . apollographql . com By implementing these optimizations, you'll significantly enhance your system's performance and reliability while maintaining its flexible architecture.","title":"Environment-Based Settings"},{"location":"infrastructure/pipeline-optimization/","text":"Chat2SVG Pipeline Optimization Documentation \u00b6 Circuit Theory & Operations Research Approach \u00b6 Overview \u00b6 This document outlines the optimized Chat2SVG pipeline architecture using principles from circuit theory and operations research to maximize performance, resource utilization, and reliability. Pipeline Architecture \u00b6 The optimized Chat2SVG pipeline integrates three stages (Template Generation, Detail Enhancement, and SVG Optimization) into a unified pipeline with shared memory, dynamic resource allocation, and adaptive processing. graph TB A[User Request] --> B[SVGPipelineController] B --> C[Resource Assessment] C --> D[Pipeline Configuration] D --> E{Circuit Breaker Gate} E -->|\"Closed\"| F[Memory Stream Pipeline] E -->|\"Open\"| G[Fallback Path] F --> H[Stage 1: Template] H -->|\"Memory Stream\"| I[Stage 2: Detail Enhancement] I -->|\"Memory Stream\"| J[Stage 3: Optimization] J --> K[Output Processing] G --> K M[Resource Monitor] -->|\"Feedback Loop\"| D M -->|\"Real-time Updates\"| I M -->|\"Parameter Tuning\"| J Core Technologies \u00b6 Memory Streaming \u00b6 Replaces disk I/O operations between stages with in-memory data transfer, significantly reducing latency: class SVGPipelineState : \"\"\"Holds the intermediate data passed between stages in memory.\"\"\" def __init__ ( self , prompt : str , style : Optional [ str ] = None ): # Stage outputs stored in memory (not as paths) self . template_svg : Optional [ str ] = None self . enhanced_svg : Optional [ str ] = None self . target_png_bytes : Optional [ bytes ] = None # As bytes, not file path self . optimized_svg : Optional [ str ] = None Circuit Theory Application \u00b6 Implements circuit breaker patterns to prevent cascading failures: class CircuitBreaker : \"\"\"Circuit breaker for managing pipeline availability.\"\"\" def __init__ ( self ): self . state = CircuitState . CLOSED self . failure_count = 0 self . last_failure_time = 0 self . reset_timeout = 60 # seconds self . failure_threshold = 3 Operations Research Solution \u00b6 Formalizes the resource allocation problem using mathematical optimization: class SVGPipelineOptimizer : \"\"\"Optimization engine based on operations research principles.\"\"\" def __init__ ( self ): # Quality contribution matrices for stages self . quality_matrix = { \"template\" : 0.6 , # 60% quality contribution \"detail\" : 0.3 , # 30% quality contribution \"optimize\" : 0.1 # 10% quality contribution } # Resource requirement matrices self . resource_requirements = { \"template\" : { \"cpu\" : 0.2 , \"memory\" : 0.2 , \"gpu\" : 0.0 }, \"detail\" : { \"cpu\" : 0.6 , \"memory\" : 0.8 , \"gpu\" : 0.9 }, \"optimize\" : { \"cpu\" : 0.4 , \"memory\" : 0.3 , \"gpu\" : 0.2 } } Dynamic Resource Adaptation \u00b6 Continuously monitors system resources and adapts pipeline parameters in real-time: async def _detect_available_resources ( self ) -> Dict [ str , float ]: \"\"\"Enhanced resource detection including GPU, RAM, and Swap.\"\"\" resources = { \"cpu\" : 10.0 , \"memory\" : 10.0 , \"swap\" : 0.0 } # Collect CPU, RAM, and swap information cpu_usage = await _to_thread ( psutil . cpu_percent , interval = 0.1 ) memory = await _to_thread ( psutil . virtual_memory ) swap = await _to_thread ( psutil . swap_memory ) resources [ \"cpu\" ] = 100.0 - cpu_usage resources [ \"memory\" ] = memory . available / memory . total * 100.0 resources [ \"swap\" ] = swap . percent # Check for GPU availability if torch is not None and torch . cuda . is_available (): # Get GPU utilization and memory with torch . cuda . device ( 0 ): total_vram = torch . cuda . get_device_properties ( 0 ) . total_memory / ( 1024 ** 3 ) used_vram = torch . cuda . memory_allocated () / ( 1024 ** 3 ) resources [ \"gpu\" ] = 100.0 resources [ \"vram\" ] = (( total_vram - used_vram ) / total_vram ) * 100.0 return resources Pipeline Controller Implementation \u00b6 class SVGPipelineController : \"\"\"Coordinates the entire SVG generation pipeline with circuit breaker.\"\"\" def __init__ ( self ): self . optimizer = SVGPipelineOptimizer () self . resource_monitor = ResourceMonitor () self . circuit_breaker = CircuitBreaker () self . shared_state = SVGPipelineState () async def initialize ( self , prompt : str , style : Optional [ str ] = None ): \"\"\"Initialize the controller with unified resource assessment.\"\"\" self . shared_state . prompt = prompt self . shared_state . style = style # Create unified temporary directory structure self . pipeline_dir = tempfile . mkdtemp ( prefix = f \"chat2svg_ { _sanitize_filename ( prompt ) } _\" ) # Assess resources once for the entire pipeline resources = await self . _detect_available_resources () self . shared_state . resource_level = self . _determine_resource_level ( resources ) # Configure all stages in one pass self . _configure_pipeline ( resources ) # Determine stages to run based on resources and configuration self . shared_state . stages_to_run = self . _determine_active_stages () return self async def execute ( self ) -> Dict [ str , Any ]: \"\"\"Execute the pipeline with optimized memory streaming.\"\"\" start_time = time . time () # Check circuit breaker if not self . circuit_breaker . allow_request (): logger . warning ( \"Circuit breaker is open, using fallback\" ) return await self . _generate_fallback ( self . shared_state . prompt ) try : # Execute the optimized pipeline if \"template\" in self . shared_state . stages_to_run : if not await self . _execute_stage_1 (): raise PipelineError ( \"Template generation failed\" ) if \"detail\" in self . shared_state . stages_to_run : if not await self . _execute_stage_2 (): raise PipelineError ( \"Detail enhancement failed\" ) if \"optimize\" in self . shared_state . stages_to_run : if not await self . _execute_stage_3 (): raise PipelineError ( \"SVG optimization failed\" ) # Record success for circuit breaker self . circuit_breaker . record_success () # Return the final result svg_content = self . shared_state . optimized_svg or self . shared_state . enhanced_svg or self . shared_state . template_svg base64_image = _encode_svg_to_png_base64 ( svg_content ) return { \"svg_content\" : svg_content , \"base64_image\" : base64_image , \"metadata\" : { \"prompt\" : self . shared_state . prompt , \"style\" : self . shared_state . style , \"resource_level\" : self . shared_state . resource_level , \"stages_run\" : self . shared_state . stages_run , \"stage_durations\" : self . shared_state . stage_durations , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat () } } except Exception as e : # Record failure for circuit breaker self . circuit_breaker . record_failure () # Use fallback on error logger . error ( f \"Pipeline error: { e } . Using fallback.\" ) return await self . _generate_fallback ( self . shared_state . prompt , str ( e )) Multi-Request Optimization \u00b6 The system implements a specialized scheduler for handling multiple concurrent requests using operations research principles: class RequestScheduler : \"\"\"Multi-request scheduler using max-flow principles.\"\"\" def __init__ ( self ): self . request_queue = asyncio . Queue () self . active_requests = {} self . resource_monitor = ResourceMonitor () async def schedule_requests ( self ): \"\"\"Schedule pending requests to maximize overall quality.\"\"\" available_resources = await self . resource_monitor . get_current_resources () # Use PuLP to solve the optimization problem model = pulp . LpProblem ( \"SVG_Pipeline_Scheduling\" , pulp . LpMaximize ) # Initialize decision variables for each request and stage x_vars = {} # Decision variable: run stage j for request i # Create decision variables for each request/stage combination for i , req in enumerate ( self . pending_requests ): for j in [ \"template\" , \"detail\" , \"optimize\" ]: x_vars [ i , j ] = pulp . LpVariable ( f \"x_ { i } _ { j } \" , cat = pulp . LpBinary ) # Create the objective function: maximize quality across all requests model += pulp . lpSum ([ self . quality_matrix [ j ] * x_vars [ i , j ] for i , _ in enumerate ( self . pending_requests ) for j in [ \"template\" , \"detail\" , \"optimize\" ] ]) # Add constraints # 1. Resource constraints for resource in [ \"cpu\" , \"memory\" , \"gpu\" ]: model += pulp . lpSum ([ self . resource_requirements [ j ][ resource ] * x_vars [ i , j ] for i , _ in enumerate ( self . pending_requests ) for j in [ \"template\" , \"detail\" , \"optimize\" ] ]) <= available_resources [ resource ] # 2. Dependency constraints (template must run first, etc.) for i , _ in enumerate ( self . pending_requests ): # Detail requires template model += x_vars [ i , \"detail\" ] <= x_vars [ i , \"template\" ] # Optimize requires detail model += x_vars [ i , \"optimize\" ] <= x_vars [ i , \"detail\" ] # Solve the optimization model model . solve () # Extract and return the solution scheduled_requests = [] for i , req in enumerate ( self . pending_requests ): stages = [ j for j in [ \"template\" , \"detail\" , \"optimize\" ] if pulp . value ( x_vars [ i , j ]) > 0.5 ] if stages : scheduled_requests . append (( req , stages )) return scheduled_requests Performance Benefits \u00b6 Expected Improvements \u00b6 Metric Original Implementation Optimized Implementation Improvement End-to-end latency 5-90s 3-40s 40-55% Memory usage pattern Spiky (file I/O) Consistent (in-memory) Smoother utilization CPU utilization Lower (I/O bound) Higher (compute bound) More efficient use Success rate under load 60-80% 85-95% 15-25% points Throughput Increase \u00b6 The multi-request optimization allows the system to process more requests concurrently: Original : ~2-3 concurrent requests Optimized : 4-8 concurrent requests (resource-dependent) Implementation Strategy \u00b6 We recommend a phased implementation approach: Phase 1: Memory Streaming \u00b6 Replace file I/O between stages with in-memory data transfer. # Before: Writing to disk between stages svg_file = os . path . join ( output_dir , \"template.svg\" ) with open ( svg_file , \"w\" , encoding = \"utf-8\" ) as f : f . write ( template_svg ) # After: Storing in memory pipeline_state . template_svg = template_svg Phase 2: Pipeline Controller \u00b6 Implement the centralized controller that manages all stages. Phase 3: Dynamic Resource Adaptation \u00b6 Add continuous resource monitoring and parameter adjustment. Phase 4: Multi-Request Optimization \u00b6 Implement the operations research-based scheduler for concurrent requests. Configuration \u00b6 The following configuration parameters can be adjusted: # File: app/config.py # Chat2SVG Resource Allocation CHAT2SVG_RESOURCE_THRESHOLDS = { \"high\" : { \"cpu_percent_available\" : int ( os . getenv ( \"CHAT2SVG_HIGH_CPU_THRESHOLD\" , \"50\" )), \"memory_percent_available\" : int ( os . getenv ( \"CHAT2SVG_HIGH_MEMORY_THRESHOLD\" , \"40\" )), }, \"medium\" : { \"cpu_percent_available\" : int ( os . getenv ( \"CHAT2SVG_MEDIUM_CPU_THRESHOLD\" , \"30\" )), \"memory_percent_available\" : int ( os . getenv ( \"CHAT2SVG_MEDIUM_MEMORY_THRESHOLD\" , \"20\" )), }, # Additional levels... } # Circuit Breaker Settings CHAT2SVG_CIRCUIT_BREAKER_THRESHOLD = int ( os . getenv ( \"CHAT2SVG_CIRCUIT_BREAKER_THRESHOLD\" , \"3\" )) CHAT2SVG_CIRCUIT_BREAKER_TIMEOUT = int ( os . getenv ( \"CHAT2SVG_CIRCUIT_BREAKER_TIMEOUT\" , \"60\" )) # Multi-Request Settings CHAT2SVG_MAX_CONCURRENT_REQUESTS = int ( os . getenv ( \"CHAT2SVG_MAX_CONCURRENT_REQUESTS\" , \"4\" )) 8. Conclusion \u00b6 This optimization approach transforms Chat2SVG from a sequential, file-based pipeline into an efficient memory-streaming architecture with intelligent resource allocation. By applying principles from circuit theory and operations research, we've created a system that: Maximizes performance through memory streaming Allocates resources optimally across stages and requests Dynamically adapts to changing system conditions Prevents cascading failures with circuit breakers Scales effectively under concurrent load The implementation is backwards compatible with the existing Chat2SVG scripts while providing significant performance improvements and enhanced reliability.","title":"Pipeline Optimization"},{"location":"infrastructure/pipeline-optimization/#chat2svg-pipeline-optimization-documentation","text":"","title":"Chat2SVG Pipeline Optimization Documentation"},{"location":"infrastructure/pipeline-optimization/#circuit-theory-operations-research-approach","text":"","title":"Circuit Theory &amp; Operations Research Approach"},{"location":"infrastructure/pipeline-optimization/#overview","text":"This document outlines the optimized Chat2SVG pipeline architecture using principles from circuit theory and operations research to maximize performance, resource utilization, and reliability.","title":"Overview"},{"location":"infrastructure/pipeline-optimization/#pipeline-architecture","text":"The optimized Chat2SVG pipeline integrates three stages (Template Generation, Detail Enhancement, and SVG Optimization) into a unified pipeline with shared memory, dynamic resource allocation, and adaptive processing. graph TB A[User Request] --> B[SVGPipelineController] B --> C[Resource Assessment] C --> D[Pipeline Configuration] D --> E{Circuit Breaker Gate} E -->|\"Closed\"| F[Memory Stream Pipeline] E -->|\"Open\"| G[Fallback Path] F --> H[Stage 1: Template] H -->|\"Memory Stream\"| I[Stage 2: Detail Enhancement] I -->|\"Memory Stream\"| J[Stage 3: Optimization] J --> K[Output Processing] G --> K M[Resource Monitor] -->|\"Feedback Loop\"| D M -->|\"Real-time Updates\"| I M -->|\"Parameter Tuning\"| J","title":"Pipeline Architecture"},{"location":"infrastructure/pipeline-optimization/#core-technologies","text":"","title":"Core Technologies"},{"location":"infrastructure/pipeline-optimization/#memory-streaming","text":"Replaces disk I/O operations between stages with in-memory data transfer, significantly reducing latency: class SVGPipelineState : \"\"\"Holds the intermediate data passed between stages in memory.\"\"\" def __init__ ( self , prompt : str , style : Optional [ str ] = None ): # Stage outputs stored in memory (not as paths) self . template_svg : Optional [ str ] = None self . enhanced_svg : Optional [ str ] = None self . target_png_bytes : Optional [ bytes ] = None # As bytes, not file path self . optimized_svg : Optional [ str ] = None","title":"Memory Streaming"},{"location":"infrastructure/pipeline-optimization/#circuit-theory-application","text":"Implements circuit breaker patterns to prevent cascading failures: class CircuitBreaker : \"\"\"Circuit breaker for managing pipeline availability.\"\"\" def __init__ ( self ): self . state = CircuitState . CLOSED self . failure_count = 0 self . last_failure_time = 0 self . reset_timeout = 60 # seconds self . failure_threshold = 3","title":"Circuit Theory Application"},{"location":"infrastructure/pipeline-optimization/#operations-research-solution","text":"Formalizes the resource allocation problem using mathematical optimization: class SVGPipelineOptimizer : \"\"\"Optimization engine based on operations research principles.\"\"\" def __init__ ( self ): # Quality contribution matrices for stages self . quality_matrix = { \"template\" : 0.6 , # 60% quality contribution \"detail\" : 0.3 , # 30% quality contribution \"optimize\" : 0.1 # 10% quality contribution } # Resource requirement matrices self . resource_requirements = { \"template\" : { \"cpu\" : 0.2 , \"memory\" : 0.2 , \"gpu\" : 0.0 }, \"detail\" : { \"cpu\" : 0.6 , \"memory\" : 0.8 , \"gpu\" : 0.9 }, \"optimize\" : { \"cpu\" : 0.4 , \"memory\" : 0.3 , \"gpu\" : 0.2 } }","title":"Operations Research Solution"},{"location":"infrastructure/pipeline-optimization/#dynamic-resource-adaptation","text":"Continuously monitors system resources and adapts pipeline parameters in real-time: async def _detect_available_resources ( self ) -> Dict [ str , float ]: \"\"\"Enhanced resource detection including GPU, RAM, and Swap.\"\"\" resources = { \"cpu\" : 10.0 , \"memory\" : 10.0 , \"swap\" : 0.0 } # Collect CPU, RAM, and swap information cpu_usage = await _to_thread ( psutil . cpu_percent , interval = 0.1 ) memory = await _to_thread ( psutil . virtual_memory ) swap = await _to_thread ( psutil . swap_memory ) resources [ \"cpu\" ] = 100.0 - cpu_usage resources [ \"memory\" ] = memory . available / memory . total * 100.0 resources [ \"swap\" ] = swap . percent # Check for GPU availability if torch is not None and torch . cuda . is_available (): # Get GPU utilization and memory with torch . cuda . device ( 0 ): total_vram = torch . cuda . get_device_properties ( 0 ) . total_memory / ( 1024 ** 3 ) used_vram = torch . cuda . memory_allocated () / ( 1024 ** 3 ) resources [ \"gpu\" ] = 100.0 resources [ \"vram\" ] = (( total_vram - used_vram ) / total_vram ) * 100.0 return resources","title":"Dynamic Resource Adaptation"},{"location":"infrastructure/pipeline-optimization/#pipeline-controller-implementation","text":"class SVGPipelineController : \"\"\"Coordinates the entire SVG generation pipeline with circuit breaker.\"\"\" def __init__ ( self ): self . optimizer = SVGPipelineOptimizer () self . resource_monitor = ResourceMonitor () self . circuit_breaker = CircuitBreaker () self . shared_state = SVGPipelineState () async def initialize ( self , prompt : str , style : Optional [ str ] = None ): \"\"\"Initialize the controller with unified resource assessment.\"\"\" self . shared_state . prompt = prompt self . shared_state . style = style # Create unified temporary directory structure self . pipeline_dir = tempfile . mkdtemp ( prefix = f \"chat2svg_ { _sanitize_filename ( prompt ) } _\" ) # Assess resources once for the entire pipeline resources = await self . _detect_available_resources () self . shared_state . resource_level = self . _determine_resource_level ( resources ) # Configure all stages in one pass self . _configure_pipeline ( resources ) # Determine stages to run based on resources and configuration self . shared_state . stages_to_run = self . _determine_active_stages () return self async def execute ( self ) -> Dict [ str , Any ]: \"\"\"Execute the pipeline with optimized memory streaming.\"\"\" start_time = time . time () # Check circuit breaker if not self . circuit_breaker . allow_request (): logger . warning ( \"Circuit breaker is open, using fallback\" ) return await self . _generate_fallback ( self . shared_state . prompt ) try : # Execute the optimized pipeline if \"template\" in self . shared_state . stages_to_run : if not await self . _execute_stage_1 (): raise PipelineError ( \"Template generation failed\" ) if \"detail\" in self . shared_state . stages_to_run : if not await self . _execute_stage_2 (): raise PipelineError ( \"Detail enhancement failed\" ) if \"optimize\" in self . shared_state . stages_to_run : if not await self . _execute_stage_3 (): raise PipelineError ( \"SVG optimization failed\" ) # Record success for circuit breaker self . circuit_breaker . record_success () # Return the final result svg_content = self . shared_state . optimized_svg or self . shared_state . enhanced_svg or self . shared_state . template_svg base64_image = _encode_svg_to_png_base64 ( svg_content ) return { \"svg_content\" : svg_content , \"base64_image\" : base64_image , \"metadata\" : { \"prompt\" : self . shared_state . prompt , \"style\" : self . shared_state . style , \"resource_level\" : self . shared_state . resource_level , \"stages_run\" : self . shared_state . stages_run , \"stage_durations\" : self . shared_state . stage_durations , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat () } } except Exception as e : # Record failure for circuit breaker self . circuit_breaker . record_failure () # Use fallback on error logger . error ( f \"Pipeline error: { e } . Using fallback.\" ) return await self . _generate_fallback ( self . shared_state . prompt , str ( e ))","title":"Pipeline Controller Implementation"},{"location":"infrastructure/pipeline-optimization/#multi-request-optimization","text":"The system implements a specialized scheduler for handling multiple concurrent requests using operations research principles: class RequestScheduler : \"\"\"Multi-request scheduler using max-flow principles.\"\"\" def __init__ ( self ): self . request_queue = asyncio . Queue () self . active_requests = {} self . resource_monitor = ResourceMonitor () async def schedule_requests ( self ): \"\"\"Schedule pending requests to maximize overall quality.\"\"\" available_resources = await self . resource_monitor . get_current_resources () # Use PuLP to solve the optimization problem model = pulp . LpProblem ( \"SVG_Pipeline_Scheduling\" , pulp . LpMaximize ) # Initialize decision variables for each request and stage x_vars = {} # Decision variable: run stage j for request i # Create decision variables for each request/stage combination for i , req in enumerate ( self . pending_requests ): for j in [ \"template\" , \"detail\" , \"optimize\" ]: x_vars [ i , j ] = pulp . LpVariable ( f \"x_ { i } _ { j } \" , cat = pulp . LpBinary ) # Create the objective function: maximize quality across all requests model += pulp . lpSum ([ self . quality_matrix [ j ] * x_vars [ i , j ] for i , _ in enumerate ( self . pending_requests ) for j in [ \"template\" , \"detail\" , \"optimize\" ] ]) # Add constraints # 1. Resource constraints for resource in [ \"cpu\" , \"memory\" , \"gpu\" ]: model += pulp . lpSum ([ self . resource_requirements [ j ][ resource ] * x_vars [ i , j ] for i , _ in enumerate ( self . pending_requests ) for j in [ \"template\" , \"detail\" , \"optimize\" ] ]) <= available_resources [ resource ] # 2. Dependency constraints (template must run first, etc.) for i , _ in enumerate ( self . pending_requests ): # Detail requires template model += x_vars [ i , \"detail\" ] <= x_vars [ i , \"template\" ] # Optimize requires detail model += x_vars [ i , \"optimize\" ] <= x_vars [ i , \"detail\" ] # Solve the optimization model model . solve () # Extract and return the solution scheduled_requests = [] for i , req in enumerate ( self . pending_requests ): stages = [ j for j in [ \"template\" , \"detail\" , \"optimize\" ] if pulp . value ( x_vars [ i , j ]) > 0.5 ] if stages : scheduled_requests . append (( req , stages )) return scheduled_requests","title":"Multi-Request Optimization"},{"location":"infrastructure/pipeline-optimization/#performance-benefits","text":"","title":"Performance Benefits"},{"location":"infrastructure/pipeline-optimization/#expected-improvements","text":"Metric Original Implementation Optimized Implementation Improvement End-to-end latency 5-90s 3-40s 40-55% Memory usage pattern Spiky (file I/O) Consistent (in-memory) Smoother utilization CPU utilization Lower (I/O bound) Higher (compute bound) More efficient use Success rate under load 60-80% 85-95% 15-25% points","title":"Expected Improvements"},{"location":"infrastructure/pipeline-optimization/#throughput-increase","text":"The multi-request optimization allows the system to process more requests concurrently: Original : ~2-3 concurrent requests Optimized : 4-8 concurrent requests (resource-dependent)","title":"Throughput Increase"},{"location":"infrastructure/pipeline-optimization/#implementation-strategy","text":"We recommend a phased implementation approach:","title":"Implementation Strategy"},{"location":"infrastructure/pipeline-optimization/#phase-1-memory-streaming","text":"Replace file I/O between stages with in-memory data transfer. # Before: Writing to disk between stages svg_file = os . path . join ( output_dir , \"template.svg\" ) with open ( svg_file , \"w\" , encoding = \"utf-8\" ) as f : f . write ( template_svg ) # After: Storing in memory pipeline_state . template_svg = template_svg","title":"Phase 1: Memory Streaming"},{"location":"infrastructure/pipeline-optimization/#phase-2-pipeline-controller","text":"Implement the centralized controller that manages all stages.","title":"Phase 2: Pipeline Controller"},{"location":"infrastructure/pipeline-optimization/#phase-3-dynamic-resource-adaptation","text":"Add continuous resource monitoring and parameter adjustment.","title":"Phase 3: Dynamic Resource Adaptation"},{"location":"infrastructure/pipeline-optimization/#phase-4-multi-request-optimization","text":"Implement the operations research-based scheduler for concurrent requests.","title":"Phase 4: Multi-Request Optimization"},{"location":"infrastructure/pipeline-optimization/#configuration","text":"The following configuration parameters can be adjusted: # File: app/config.py # Chat2SVG Resource Allocation CHAT2SVG_RESOURCE_THRESHOLDS = { \"high\" : { \"cpu_percent_available\" : int ( os . getenv ( \"CHAT2SVG_HIGH_CPU_THRESHOLD\" , \"50\" )), \"memory_percent_available\" : int ( os . getenv ( \"CHAT2SVG_HIGH_MEMORY_THRESHOLD\" , \"40\" )), }, \"medium\" : { \"cpu_percent_available\" : int ( os . getenv ( \"CHAT2SVG_MEDIUM_CPU_THRESHOLD\" , \"30\" )), \"memory_percent_available\" : int ( os . getenv ( \"CHAT2SVG_MEDIUM_MEMORY_THRESHOLD\" , \"20\" )), }, # Additional levels... } # Circuit Breaker Settings CHAT2SVG_CIRCUIT_BREAKER_THRESHOLD = int ( os . getenv ( \"CHAT2SVG_CIRCUIT_BREAKER_THRESHOLD\" , \"3\" )) CHAT2SVG_CIRCUIT_BREAKER_TIMEOUT = int ( os . getenv ( \"CHAT2SVG_CIRCUIT_BREAKER_TIMEOUT\" , \"60\" )) # Multi-Request Settings CHAT2SVG_MAX_CONCURRENT_REQUESTS = int ( os . getenv ( \"CHAT2SVG_MAX_CONCURRENT_REQUESTS\" , \"4\" ))","title":"Configuration"},{"location":"infrastructure/pipeline-optimization/#8-conclusion","text":"This optimization approach transforms Chat2SVG from a sequential, file-based pipeline into an efficient memory-streaming architecture with intelligent resource allocation. By applying principles from circuit theory and operations research, we've created a system that: Maximizes performance through memory streaming Allocates resources optimally across stages and requests Dynamically adapts to changing system conditions Prevents cascading failures with circuit breakers Scales effectively under concurrent load The implementation is backwards compatible with the existing Chat2SVG scripts while providing significant performance improvements and enhanced reliability.","title":"8. Conclusion"},{"location":"model-integration/architecture/","text":"Model Integration Architecture \u00b6 Overview \u00b6 The Opossum Search system implements a sophisticated model integration architecture that dynamically selects and utilizes various AI models based on request characteristics, service availability, and capability requirements. This hybrid approach enables resilience, flexibility, and optimal performance across diverse search scenarios. Architectural Principles \u00b6 The model integration architecture follows several key principles: Service Independence : No hard dependencies on any single AI service provider Graceful Degradation : System continues functioning when services are unavailable Capability-Based Routing : Requests are directed to models best suited to fulfill them Transparent Fallbacks : Fallback mechanisms operate without requiring client awareness Consistent Interface : Uniform API regardless of underlying model implementation Core Components \u00b6 graph TD A[Client Request] --> B[API Gateway] B --> C[Request Analyzer] C --> D[Model Selector] D --> E{Service Availability} E -->|Primary Option| F[Gemini Backend] E -->|Fallback Option| G[Ollama Backend] E -->|Local Fallback| H[Transformers Backend] F --> I[Response Formatter] G --> I H --> I I --> J[Client Response] K[Capability Matrix] -.-> D L[Configuration] -.-> D M[Service Monitor] -.-> E Key Components \u00b6 Request Analyzer : Examines incoming requests to determine their characteristics and requirements. Model Selector : The central decision-making component that determines which model backend to use for a given request. Service Availability : Monitors the status and availability of each model service. Capability Matrix : Defines the capabilities of each model backend and their suitability for different request types. Model Backends : Gemini Backend : Integration with Google's Gemini API Ollama Backend : Integration with locally deployed Ollama models Transformers Backend : Direct integration with Hugging Face Transformers Response Formatter : Ensures consistent response format regardless of the model used. Request Flow \u00b6 The typical flow of a request through the model integration system: def process_request ( request ): \"\"\"Process an incoming request through the model integration pipeline\"\"\" # 1. Analyze request to determine characteristics and requirements request_profile = RequestAnalyzer . analyze ( request ) # 2. Select optimal model based on request profile and current availability selected_model = ModelSelector . select_model ( request_profile = request_profile , capability_requirements = request_profile . capabilities_needed ) # 3. Process request with selected model try : response = selected_model . process ( request . content ) return format_response ( response ) except ModelUnavailableError : # 4. Handle failures with automatic fallback fallback_model = ModelSelector . select_fallback ( primary_model = selected_model , request_profile = request_profile ) response = fallback_model . process ( request . content ) return format_response ( response ) Hybrid Model Backend \u00b6 The core of the architecture is the HybridModelBackend class, which encapsulates the logic for selecting between different model backends: class HybridModelBackend : \"\"\"Manages multiple model backends with intelligent selection and fallback\"\"\" def __init__ ( self , config ): self . config = config self . backends = { 'gemini' : GeminiBackend ( config [ 'gemini' ]), 'ollama' : OllamaBackend ( config [ 'ollama' ]), 'transformers' : TransformersBackend ( config [ 'transformers' ]) } self . availability = ServiceAvailability () self . capability_matrix = CapabilityMatrix () def process ( self , query , context = None , requirements = None ): \"\"\"Process a query using the optimal model backend\"\"\" selected_backend = self . _select_backend ( query , requirements ) try : return selected_backend . process ( query , context ) except ServiceUnavailableError : # Mark service as unavailable and try fallback self . availability . mark_unavailable ( selected_backend . name ) fallback = self . _select_fallback ( selected_backend . name , requirements ) return fallback . process ( query , context ) def _select_backend ( self , query , requirements = None ): \"\"\"Select the optimal backend based on query and requirements\"\"\" # Default requirements if none specified requirements = requirements or [ 'general_knowledge' ] # Get available backends available_backends = { name : backend for name , backend in self . backends . items () if self . availability . is_available ( name ) } if not available_backends : # If nothing is available, try the local transformers backend # which should always be available as a last resort return self . backends [ 'transformers' ] # Score each available backend based on capability matrix scores = {} for name , backend in available_backends . items (): scores [ name ] = self . capability_matrix . score_backend ( backend_name = name , requirements = requirements , query = query ) # Select backend with highest score selected_name = max ( scores , key = scores . get ) return self . backends [ selected_name ] def _select_fallback ( self , failed_backend , requirements = None ): \"\"\"Select a fallback backend when the primary choice fails\"\"\" # Define fallback chain fallback_chain = { 'gemini' : 'ollama' , 'ollama' : 'transformers' , 'transformers' : 'gemini' # Circular as last resort } # Try the next backend in the fallback chain next_backend = fallback_chain [ failed_backend ] # If that's available, use it if self . availability . is_available ( next_backend ): return self . backends [ next_backend ] # Otherwise find any available backend for name , backend in self . backends . items (): if name != failed_backend and self . availability . is_available ( name ): return backend # If all else fails, use transformers as last resort return self . backends [ 'transformers' ] Service Availability \u00b6 The ServiceAvailability component monitors the health and availability of model services: class ServiceAvailability : \"\"\"Tracks availability of model services and implements circuit breaker pattern\"\"\" def __init__ ( self ): self . status = { 'gemini' : { 'available' : True , 'last_checked' : time . time ()}, 'ollama' : { 'available' : True , 'last_checked' : time . time ()}, 'transformers' : { 'available' : True , 'last_checked' : time . time ()} } self . failure_thresholds = { 'gemini' : 3 , 'ollama' : 2 , 'transformers' : 5 } self . failure_counts = { 'gemini' : 0 , 'ollama' : 0 , 'transformers' : 0 } self . recovery_intervals = { 'gemini' : 300 , # 5 minutes 'ollama' : 60 , # 1 minute 'transformers' : 120 # 2 minutes } def is_available ( self , service_name ): \"\"\"Check if a service is currently available\"\"\" status = self . status . get ( service_name , { 'available' : False }) # If service is marked unavailable, check if recovery interval has passed if not status [ 'available' ]: elapsed = time . time () - status [ 'last_checked' ] if elapsed > self . recovery_intervals . get ( service_name , 300 ): # Time to retry this service self . status [ service_name ][ 'available' ] = True self . failure_counts [ service_name ] = 0 return self . status . get ( service_name , { 'available' : False })[ 'available' ] def mark_unavailable ( self , service_name ): \"\"\"Mark a service as unavailable after failure\"\"\" self . failure_counts [ service_name ] = self . failure_counts . get ( service_name , 0 ) + 1 # If we've hit the threshold, mark service as unavailable if self . failure_counts [ service_name ] >= self . failure_thresholds . get ( service_name , 3 ): self . status [ service_name ] = { 'available' : False , 'last_checked' : time . time () } logger . warning ( f \"Service { service_name } marked unavailable after { self . failure_counts [ service_name ] } failures\" ) def mark_successful ( self , service_name ): \"\"\"Mark a successful service call to reset failure count\"\"\" self . failure_counts [ service_name ] = 0 self . status [ service_name ] = { 'available' : True , 'last_checked' : time . time () } Multi-Modal Handling \u00b6 The model integration architecture supports multi-modal requests involving text, images, and structured data: def process_multimodal_request ( text , images = None , structured_data = None ): \"\"\"Process a request containing multiple input modalities\"\"\" # Determine required capabilities based on input types capabilities = [ 'text_understanding' ] if images : capabilities . append ( 'image_understanding' ) if structured_data : capabilities . append ( 'structured_data_processing' ) # Select model based on required capabilities model = ModelSelector . select_model ( capabilities = capabilities ) # Process with selected model if model . supports_multimodal_input (): # Model can process all modalities together return model . process_multimodal ( text , images , structured_data ) else : # Handle with separate processing and integration text_result = model . process_text ( text ) if images and model . supports_capability ( 'image_understanding' ): image_results = [ model . process_image ( img ) for img in images ] else : # Fallback to image-specific model image_model = ModelSelector . select_model ( capabilities = [ 'image_understanding' ]) image_results = [ image_model . process_image ( img ) for img in images ] # Integrate results return integrate_multimodal_results ( text_result , image_results , structured_data ) Resilience Patterns \u00b6 The architecture implements several resilience patterns: Circuit Breaker : Temporarily disables services after consecutive failures Fallback Chain : Defines explicit fallback paths when services fail Capability Degradation : Adjusts capabilities based on available services Automatic Retry : Implements retry with backoff for transient errors Health Monitoring : Proactively checks service health Configuration Example \u00b6 model_integration : default_backend : \"gemini\" backends : gemini : api_key : \"${GEMINI_API_KEY}\" timeout : 10 max_retries : 2 model : \"gemini-pro\" ollama : host : \"localhost\" port : 11434 timeout : 15 models : - name : \"llama2\" default : true - name : \"mistral\" - name : \"vicuna\" transformers : model_path : \"./models/local\" default_model : \"microsoft/phi-2\" quantization : \"int8\" device : \"cuda\" availability : check_interval : 60 failure_thresholds : gemini : 3 ollama : 2 transformers : 5 recovery_intervals : gemini : 300 ollama : 60 transformers : 120 Related Documentation \u00b6 Backend Selection: Details on how models are selected for specific requests Capability Matrix: Comprehensive mapping of model capabilities Provider Integration: Specific implementation details for each provider Model Configuration: Configuration options for model backends","title":"Architecture"},{"location":"model-integration/architecture/#model-integration-architecture","text":"","title":"Model Integration Architecture"},{"location":"model-integration/architecture/#overview","text":"The Opossum Search system implements a sophisticated model integration architecture that dynamically selects and utilizes various AI models based on request characteristics, service availability, and capability requirements. This hybrid approach enables resilience, flexibility, and optimal performance across diverse search scenarios.","title":"Overview"},{"location":"model-integration/architecture/#architectural-principles","text":"The model integration architecture follows several key principles: Service Independence : No hard dependencies on any single AI service provider Graceful Degradation : System continues functioning when services are unavailable Capability-Based Routing : Requests are directed to models best suited to fulfill them Transparent Fallbacks : Fallback mechanisms operate without requiring client awareness Consistent Interface : Uniform API regardless of underlying model implementation","title":"Architectural Principles"},{"location":"model-integration/architecture/#core-components","text":"graph TD A[Client Request] --> B[API Gateway] B --> C[Request Analyzer] C --> D[Model Selector] D --> E{Service Availability} E -->|Primary Option| F[Gemini Backend] E -->|Fallback Option| G[Ollama Backend] E -->|Local Fallback| H[Transformers Backend] F --> I[Response Formatter] G --> I H --> I I --> J[Client Response] K[Capability Matrix] -.-> D L[Configuration] -.-> D M[Service Monitor] -.-> E","title":"Core Components"},{"location":"model-integration/architecture/#key-components","text":"Request Analyzer : Examines incoming requests to determine their characteristics and requirements. Model Selector : The central decision-making component that determines which model backend to use for a given request. Service Availability : Monitors the status and availability of each model service. Capability Matrix : Defines the capabilities of each model backend and their suitability for different request types. Model Backends : Gemini Backend : Integration with Google's Gemini API Ollama Backend : Integration with locally deployed Ollama models Transformers Backend : Direct integration with Hugging Face Transformers Response Formatter : Ensures consistent response format regardless of the model used.","title":"Key Components"},{"location":"model-integration/architecture/#request-flow","text":"The typical flow of a request through the model integration system: def process_request ( request ): \"\"\"Process an incoming request through the model integration pipeline\"\"\" # 1. Analyze request to determine characteristics and requirements request_profile = RequestAnalyzer . analyze ( request ) # 2. Select optimal model based on request profile and current availability selected_model = ModelSelector . select_model ( request_profile = request_profile , capability_requirements = request_profile . capabilities_needed ) # 3. Process request with selected model try : response = selected_model . process ( request . content ) return format_response ( response ) except ModelUnavailableError : # 4. Handle failures with automatic fallback fallback_model = ModelSelector . select_fallback ( primary_model = selected_model , request_profile = request_profile ) response = fallback_model . process ( request . content ) return format_response ( response )","title":"Request Flow"},{"location":"model-integration/architecture/#hybrid-model-backend","text":"The core of the architecture is the HybridModelBackend class, which encapsulates the logic for selecting between different model backends: class HybridModelBackend : \"\"\"Manages multiple model backends with intelligent selection and fallback\"\"\" def __init__ ( self , config ): self . config = config self . backends = { 'gemini' : GeminiBackend ( config [ 'gemini' ]), 'ollama' : OllamaBackend ( config [ 'ollama' ]), 'transformers' : TransformersBackend ( config [ 'transformers' ]) } self . availability = ServiceAvailability () self . capability_matrix = CapabilityMatrix () def process ( self , query , context = None , requirements = None ): \"\"\"Process a query using the optimal model backend\"\"\" selected_backend = self . _select_backend ( query , requirements ) try : return selected_backend . process ( query , context ) except ServiceUnavailableError : # Mark service as unavailable and try fallback self . availability . mark_unavailable ( selected_backend . name ) fallback = self . _select_fallback ( selected_backend . name , requirements ) return fallback . process ( query , context ) def _select_backend ( self , query , requirements = None ): \"\"\"Select the optimal backend based on query and requirements\"\"\" # Default requirements if none specified requirements = requirements or [ 'general_knowledge' ] # Get available backends available_backends = { name : backend for name , backend in self . backends . items () if self . availability . is_available ( name ) } if not available_backends : # If nothing is available, try the local transformers backend # which should always be available as a last resort return self . backends [ 'transformers' ] # Score each available backend based on capability matrix scores = {} for name , backend in available_backends . items (): scores [ name ] = self . capability_matrix . score_backend ( backend_name = name , requirements = requirements , query = query ) # Select backend with highest score selected_name = max ( scores , key = scores . get ) return self . backends [ selected_name ] def _select_fallback ( self , failed_backend , requirements = None ): \"\"\"Select a fallback backend when the primary choice fails\"\"\" # Define fallback chain fallback_chain = { 'gemini' : 'ollama' , 'ollama' : 'transformers' , 'transformers' : 'gemini' # Circular as last resort } # Try the next backend in the fallback chain next_backend = fallback_chain [ failed_backend ] # If that's available, use it if self . availability . is_available ( next_backend ): return self . backends [ next_backend ] # Otherwise find any available backend for name , backend in self . backends . items (): if name != failed_backend and self . availability . is_available ( name ): return backend # If all else fails, use transformers as last resort return self . backends [ 'transformers' ]","title":"Hybrid Model Backend"},{"location":"model-integration/architecture/#service-availability","text":"The ServiceAvailability component monitors the health and availability of model services: class ServiceAvailability : \"\"\"Tracks availability of model services and implements circuit breaker pattern\"\"\" def __init__ ( self ): self . status = { 'gemini' : { 'available' : True , 'last_checked' : time . time ()}, 'ollama' : { 'available' : True , 'last_checked' : time . time ()}, 'transformers' : { 'available' : True , 'last_checked' : time . time ()} } self . failure_thresholds = { 'gemini' : 3 , 'ollama' : 2 , 'transformers' : 5 } self . failure_counts = { 'gemini' : 0 , 'ollama' : 0 , 'transformers' : 0 } self . recovery_intervals = { 'gemini' : 300 , # 5 minutes 'ollama' : 60 , # 1 minute 'transformers' : 120 # 2 minutes } def is_available ( self , service_name ): \"\"\"Check if a service is currently available\"\"\" status = self . status . get ( service_name , { 'available' : False }) # If service is marked unavailable, check if recovery interval has passed if not status [ 'available' ]: elapsed = time . time () - status [ 'last_checked' ] if elapsed > self . recovery_intervals . get ( service_name , 300 ): # Time to retry this service self . status [ service_name ][ 'available' ] = True self . failure_counts [ service_name ] = 0 return self . status . get ( service_name , { 'available' : False })[ 'available' ] def mark_unavailable ( self , service_name ): \"\"\"Mark a service as unavailable after failure\"\"\" self . failure_counts [ service_name ] = self . failure_counts . get ( service_name , 0 ) + 1 # If we've hit the threshold, mark service as unavailable if self . failure_counts [ service_name ] >= self . failure_thresholds . get ( service_name , 3 ): self . status [ service_name ] = { 'available' : False , 'last_checked' : time . time () } logger . warning ( f \"Service { service_name } marked unavailable after { self . failure_counts [ service_name ] } failures\" ) def mark_successful ( self , service_name ): \"\"\"Mark a successful service call to reset failure count\"\"\" self . failure_counts [ service_name ] = 0 self . status [ service_name ] = { 'available' : True , 'last_checked' : time . time () }","title":"Service Availability"},{"location":"model-integration/architecture/#multi-modal-handling","text":"The model integration architecture supports multi-modal requests involving text, images, and structured data: def process_multimodal_request ( text , images = None , structured_data = None ): \"\"\"Process a request containing multiple input modalities\"\"\" # Determine required capabilities based on input types capabilities = [ 'text_understanding' ] if images : capabilities . append ( 'image_understanding' ) if structured_data : capabilities . append ( 'structured_data_processing' ) # Select model based on required capabilities model = ModelSelector . select_model ( capabilities = capabilities ) # Process with selected model if model . supports_multimodal_input (): # Model can process all modalities together return model . process_multimodal ( text , images , structured_data ) else : # Handle with separate processing and integration text_result = model . process_text ( text ) if images and model . supports_capability ( 'image_understanding' ): image_results = [ model . process_image ( img ) for img in images ] else : # Fallback to image-specific model image_model = ModelSelector . select_model ( capabilities = [ 'image_understanding' ]) image_results = [ image_model . process_image ( img ) for img in images ] # Integrate results return integrate_multimodal_results ( text_result , image_results , structured_data )","title":"Multi-Modal Handling"},{"location":"model-integration/architecture/#resilience-patterns","text":"The architecture implements several resilience patterns: Circuit Breaker : Temporarily disables services after consecutive failures Fallback Chain : Defines explicit fallback paths when services fail Capability Degradation : Adjusts capabilities based on available services Automatic Retry : Implements retry with backoff for transient errors Health Monitoring : Proactively checks service health","title":"Resilience Patterns"},{"location":"model-integration/architecture/#configuration-example","text":"model_integration : default_backend : \"gemini\" backends : gemini : api_key : \"${GEMINI_API_KEY}\" timeout : 10 max_retries : 2 model : \"gemini-pro\" ollama : host : \"localhost\" port : 11434 timeout : 15 models : - name : \"llama2\" default : true - name : \"mistral\" - name : \"vicuna\" transformers : model_path : \"./models/local\" default_model : \"microsoft/phi-2\" quantization : \"int8\" device : \"cuda\" availability : check_interval : 60 failure_thresholds : gemini : 3 ollama : 2 transformers : 5 recovery_intervals : gemini : 300 ollama : 60 transformers : 120","title":"Configuration Example"},{"location":"model-integration/architecture/#related-documentation","text":"Backend Selection: Details on how models are selected for specific requests Capability Matrix: Comprehensive mapping of model capabilities Provider Integration: Specific implementation details for each provider Model Configuration: Configuration options for model backends","title":"Related Documentation"},{"location":"model-integration/backend-selection/","text":"Backend Selection \u00b6 Overview \u00b6 Opossum Search implements a sophisticated backend selection system that dynamically chooses the optimal AI model for each request. This selection process considers multiple factors including request characteristics, model capabilities, service availability, and performance requirements to make intelligent routing decisions. Selection Process \u00b6 The backend selection process follows this general flow: flowchart TD A[Incoming Request] --> B[Analyze Request] B --> C[Extract Capability Requirements] C --> D[Check Service Availability] D --> E[Score Available Backends] E --> F[Select Highest Scoring Backend] F --> G[Process Request] G -->|Success| H[Return Response] G -->|Failure| I[Select Fallback Backend] I --> J[Process with Fallback] J --> H Request Analysis \u00b6 The first step is analyzing the request to determine its characteristics and requirements: def analyze_request ( request ): \"\"\"Analyze request to determine capabilities needed\"\"\" capabilities = [ \"general_knowledge\" ] # Base capability for all requests # Check for multimodal content if request . has_images (): capabilities . append ( \"image_understanding\" ) # Specific image capabilities if detect_image_type ( request . images [ 0 ]) == \"chart\" : capabilities . append ( \"chart_analysis\" ) # Check for code-related queries if is_code_related ( request . query ): capabilities . append ( \"code_understanding\" ) # Specific programming language lang = detect_language ( request . query ) if lang : capabilities . append ( f \" { lang } _expertise\" ) # Check for specialized domains domain = detect_domain ( request . query ) if domain : capabilities . append ( f \" { domain } _knowledge\" ) # Check complexity for resource allocation if estimate_complexity ( request ) > COMPLEXITY_THRESHOLD : capabilities . append ( \"high_complexity_handling\" ) return capabilities Scoring System \u00b6 The backend selection system uses a weighted scoring approach to rank available backends: Capability Matching \u00b6 Each backend is scored based on how well it matches the required capabilities: def score_backend_capabilities ( backend_name , required_capabilities ): \"\"\"Score a backend based on capability matching\"\"\" # Get backend capabilities from the capability matrix backend_capabilities = capability_matrix . get_capabilities ( backend_name ) score = 0 for capability in required_capabilities : # Check if backend supports this capability if capability in backend_capabilities : capability_score = capability_matrix . get_score ( backend_name , capability ) score += capability_score else : # Penalize missing critical capabilities if capability in CRITICAL_CAPABILITIES : score -= CRITICAL_CAPABILITY_PENALTY return score Performance Considerations \u00b6 Performance characteristics are factored into the selection: def score_backend_performance ( backend_name , request ): \"\"\"Score a backend based on performance characteristics\"\"\" performance_score = 0 # Response time considerations if request . requires_low_latency (): response_time_score = performance_metrics . get_response_time_score ( backend_name ) performance_score += LATENCY_WEIGHT * response_time_score # Resource efficiency if request . is_resource_intensive (): efficiency_score = performance_metrics . get_efficiency_score ( backend_name ) performance_score += EFFICIENCY_WEIGHT * efficiency_score # Reliability score based on recent performance reliability_score = performance_metrics . get_reliability_score ( backend_name ) performance_score += RELIABILITY_WEIGHT * reliability_score return performance_score Cost Optimization \u00b6 For systems with usage-based pricing (like Gemini API), cost factors are considered: def score_backend_cost ( backend_name , request ): \"\"\"Score a backend based on cost considerations\"\"\" # Base cost for this backend base_cost = cost_model . get_base_cost ( backend_name ) # Estimated tokens for this request estimated_tokens = estimate_tokens ( request ) # Calculate estimated cost estimated_cost = base_cost * estimated_tokens # Convert to score (lower cost = higher score) max_cost = cost_model . get_max_cost () cost_score = 1.0 - ( estimated_cost / max_cost ) return cost_score * COST_WEIGHT Selection Algorithm \u00b6 The core selection algorithm combines multiple factors to choose the optimal backend: def select_backend ( request ): \"\"\"Select the optimal backend for a request\"\"\" # Analyze request required_capabilities = analyze_request ( request ) # Get available backends available_backends = service_availability . get_available_backends () if not available_backends : # If no backends are available, use local transformers as last resort logger . warning ( \"No backends available, falling back to local transformers\" ) return backends [ 'transformers' ] # Score each available backend scores = {} for backend_name in available_backends : # Calculate capability score (weighted highest) capability_score = score_backend_capabilities ( backend_name , required_capabilities ) # Calculate performance score performance_score = score_backend_performance ( backend_name , request ) # Calculate cost score if applicable cost_score = score_backend_cost ( backend_name , request ) # Calculate overall score with appropriate weights scores [ backend_name ] = ( CAPABILITY_WEIGHT * capability_score + PERFORMANCE_WEIGHT * performance_score + COST_WEIGHT * cost_score ) # Select backend with highest score selected_backend = max ( scores , key = scores . get ) logger . info ( f \"Selected backend { selected_backend } with score { scores [ selected_backend ] } \" ) logger . debug ( f \"All backend scores: { scores } \" ) return backends [ selected_backend ] Fallback Mechanism \u00b6 When a selected backend fails, the system implements a sophisticated fallback mechanism: def select_fallback ( failed_backend , required_capabilities ): \"\"\"Select a fallback backend when the primary choice fails\"\"\" # Define the default fallback chain fallback_chain = { 'gemini' : 'ollama' , 'ollama' : 'transformers' , 'transformers' : 'gemini' # Circular as last resort } # Try the default fallback if available next_backend = fallback_chain . get ( failed_backend , 'transformers' ) if service_availability . is_available ( next_backend ): return backends [ next_backend ] # If default fallback isn't available, score remaining backends available_backends = [ b for b in service_availability . get_available_backends () if b != failed_backend ] if not available_backends : # If nothing else is available, use transformers as final fallback # (assumes transformers can always run locally) logger . error ( f \"No available fallbacks for { failed_backend } , using transformers as last resort\" ) return backends [ 'transformers' ] # Score available backends for this specific request scores = {} for backend_name in available_backends : capability_score = score_backend_capabilities ( backend_name , required_capabilities ) scores [ backend_name ] = capability_score # Select highest scoring available backend selected_fallback = max ( scores , key = scores . get ) logger . info ( f \"Selected { selected_fallback } as fallback for { failed_backend } \" ) return backends [ selected_fallback ] Decision Factors \u00b6 The backend selection considers these key factors: 1. Capability Requirements \u00b6 Different request types require different model capabilities: Request Type Key Capabilities Text-only queries general_knowledge , text_understanding Image analysis image_understanding , visual_reasoning Code questions code_understanding , [language]_expertise Creative tasks creative_generation , long_context Technical domains technical_knowledge , domain_expertise 2. Service Characteristics \u00b6 Each backend has different characteristics that affect selection: Backend Strengths Limitations Gemini Advanced reasoning, multimodal, up-to-date knowledge External dependency, cost, potential latency Ollama Local deployment, customization, no data sharing Limited context window, less powerful than Gemini Transformers Maximum control, always available locally Most limited capabilities, higher resource usage 3. Operational Factors \u00b6 Operational considerations that influence selection: Availability : Current status of each service Response time : Historical and current latency Cost : Usage-based pricing considerations Resource usage : CPU, memory, and GPU utilization Error rates : Recent failure patterns Selection Examples \u00b6 Example 1: General Knowledge Query \u00b6 For a straightforward factual question: Query: \"What is the capital of France?\" Selection process: Required capabilities: [\"general_knowledge\"] All backends support this capability Assuming all are available, scores might be: Gemini: 0.95 (highest accuracy) Ollama: 0.85 (good accuracy, lower latency) Transformers: 0.70 (limited knowledge) Selected: Gemini Example 2: Image Analysis with Unavailable Primary \u00b6 For an image analysis request when Gemini is unavailable: Query: \"What's shown in this image?\" with attached image Selection process: Required capabilities: [\"image_understanding\"] Gemini scores highest but is unavailable Fallback scores: Ollama: 0.75 (limited image capabilities) Transformers: 0.60 (basic image understanding) Selected: Ollama as fallback Example 3: Complex Code Analysis \u00b6 For a complex programming question: Query: \"Explain this Python decorator pattern and suggest improvements\" with code snippet Selection process: Required capabilities: [\"code_understanding\", \"python_expertise\"] Scores: Gemini: 0.92 (strong code understanding) Ollama: 0.88 (good code capabilities with CodeLlama) Transformers: 0.65 (limited code analysis) If optimizing for cost: Ollama might be selected despite slightly lower score If optimizing for quality: Gemini would be selected Configuration Options \u00b6 The backend selection system offers several configuration options: backend_selection : # Scoring weights weights : capability : 0.6 # Capability matching importance performance : 0.3 # Performance considerations cost : 0.1 # Cost optimization (if applicable) # Selection preferences preferences : prefer_local : false # Prefer local models when capabilities are similar cost_sensitive : false # Increase cost factor importance latency_threshold : 500 # Max acceptable latency in ms # Fallback configuration fallbacks : retry_original : true # Whether to retry original backend after timeout fallback_chain : gemini : \"ollama\" ollama : \"transformers\" transformers : \"gemini\" max_fallbacks : 2 # Maximum number of fallbacks to attempt # Feature flags features : dynamic_scoring : true # Adjust scores based on historical performance context_aware : true # Consider conversation context in selection adaptive_routing : true # Learn from successful/failed selections Monitoring and Telemetry \u00b6 The backend selection system collects detailed telemetry to improve decision quality: def record_selection_telemetry ( request , selected_backend , success , duration_ms ): \"\"\"Record telemetry data for backend selection\"\"\" telemetry = { \"timestamp\" : time . time (), \"request_type\" : categorize_request ( request ), \"required_capabilities\" : analyze_request ( request ), \"selected_backend\" : selected_backend . name , \"success\" : success , \"duration_ms\" : duration_ms , \"fallback_used\" : selected_backend . is_fallback , \"scores\" : backend_scores # Scores from selection process } # Save telemetry telemetry_store . add ( telemetry ) # Update metrics for monitoring if success : metrics . increment ( f \"backend. { selected_backend . name } .success\" ) else : metrics . increment ( f \"backend. { selected_backend . name } .failure\" ) metrics . record ( f \"backend. { selected_backend . name } .duration\" , duration_ms ) This telemetry data enables: Optimization of scoring weights Identification of backend strengths and weaknesses Detection of patterns in backend performance Continuous improvement of selection accuracy Related Documentation \u00b6 Model Integration Architecture: Overall architecture of the model integration system Capability Matrix: Detailed mapping of model capabilities Provider Integration: Implementation details for each provider Model Configuration: Configuration options for model backends Service Availability: How service availability is monitored","title":"Backend Selection"},{"location":"model-integration/backend-selection/#backend-selection","text":"","title":"Backend Selection"},{"location":"model-integration/backend-selection/#overview","text":"Opossum Search implements a sophisticated backend selection system that dynamically chooses the optimal AI model for each request. This selection process considers multiple factors including request characteristics, model capabilities, service availability, and performance requirements to make intelligent routing decisions.","title":"Overview"},{"location":"model-integration/backend-selection/#selection-process","text":"The backend selection process follows this general flow: flowchart TD A[Incoming Request] --> B[Analyze Request] B --> C[Extract Capability Requirements] C --> D[Check Service Availability] D --> E[Score Available Backends] E --> F[Select Highest Scoring Backend] F --> G[Process Request] G -->|Success| H[Return Response] G -->|Failure| I[Select Fallback Backend] I --> J[Process with Fallback] J --> H","title":"Selection Process"},{"location":"model-integration/backend-selection/#request-analysis","text":"The first step is analyzing the request to determine its characteristics and requirements: def analyze_request ( request ): \"\"\"Analyze request to determine capabilities needed\"\"\" capabilities = [ \"general_knowledge\" ] # Base capability for all requests # Check for multimodal content if request . has_images (): capabilities . append ( \"image_understanding\" ) # Specific image capabilities if detect_image_type ( request . images [ 0 ]) == \"chart\" : capabilities . append ( \"chart_analysis\" ) # Check for code-related queries if is_code_related ( request . query ): capabilities . append ( \"code_understanding\" ) # Specific programming language lang = detect_language ( request . query ) if lang : capabilities . append ( f \" { lang } _expertise\" ) # Check for specialized domains domain = detect_domain ( request . query ) if domain : capabilities . append ( f \" { domain } _knowledge\" ) # Check complexity for resource allocation if estimate_complexity ( request ) > COMPLEXITY_THRESHOLD : capabilities . append ( \"high_complexity_handling\" ) return capabilities","title":"Request Analysis"},{"location":"model-integration/backend-selection/#scoring-system","text":"The backend selection system uses a weighted scoring approach to rank available backends:","title":"Scoring System"},{"location":"model-integration/backend-selection/#capability-matching","text":"Each backend is scored based on how well it matches the required capabilities: def score_backend_capabilities ( backend_name , required_capabilities ): \"\"\"Score a backend based on capability matching\"\"\" # Get backend capabilities from the capability matrix backend_capabilities = capability_matrix . get_capabilities ( backend_name ) score = 0 for capability in required_capabilities : # Check if backend supports this capability if capability in backend_capabilities : capability_score = capability_matrix . get_score ( backend_name , capability ) score += capability_score else : # Penalize missing critical capabilities if capability in CRITICAL_CAPABILITIES : score -= CRITICAL_CAPABILITY_PENALTY return score","title":"Capability Matching"},{"location":"model-integration/backend-selection/#performance-considerations","text":"Performance characteristics are factored into the selection: def score_backend_performance ( backend_name , request ): \"\"\"Score a backend based on performance characteristics\"\"\" performance_score = 0 # Response time considerations if request . requires_low_latency (): response_time_score = performance_metrics . get_response_time_score ( backend_name ) performance_score += LATENCY_WEIGHT * response_time_score # Resource efficiency if request . is_resource_intensive (): efficiency_score = performance_metrics . get_efficiency_score ( backend_name ) performance_score += EFFICIENCY_WEIGHT * efficiency_score # Reliability score based on recent performance reliability_score = performance_metrics . get_reliability_score ( backend_name ) performance_score += RELIABILITY_WEIGHT * reliability_score return performance_score","title":"Performance Considerations"},{"location":"model-integration/backend-selection/#cost-optimization","text":"For systems with usage-based pricing (like Gemini API), cost factors are considered: def score_backend_cost ( backend_name , request ): \"\"\"Score a backend based on cost considerations\"\"\" # Base cost for this backend base_cost = cost_model . get_base_cost ( backend_name ) # Estimated tokens for this request estimated_tokens = estimate_tokens ( request ) # Calculate estimated cost estimated_cost = base_cost * estimated_tokens # Convert to score (lower cost = higher score) max_cost = cost_model . get_max_cost () cost_score = 1.0 - ( estimated_cost / max_cost ) return cost_score * COST_WEIGHT","title":"Cost Optimization"},{"location":"model-integration/backend-selection/#selection-algorithm","text":"The core selection algorithm combines multiple factors to choose the optimal backend: def select_backend ( request ): \"\"\"Select the optimal backend for a request\"\"\" # Analyze request required_capabilities = analyze_request ( request ) # Get available backends available_backends = service_availability . get_available_backends () if not available_backends : # If no backends are available, use local transformers as last resort logger . warning ( \"No backends available, falling back to local transformers\" ) return backends [ 'transformers' ] # Score each available backend scores = {} for backend_name in available_backends : # Calculate capability score (weighted highest) capability_score = score_backend_capabilities ( backend_name , required_capabilities ) # Calculate performance score performance_score = score_backend_performance ( backend_name , request ) # Calculate cost score if applicable cost_score = score_backend_cost ( backend_name , request ) # Calculate overall score with appropriate weights scores [ backend_name ] = ( CAPABILITY_WEIGHT * capability_score + PERFORMANCE_WEIGHT * performance_score + COST_WEIGHT * cost_score ) # Select backend with highest score selected_backend = max ( scores , key = scores . get ) logger . info ( f \"Selected backend { selected_backend } with score { scores [ selected_backend ] } \" ) logger . debug ( f \"All backend scores: { scores } \" ) return backends [ selected_backend ]","title":"Selection Algorithm"},{"location":"model-integration/backend-selection/#fallback-mechanism","text":"When a selected backend fails, the system implements a sophisticated fallback mechanism: def select_fallback ( failed_backend , required_capabilities ): \"\"\"Select a fallback backend when the primary choice fails\"\"\" # Define the default fallback chain fallback_chain = { 'gemini' : 'ollama' , 'ollama' : 'transformers' , 'transformers' : 'gemini' # Circular as last resort } # Try the default fallback if available next_backend = fallback_chain . get ( failed_backend , 'transformers' ) if service_availability . is_available ( next_backend ): return backends [ next_backend ] # If default fallback isn't available, score remaining backends available_backends = [ b for b in service_availability . get_available_backends () if b != failed_backend ] if not available_backends : # If nothing else is available, use transformers as final fallback # (assumes transformers can always run locally) logger . error ( f \"No available fallbacks for { failed_backend } , using transformers as last resort\" ) return backends [ 'transformers' ] # Score available backends for this specific request scores = {} for backend_name in available_backends : capability_score = score_backend_capabilities ( backend_name , required_capabilities ) scores [ backend_name ] = capability_score # Select highest scoring available backend selected_fallback = max ( scores , key = scores . get ) logger . info ( f \"Selected { selected_fallback } as fallback for { failed_backend } \" ) return backends [ selected_fallback ]","title":"Fallback Mechanism"},{"location":"model-integration/backend-selection/#decision-factors","text":"The backend selection considers these key factors:","title":"Decision Factors"},{"location":"model-integration/backend-selection/#1-capability-requirements","text":"Different request types require different model capabilities: Request Type Key Capabilities Text-only queries general_knowledge , text_understanding Image analysis image_understanding , visual_reasoning Code questions code_understanding , [language]_expertise Creative tasks creative_generation , long_context Technical domains technical_knowledge , domain_expertise","title":"1. Capability Requirements"},{"location":"model-integration/backend-selection/#2-service-characteristics","text":"Each backend has different characteristics that affect selection: Backend Strengths Limitations Gemini Advanced reasoning, multimodal, up-to-date knowledge External dependency, cost, potential latency Ollama Local deployment, customization, no data sharing Limited context window, less powerful than Gemini Transformers Maximum control, always available locally Most limited capabilities, higher resource usage","title":"2. Service Characteristics"},{"location":"model-integration/backend-selection/#3-operational-factors","text":"Operational considerations that influence selection: Availability : Current status of each service Response time : Historical and current latency Cost : Usage-based pricing considerations Resource usage : CPU, memory, and GPU utilization Error rates : Recent failure patterns","title":"3. Operational Factors"},{"location":"model-integration/backend-selection/#selection-examples","text":"","title":"Selection Examples"},{"location":"model-integration/backend-selection/#example-1-general-knowledge-query","text":"For a straightforward factual question: Query: \"What is the capital of France?\" Selection process: Required capabilities: [\"general_knowledge\"] All backends support this capability Assuming all are available, scores might be: Gemini: 0.95 (highest accuracy) Ollama: 0.85 (good accuracy, lower latency) Transformers: 0.70 (limited knowledge) Selected: Gemini","title":"Example 1: General Knowledge Query"},{"location":"model-integration/backend-selection/#example-2-image-analysis-with-unavailable-primary","text":"For an image analysis request when Gemini is unavailable: Query: \"What's shown in this image?\" with attached image Selection process: Required capabilities: [\"image_understanding\"] Gemini scores highest but is unavailable Fallback scores: Ollama: 0.75 (limited image capabilities) Transformers: 0.60 (basic image understanding) Selected: Ollama as fallback","title":"Example 2: Image Analysis with Unavailable Primary"},{"location":"model-integration/backend-selection/#example-3-complex-code-analysis","text":"For a complex programming question: Query: \"Explain this Python decorator pattern and suggest improvements\" with code snippet Selection process: Required capabilities: [\"code_understanding\", \"python_expertise\"] Scores: Gemini: 0.92 (strong code understanding) Ollama: 0.88 (good code capabilities with CodeLlama) Transformers: 0.65 (limited code analysis) If optimizing for cost: Ollama might be selected despite slightly lower score If optimizing for quality: Gemini would be selected","title":"Example 3: Complex Code Analysis"},{"location":"model-integration/backend-selection/#configuration-options","text":"The backend selection system offers several configuration options: backend_selection : # Scoring weights weights : capability : 0.6 # Capability matching importance performance : 0.3 # Performance considerations cost : 0.1 # Cost optimization (if applicable) # Selection preferences preferences : prefer_local : false # Prefer local models when capabilities are similar cost_sensitive : false # Increase cost factor importance latency_threshold : 500 # Max acceptable latency in ms # Fallback configuration fallbacks : retry_original : true # Whether to retry original backend after timeout fallback_chain : gemini : \"ollama\" ollama : \"transformers\" transformers : \"gemini\" max_fallbacks : 2 # Maximum number of fallbacks to attempt # Feature flags features : dynamic_scoring : true # Adjust scores based on historical performance context_aware : true # Consider conversation context in selection adaptive_routing : true # Learn from successful/failed selections","title":"Configuration Options"},{"location":"model-integration/backend-selection/#monitoring-and-telemetry","text":"The backend selection system collects detailed telemetry to improve decision quality: def record_selection_telemetry ( request , selected_backend , success , duration_ms ): \"\"\"Record telemetry data for backend selection\"\"\" telemetry = { \"timestamp\" : time . time (), \"request_type\" : categorize_request ( request ), \"required_capabilities\" : analyze_request ( request ), \"selected_backend\" : selected_backend . name , \"success\" : success , \"duration_ms\" : duration_ms , \"fallback_used\" : selected_backend . is_fallback , \"scores\" : backend_scores # Scores from selection process } # Save telemetry telemetry_store . add ( telemetry ) # Update metrics for monitoring if success : metrics . increment ( f \"backend. { selected_backend . name } .success\" ) else : metrics . increment ( f \"backend. { selected_backend . name } .failure\" ) metrics . record ( f \"backend. { selected_backend . name } .duration\" , duration_ms ) This telemetry data enables: Optimization of scoring weights Identification of backend strengths and weaknesses Detection of patterns in backend performance Continuous improvement of selection accuracy","title":"Monitoring and Telemetry"},{"location":"model-integration/backend-selection/#related-documentation","text":"Model Integration Architecture: Overall architecture of the model integration system Capability Matrix: Detailed mapping of model capabilities Provider Integration: Implementation details for each provider Model Configuration: Configuration options for model backends Service Availability: How service availability is monitored","title":"Related Documentation"},{"location":"model-integration/capability-matrix/","text":"Capability Matrix \u00b6 Overview \u00b6 The Capability Matrix is a foundational component of Opossum Search's model integration system that maps specific AI capabilities to available model backends. This matrix enables the dynamic selection of the most appropriate model for each request based on required capabilities, creating an intelligent routing system that optimizes for both performance and quality. Purpose and Function \u00b6 The Capability Matrix serves several critical functions: Capability Mapping : Documents which capabilities each backend supports Quality Scoring : Rates each backend's proficiency for specific capabilities Selection Support : Provides data for the backend selection algorithm Gap Analysis : Identifies capability gaps across the system Version Tracking : Tracks capability changes across model versions Core Structure \u00b6 The Capability Matrix is implemented as a multi-dimensional mapping: graph LR A[Capability Matrix] --> B{Backend} B --> C[Gemini] B --> D[Ollama] B --> E[Transformers] C --> F{Capabilities} D --> F E --> F F --> G[text_understanding] F --> H[image_understanding] F --> I[code_generation] F --> J[domain_knowledge] G --> K{Scoring} H --> K I --> K J --> K K --> L[Proficiency Score] K --> M[Confidence Level] Matrix Implementation \u00b6 The Capability Matrix is implemented as a nested dictionary structure: class CapabilityMatrix : \"\"\"Maps AI capabilities to model backends with quality scores\"\"\" def __init__ ( self ): # Initialize the base matrix structure self . matrix = { 'gemini' : self . _init_gemini_capabilities (), 'ollama' : self . _init_ollama_capabilities (), 'transformers' : self . _init_transformers_capabilities () } def _init_gemini_capabilities ( self ): \"\"\"Initialize Gemini capabilities\"\"\" return { # General capabilities 'text_understanding' : 0.95 , 'text_generation' : 0.94 , 'conversation' : 0.92 , 'general_knowledge' : 0.93 , # Specialized capabilities 'image_understanding' : 0.90 , 'code_understanding' : 0.85 , 'code_generation' : 0.82 , 'reasoning' : 0.88 , 'math' : 0.75 , 'creative_writing' : 0.88 , # Domain knowledge 'science' : 0.86 , 'medicine' : 0.80 , 'finance' : 0.82 , 'legal' : 0.77 , 'current_events' : 0.70 , # Limited by training cutoff # Languages 'multilingual' : 0.85 , 'translation' : 0.83 , # Advanced features 'multimodal' : 0.88 , 'long_context' : 0.80 , 'structured_output' : 0.85 } def _init_ollama_capabilities ( self ): \"\"\"Initialize Ollama capabilities\"\"\" return { # General capabilities 'text_understanding' : 0.85 , 'text_generation' : 0.87 , 'conversation' : 0.83 , 'general_knowledge' : 0.84 , # Specialized capabilities 'image_understanding' : 0.60 , # Limited in most models 'code_understanding' : 0.82 , # Good with CodeLlama 'code_generation' : 0.80 , # Good with CodeLlama 'reasoning' : 0.80 , 'math' : 0.70 , 'creative_writing' : 0.81 , # Domain knowledge 'science' : 0.75 , 'medicine' : 0.65 , 'finance' : 0.70 , 'legal' : 0.65 , 'current_events' : 0.20 , # Very limited by training data # Languages 'multilingual' : 0.70 , 'translation' : 0.65 , # Advanced features 'multimodal' : 0.30 , # Very limited 'long_context' : 0.75 , # Varies by model 'structured_output' : 0.70 } def _init_transformers_capabilities ( self ): \"\"\"Initialize Transformers capabilities\"\"\" return { # General capabilities 'text_understanding' : 0.75 , 'text_generation' : 0.72 , 'conversation' : 0.65 , 'general_knowledge' : 0.70 , # Specialized capabilities 'image_understanding' : 0.50 , # With appropriate models 'code_understanding' : 0.65 , 'code_generation' : 0.60 , 'reasoning' : 0.60 , 'math' : 0.55 , 'creative_writing' : 0.65 , # Domain knowledge 'science' : 0.60 , 'medicine' : 0.50 , 'finance' : 0.55 , 'legal' : 0.50 , 'current_events' : 0.10 , # Extremely limited # Languages 'multilingual' : 0.60 , 'translation' : 0.65 , # With specialized models # Advanced features 'multimodal' : 0.20 , # Very limited 'long_context' : 0.40 , # Very limited 'structured_output' : 0.50 } def get_score ( self , backend , capability ): \"\"\"Get the capability score for a specific backend\"\"\" if backend not in self . matrix : return 0.0 return self . matrix [ backend ] . get ( capability , 0.0 ) def get_capabilities ( self , backend ): \"\"\"Get all capabilities supported by a backend\"\"\" if backend not in self . matrix : return [] # Return capabilities with non-zero scores return [ cap for cap , score in self . matrix [ backend ] . items () if score > 0.0 ] def get_best_backend_for_capability ( self , capability ): \"\"\"Get the best backend for a specific capability\"\"\" best_score = 0.0 best_backend = None for backend , capabilities in self . matrix . items (): score = capabilities . get ( capability , 0.0 ) if score > best_score : best_score = score best_backend = backend return best_backend , best_score def score_backend ( self , backend_name , requirements , query = None ): \"\"\"Score a backend based on capability requirements\"\"\" if backend_name not in self . matrix : return 0.0 total_score = 0.0 capabilities = self . matrix [ backend_name ] for requirement in requirements : # Get score for this capability score = capabilities . get ( requirement , 0.0 ) # Apply weighting based on importance weight = self . _get_requirement_weight ( requirement ) total_score += score * weight return total_score / len ( requirements ) def _get_requirement_weight ( self , requirement ): \"\"\"Get weight for a requirement based on importance\"\"\" # Critical capabilities have higher weights critical_capabilities = { 'image_understanding' : 1.5 , # Critical for image queries 'code_generation' : 1.3 , # Important for coding tasks 'current_events' : 1.2 , # Important for news queries 'long_context' : 1.2 # Important for context-heavy tasks } return critical_capabilities . get ( requirement , 1.0 ) Capability Categories \u00b6 The matrix organizes capabilities into several categories: General Capabilities \u00b6 Capability Description Importance text_understanding Comprehension of natural language Critical text_generation Production of coherent text Critical conversation Multi-turn dialogue capabilities High general_knowledge Broad factual knowledge High Specialized Capabilities \u00b6 Capability Description Importance image_understanding Visual content analysis Critical for visual tasks code_understanding Source code comprehension Critical for code tasks code_generation Source code production Critical for code tasks reasoning Logical inference and problem-solving High math Mathematical computation and analysis Medium creative_writing Generation of creative content Medium Domain Knowledge \u00b6 Capability Description Importance science Scientific knowledge and reasoning Medium medicine Medical and health information Medium finance Financial and economic knowledge Medium legal Legal concepts and reasoning Medium current_events Recent world events Medium Language Capabilities \u00b6 Capability Description Importance multilingual Support for multiple languages Medium translation Language translation Medium Advanced Features \u00b6 Capability Description Importance multimodal Processing multiple input types High long_context Handling extended context windows High structured_output Producing structured data formats Medium Complete Capability Matrix \u00b6 The following table shows the complete capability scoring across all backends: Capability Gemini Ollama Transformers General Capabilities text_understanding 0.95 0.85 0.75 text_generation 0.94 0.87 0.72 conversation 0.92 0.83 0.65 general_knowledge 0.93 0.84 0.70 Specialized Capabilities image_understanding 0.90 0.60 0.50 code_understanding 0.85 0.82 0.65 code_generation 0.82 0.80 0.60 reasoning 0.88 0.80 0.60 math 0.75 0.70 0.55 creative_writing 0.88 0.81 0.65 Domain Knowledge science 0.86 0.75 0.60 medicine 0.80 0.65 0.50 finance 0.82 0.70 0.55 legal 0.77 0.65 0.50 current_events 0.70 0.20 0.10 Languages multilingual 0.85 0.70 0.60 translation 0.83 0.65 0.65 Advanced Features multimodal 0.88 0.30 0.20 long_context 0.80 0.75 0.40 structured_output 0.85 0.70 0.50 Model-Specific Capabilities \u00b6 Some capabilities are specific to particular model configurations: Ollama-Specific Capabilities \u00b6 Ollama capabilities vary by the specific model loaded: def get_ollama_model_specific_capabilities ( model_name ): \"\"\"Get capability adjustments for specific Ollama models\"\"\" # Model-specific capability adjustments adjustments = { 'llama2' : { 'reasoning' : + 0.05 , 'math' : + 0.05 }, 'codellama' : { 'code_understanding' : + 0.10 , 'code_generation' : + 0.15 }, 'mistral' : { 'text_understanding' : + 0.05 , 'reasoning' : + 0.07 }, 'vicuna' : { 'conversation' : + 0.08 , 'creative_writing' : + 0.05 }, 'orca-mini' : { 'instruction_following' : + 0.10 } } return adjustments . get ( model_name , {}) Usage in Backend Selection \u00b6 The capability matrix is a key input to the backend selection process: def select_optimal_backend ( request , available_backends , capability_matrix ): \"\"\"Select the optimal backend based on request capabilities\"\"\" # Extract required capabilities from request required_capabilities = analyze_request ( request ) # Score each available backend scores = {} for backend_name in available_backends : # Calculate base capability score capability_score = 0 for capability in required_capabilities : score = capability_matrix . get_score ( backend_name , capability ) weight = get_capability_weight ( capability ) capability_score += score * weight # Normalize score if required_capabilities : capability_score /= len ( required_capabilities ) # Apply other factors (performance, cost, etc.) adjusted_score = apply_scoring_factors ( backend_name , capability_score , request ) scores [ backend_name ] = adjusted_score # Select highest scoring backend if not scores : return DEFAULT_BACKEND return max ( scores . items (), key = lambda x : x [ 1 ])[ 0 ] Capability Detection from Queries \u00b6 The system detects required capabilities from queries: def detect_capabilities_from_query ( query ): \"\"\"Detect required capabilities from a query\"\"\" capabilities = [ 'text_understanding' ] # Base capability # Check for image content if contains_images ( query ): capabilities . append ( 'image_understanding' ) # Detect specific image types if detect_chart ( query . images [ 0 ]): capabilities . append ( 'chart_analysis' ) if detect_document ( query . images [ 0 ]): capabilities . append ( 'document_analysis' ) # Check for code content if contains_code ( query ): capabilities . append ( 'code_understanding' ) # Detect programming language language = detect_language ( query ) if language : capabilities . append ( f ' { language } _expertise' ) # Detect domain-specific needs domain = detect_domain ( query ) if domain : capabilities . append ( f ' { domain } _knowledge' ) # Check for specific tasks if is_creative_task ( query ): capabilities . append ( 'creative_writing' ) if is_reasoning_task ( query ): capabilities . append ( 'reasoning' ) if is_math_task ( query ): capabilities . append ( 'math' ) return capabilities Extending the Capability Matrix \u00b6 The capability matrix can be extended with new capabilities: def extend_capability_matrix ( matrix , new_capabilities ): \"\"\"Add new capabilities to the capability matrix\"\"\" for capability_name , capability_info in new_capabilities . items (): # Add to each backend with default scores for backend in matrix . matrix : default_score = capability_info . get ( 'default_scores' , {}) . get ( backend , 0.0 ) matrix . matrix [ backend ][ capability_name ] = default_score return matrix Dynamic Capability Adjustments \u00b6 The system can dynamically adjust capability scores based on performance feedback: def adjust_capability_scores ( matrix , feedback_data ): \"\"\"Adjust capability scores based on performance feedback\"\"\" for item in feedback_data : backend = item [ 'backend' ] capability = item [ 'capability' ] success = item [ 'success' ] if backend not in matrix . matrix : continue if capability not in matrix . matrix [ backend ]: continue # Get current score current_score = matrix . matrix [ backend ][ capability ] # Adjust score based on success/failure adjustment = 0.01 if success else - 0.02 new_score = max ( 0.0 , min ( 1.0 , current_score + adjustment )) # Update score matrix . matrix [ backend ][ capability ] = new_score return matrix Configuration \u00b6 The capability matrix can be configured through settings: capability_matrix : # Enable dynamic adjustments based on performance enable_dynamic_adjustment : true # Minimum score threshold for capability support minimum_score_threshold : 0.2 # Custom capability weights capability_weights : image_understanding : 1.5 code_generation : 1.3 reasoning : 1.2 math : 1.1 # Override default scores score_overrides : gemini : math : 0.85 # Override math score for Gemini ollama : code_understanding : 0.90 # Override for CodeLlama model Related Documentation \u00b6 Model Integration Architecture Backend Selection Provider Integration Model Configuration Service Availability","title":"Capability Matrix"},{"location":"model-integration/capability-matrix/#capability-matrix","text":"","title":"Capability Matrix"},{"location":"model-integration/capability-matrix/#overview","text":"The Capability Matrix is a foundational component of Opossum Search's model integration system that maps specific AI capabilities to available model backends. This matrix enables the dynamic selection of the most appropriate model for each request based on required capabilities, creating an intelligent routing system that optimizes for both performance and quality.","title":"Overview"},{"location":"model-integration/capability-matrix/#purpose-and-function","text":"The Capability Matrix serves several critical functions: Capability Mapping : Documents which capabilities each backend supports Quality Scoring : Rates each backend's proficiency for specific capabilities Selection Support : Provides data for the backend selection algorithm Gap Analysis : Identifies capability gaps across the system Version Tracking : Tracks capability changes across model versions","title":"Purpose and Function"},{"location":"model-integration/capability-matrix/#core-structure","text":"The Capability Matrix is implemented as a multi-dimensional mapping: graph LR A[Capability Matrix] --> B{Backend} B --> C[Gemini] B --> D[Ollama] B --> E[Transformers] C --> F{Capabilities} D --> F E --> F F --> G[text_understanding] F --> H[image_understanding] F --> I[code_generation] F --> J[domain_knowledge] G --> K{Scoring} H --> K I --> K J --> K K --> L[Proficiency Score] K --> M[Confidence Level]","title":"Core Structure"},{"location":"model-integration/capability-matrix/#matrix-implementation","text":"The Capability Matrix is implemented as a nested dictionary structure: class CapabilityMatrix : \"\"\"Maps AI capabilities to model backends with quality scores\"\"\" def __init__ ( self ): # Initialize the base matrix structure self . matrix = { 'gemini' : self . _init_gemini_capabilities (), 'ollama' : self . _init_ollama_capabilities (), 'transformers' : self . _init_transformers_capabilities () } def _init_gemini_capabilities ( self ): \"\"\"Initialize Gemini capabilities\"\"\" return { # General capabilities 'text_understanding' : 0.95 , 'text_generation' : 0.94 , 'conversation' : 0.92 , 'general_knowledge' : 0.93 , # Specialized capabilities 'image_understanding' : 0.90 , 'code_understanding' : 0.85 , 'code_generation' : 0.82 , 'reasoning' : 0.88 , 'math' : 0.75 , 'creative_writing' : 0.88 , # Domain knowledge 'science' : 0.86 , 'medicine' : 0.80 , 'finance' : 0.82 , 'legal' : 0.77 , 'current_events' : 0.70 , # Limited by training cutoff # Languages 'multilingual' : 0.85 , 'translation' : 0.83 , # Advanced features 'multimodal' : 0.88 , 'long_context' : 0.80 , 'structured_output' : 0.85 } def _init_ollama_capabilities ( self ): \"\"\"Initialize Ollama capabilities\"\"\" return { # General capabilities 'text_understanding' : 0.85 , 'text_generation' : 0.87 , 'conversation' : 0.83 , 'general_knowledge' : 0.84 , # Specialized capabilities 'image_understanding' : 0.60 , # Limited in most models 'code_understanding' : 0.82 , # Good with CodeLlama 'code_generation' : 0.80 , # Good with CodeLlama 'reasoning' : 0.80 , 'math' : 0.70 , 'creative_writing' : 0.81 , # Domain knowledge 'science' : 0.75 , 'medicine' : 0.65 , 'finance' : 0.70 , 'legal' : 0.65 , 'current_events' : 0.20 , # Very limited by training data # Languages 'multilingual' : 0.70 , 'translation' : 0.65 , # Advanced features 'multimodal' : 0.30 , # Very limited 'long_context' : 0.75 , # Varies by model 'structured_output' : 0.70 } def _init_transformers_capabilities ( self ): \"\"\"Initialize Transformers capabilities\"\"\" return { # General capabilities 'text_understanding' : 0.75 , 'text_generation' : 0.72 , 'conversation' : 0.65 , 'general_knowledge' : 0.70 , # Specialized capabilities 'image_understanding' : 0.50 , # With appropriate models 'code_understanding' : 0.65 , 'code_generation' : 0.60 , 'reasoning' : 0.60 , 'math' : 0.55 , 'creative_writing' : 0.65 , # Domain knowledge 'science' : 0.60 , 'medicine' : 0.50 , 'finance' : 0.55 , 'legal' : 0.50 , 'current_events' : 0.10 , # Extremely limited # Languages 'multilingual' : 0.60 , 'translation' : 0.65 , # With specialized models # Advanced features 'multimodal' : 0.20 , # Very limited 'long_context' : 0.40 , # Very limited 'structured_output' : 0.50 } def get_score ( self , backend , capability ): \"\"\"Get the capability score for a specific backend\"\"\" if backend not in self . matrix : return 0.0 return self . matrix [ backend ] . get ( capability , 0.0 ) def get_capabilities ( self , backend ): \"\"\"Get all capabilities supported by a backend\"\"\" if backend not in self . matrix : return [] # Return capabilities with non-zero scores return [ cap for cap , score in self . matrix [ backend ] . items () if score > 0.0 ] def get_best_backend_for_capability ( self , capability ): \"\"\"Get the best backend for a specific capability\"\"\" best_score = 0.0 best_backend = None for backend , capabilities in self . matrix . items (): score = capabilities . get ( capability , 0.0 ) if score > best_score : best_score = score best_backend = backend return best_backend , best_score def score_backend ( self , backend_name , requirements , query = None ): \"\"\"Score a backend based on capability requirements\"\"\" if backend_name not in self . matrix : return 0.0 total_score = 0.0 capabilities = self . matrix [ backend_name ] for requirement in requirements : # Get score for this capability score = capabilities . get ( requirement , 0.0 ) # Apply weighting based on importance weight = self . _get_requirement_weight ( requirement ) total_score += score * weight return total_score / len ( requirements ) def _get_requirement_weight ( self , requirement ): \"\"\"Get weight for a requirement based on importance\"\"\" # Critical capabilities have higher weights critical_capabilities = { 'image_understanding' : 1.5 , # Critical for image queries 'code_generation' : 1.3 , # Important for coding tasks 'current_events' : 1.2 , # Important for news queries 'long_context' : 1.2 # Important for context-heavy tasks } return critical_capabilities . get ( requirement , 1.0 )","title":"Matrix Implementation"},{"location":"model-integration/capability-matrix/#capability-categories","text":"The matrix organizes capabilities into several categories:","title":"Capability Categories"},{"location":"model-integration/capability-matrix/#general-capabilities","text":"Capability Description Importance text_understanding Comprehension of natural language Critical text_generation Production of coherent text Critical conversation Multi-turn dialogue capabilities High general_knowledge Broad factual knowledge High","title":"General Capabilities"},{"location":"model-integration/capability-matrix/#specialized-capabilities","text":"Capability Description Importance image_understanding Visual content analysis Critical for visual tasks code_understanding Source code comprehension Critical for code tasks code_generation Source code production Critical for code tasks reasoning Logical inference and problem-solving High math Mathematical computation and analysis Medium creative_writing Generation of creative content Medium","title":"Specialized Capabilities"},{"location":"model-integration/capability-matrix/#domain-knowledge","text":"Capability Description Importance science Scientific knowledge and reasoning Medium medicine Medical and health information Medium finance Financial and economic knowledge Medium legal Legal concepts and reasoning Medium current_events Recent world events Medium","title":"Domain Knowledge"},{"location":"model-integration/capability-matrix/#language-capabilities","text":"Capability Description Importance multilingual Support for multiple languages Medium translation Language translation Medium","title":"Language Capabilities"},{"location":"model-integration/capability-matrix/#advanced-features","text":"Capability Description Importance multimodal Processing multiple input types High long_context Handling extended context windows High structured_output Producing structured data formats Medium","title":"Advanced Features"},{"location":"model-integration/capability-matrix/#complete-capability-matrix","text":"The following table shows the complete capability scoring across all backends: Capability Gemini Ollama Transformers General Capabilities text_understanding 0.95 0.85 0.75 text_generation 0.94 0.87 0.72 conversation 0.92 0.83 0.65 general_knowledge 0.93 0.84 0.70 Specialized Capabilities image_understanding 0.90 0.60 0.50 code_understanding 0.85 0.82 0.65 code_generation 0.82 0.80 0.60 reasoning 0.88 0.80 0.60 math 0.75 0.70 0.55 creative_writing 0.88 0.81 0.65 Domain Knowledge science 0.86 0.75 0.60 medicine 0.80 0.65 0.50 finance 0.82 0.70 0.55 legal 0.77 0.65 0.50 current_events 0.70 0.20 0.10 Languages multilingual 0.85 0.70 0.60 translation 0.83 0.65 0.65 Advanced Features multimodal 0.88 0.30 0.20 long_context 0.80 0.75 0.40 structured_output 0.85 0.70 0.50","title":"Complete Capability Matrix"},{"location":"model-integration/capability-matrix/#model-specific-capabilities","text":"Some capabilities are specific to particular model configurations:","title":"Model-Specific Capabilities"},{"location":"model-integration/capability-matrix/#ollama-specific-capabilities","text":"Ollama capabilities vary by the specific model loaded: def get_ollama_model_specific_capabilities ( model_name ): \"\"\"Get capability adjustments for specific Ollama models\"\"\" # Model-specific capability adjustments adjustments = { 'llama2' : { 'reasoning' : + 0.05 , 'math' : + 0.05 }, 'codellama' : { 'code_understanding' : + 0.10 , 'code_generation' : + 0.15 }, 'mistral' : { 'text_understanding' : + 0.05 , 'reasoning' : + 0.07 }, 'vicuna' : { 'conversation' : + 0.08 , 'creative_writing' : + 0.05 }, 'orca-mini' : { 'instruction_following' : + 0.10 } } return adjustments . get ( model_name , {})","title":"Ollama-Specific Capabilities"},{"location":"model-integration/capability-matrix/#usage-in-backend-selection","text":"The capability matrix is a key input to the backend selection process: def select_optimal_backend ( request , available_backends , capability_matrix ): \"\"\"Select the optimal backend based on request capabilities\"\"\" # Extract required capabilities from request required_capabilities = analyze_request ( request ) # Score each available backend scores = {} for backend_name in available_backends : # Calculate base capability score capability_score = 0 for capability in required_capabilities : score = capability_matrix . get_score ( backend_name , capability ) weight = get_capability_weight ( capability ) capability_score += score * weight # Normalize score if required_capabilities : capability_score /= len ( required_capabilities ) # Apply other factors (performance, cost, etc.) adjusted_score = apply_scoring_factors ( backend_name , capability_score , request ) scores [ backend_name ] = adjusted_score # Select highest scoring backend if not scores : return DEFAULT_BACKEND return max ( scores . items (), key = lambda x : x [ 1 ])[ 0 ]","title":"Usage in Backend Selection"},{"location":"model-integration/capability-matrix/#capability-detection-from-queries","text":"The system detects required capabilities from queries: def detect_capabilities_from_query ( query ): \"\"\"Detect required capabilities from a query\"\"\" capabilities = [ 'text_understanding' ] # Base capability # Check for image content if contains_images ( query ): capabilities . append ( 'image_understanding' ) # Detect specific image types if detect_chart ( query . images [ 0 ]): capabilities . append ( 'chart_analysis' ) if detect_document ( query . images [ 0 ]): capabilities . append ( 'document_analysis' ) # Check for code content if contains_code ( query ): capabilities . append ( 'code_understanding' ) # Detect programming language language = detect_language ( query ) if language : capabilities . append ( f ' { language } _expertise' ) # Detect domain-specific needs domain = detect_domain ( query ) if domain : capabilities . append ( f ' { domain } _knowledge' ) # Check for specific tasks if is_creative_task ( query ): capabilities . append ( 'creative_writing' ) if is_reasoning_task ( query ): capabilities . append ( 'reasoning' ) if is_math_task ( query ): capabilities . append ( 'math' ) return capabilities","title":"Capability Detection from Queries"},{"location":"model-integration/capability-matrix/#extending-the-capability-matrix","text":"The capability matrix can be extended with new capabilities: def extend_capability_matrix ( matrix , new_capabilities ): \"\"\"Add new capabilities to the capability matrix\"\"\" for capability_name , capability_info in new_capabilities . items (): # Add to each backend with default scores for backend in matrix . matrix : default_score = capability_info . get ( 'default_scores' , {}) . get ( backend , 0.0 ) matrix . matrix [ backend ][ capability_name ] = default_score return matrix","title":"Extending the Capability Matrix"},{"location":"model-integration/capability-matrix/#dynamic-capability-adjustments","text":"The system can dynamically adjust capability scores based on performance feedback: def adjust_capability_scores ( matrix , feedback_data ): \"\"\"Adjust capability scores based on performance feedback\"\"\" for item in feedback_data : backend = item [ 'backend' ] capability = item [ 'capability' ] success = item [ 'success' ] if backend not in matrix . matrix : continue if capability not in matrix . matrix [ backend ]: continue # Get current score current_score = matrix . matrix [ backend ][ capability ] # Adjust score based on success/failure adjustment = 0.01 if success else - 0.02 new_score = max ( 0.0 , min ( 1.0 , current_score + adjustment )) # Update score matrix . matrix [ backend ][ capability ] = new_score return matrix","title":"Dynamic Capability Adjustments"},{"location":"model-integration/capability-matrix/#configuration","text":"The capability matrix can be configured through settings: capability_matrix : # Enable dynamic adjustments based on performance enable_dynamic_adjustment : true # Minimum score threshold for capability support minimum_score_threshold : 0.2 # Custom capability weights capability_weights : image_understanding : 1.5 code_generation : 1.3 reasoning : 1.2 math : 1.1 # Override default scores score_overrides : gemini : math : 0.85 # Override math score for Gemini ollama : code_understanding : 0.90 # Override for CodeLlama model","title":"Configuration"},{"location":"model-integration/capability-matrix/#related-documentation","text":"Model Integration Architecture Backend Selection Provider Integration Model Configuration Service Availability","title":"Related Documentation"},{"location":"model-integration/configuration/","text":"Model Configuration \u00b6 Overview \u00b6 The Opossum Search model integration system provides a flexible configuration framework that allows fine-grained control over model behavior, performance characteristics, and fallback mechanisms. This document details the configuration options available for each model provider and explains how to optimize settings for different deployment scenarios. Configuration Structure \u00b6 Model configuration follows a hierarchical structure: model_integration : # Global settings default_backend : \"gemini\" enable_fallbacks : true request_timeout : 15 # Backend-specific configurations backends : gemini : # Gemini-specific settings ollama : # Ollama-specific settings transformers : # Transformers-specific settings # Feature flags and behavior settings features : # Feature-specific settings Global Configuration Options \u00b6 These settings apply to the entire model integration system: Option Description Default Valid Values default_backend Primary backend to use when no specific selection is made \"gemini\" \"gemini\" , \"ollama\" , \"transformers\" enable_fallbacks Whether to use fallback backends when primary fails true true , false request_timeout Global request timeout in seconds 15 Positive integer max_retries Maximum number of retries for transient errors 2 Non-negative integer telemetry_enabled Whether to collect usage telemetry true true , false cache_responses Whether to cache responses true true , false cache_ttl Time-to-live for cached responses in seconds 3600 Positive integer Provider-Specific Configuration \u00b6 Gemini Configuration \u00b6 gemini : api_key : \"${GEMINI_API_KEY}\" # Environment variable reference model : \"gemini-pro\" temperature : 0.7 top_p : 0.95 top_k : 40 max_tokens : 800 timeout : 10 max_retries : 2 safety_settings : harassment : \"block_only_high\" hate_speech : \"block_medium_and_above\" sexually_explicit : \"block_high\" dangerous_content : \"block_medium_and_above\" quota_management : max_daily_requests : 5000 max_tokens_per_minute : 100000 alert_at_percentage : 80 Gemini Configuration Options \u00b6 Option Description Default Valid Values api_key Gemini API key Required Valid API key string model Gemini model to use \"gemini-pro\" \"gemini-pro\" , \"gemini-pro-vision\" temperature Randomness in generation 0.7 0.0 to 1.0 top_p Nucleus sampling parameter 0.95 0.0 to 1.0 top_k Number of top tokens to consider 40 Positive integer max_tokens Maximum tokens to generate 800 Positive integer timeout Request timeout in seconds 10 Positive integer safety_settings Content filtering settings (default values) See safety levels quota_management Settings to manage API quota (default values) See quota options Ollama Configuration \u00b6 ollama : host : \"localhost\" port : 11434 timeout : 15 models : - name : \"llama2\" default : true temperature : 0.7 context_window : 4096 - name : \"codellama\" temperature : 0.6 context_window : 8192 - name : \"mistral\" temperature : 0.8 context_window : 8192 concurrency : 2 startup_timeout : 60 health_check_interval : 300 Ollama Configuration Options \u00b6 Option Description Default Valid Values host Ollama server hostname \"localhost\" Valid hostname or IP port Ollama server port 11434 Valid port number timeout Request timeout in seconds 15 Positive integer models List of models to use [{\"name\": \"llama2\", \"default\": true}] Array of model configs concurrency Maximum concurrent requests 2 Positive integer startup_timeout Time to wait for server startup 60 Positive integer health_check_interval Time between health checks in seconds 300 Positive integer For each model in the models array: Option Description Default Valid Values name Model name Required Valid Ollama model name default Whether this is the default model false true , false temperature Generation temperature 0.7 0.0 to 1.0 context_window Context window size Model-dependent Positive integer repeat_penalty Penalty for repeated tokens 1.1 Positive float top_p Nucleus sampling parameter 0.9 0.0 to 1.0 Transformers Configuration \u00b6 transformers : model_path : \"./models/local\" default_model : \"microsoft/phi-2\" device : \"auto\" # \"auto\", \"cpu\", \"cuda\", \"mps\" quantization : \"int8\" # \"none\", \"int8\", \"int4\" models : - name : \"microsoft/phi-2\" default : true - name : \"TheBloke/Llama-2-7B-GGUF\" file : \"llama-2-7b.Q4_K_M.gguf\" quantization : \"int4\" - name : \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" max_memory : \"4GB\" low_memory_mode : false cpu_threads : 4 generate_params : max_new_tokens : 512 temperature : 0.7 top_p : 0.9 top_k : 50 repetition_penalty : 1.1 Transformers Configuration Options \u00b6 Option Description Default Valid Values model_path Path to store model files \"./models/local\" Valid directory path default_model Default model identifier \"microsoft/phi-2\" Valid model ID or name device Compute device to use \"auto\" \"auto\" , \"cpu\" , \"cuda\" , \"mps\" quantization Quantization level \"int8\" \"none\" , \"int8\" , \"int4\" models List of models to use (default values) Array of model configs max_memory Maximum memory allocation \"4GB\" Memory size string low_memory_mode Optimize for low memory false true , false cpu_threads CPU threads for computation 4 Positive integer generate_params Generation parameters (default values) See generation options For each model in the models array: Option Description Default Valid Values name Model identifier or HF repo Required Valid model ID or name default Whether this is the default model false true , false file Specific model file for GGUF models Model-dependent Valid filename quantization Model-specific quantization Global setting \"none\" , \"int8\" , \"int4\" Feature Configuration \u00b6 features : dynamic_model_selection : enabled : true capability_weight : 0.6 performance_weight : 0.3 cost_weight : 0.1 multimodal : enabled : true preferred_backend : \"gemini\" fallback_backend : \"transformers\" context_management : max_history_items : 10 include_system_messages : true telemetry : enabled : true sampling_rate : 0.1 detailed_metrics : false Environment Variables Integration \u00b6 The configuration system supports environment variable substitution: gemini : api_key : \"${GEMINI_API_KEY}\" ollama : host : \"${OLLAMA_HOST:-localhost}\" port : \"${OLLAMA_PORT:-11434}\" Environment variables can have default values using the :- syntax. Configuration Validation \u00b6 The system validates configuration at startup: def validate_config ( config ): \"\"\"Validate model integration configuration\"\"\" errors = [] warnings = [] # Check for required settings if 'backends' not in config : errors . append ( \"Missing 'backends' section in configuration\" ) # Validate default_backend default_backend = config . get ( 'default_backend' ) if default_backend and default_backend not in config . get ( 'backends' , {}): errors . append ( f \"Default backend ' { default_backend } ' not defined in backends section\" ) # Validate Gemini settings if present if 'gemini' in config . get ( 'backends' , {}): gemini_config = config [ 'backends' ][ 'gemini' ] if not gemini_config . get ( 'api_key' ): errors . append ( \"Missing Gemini API key\" ) # Report validation results if errors : raise ConfigurationError ( f \"Configuration validation failed: { ', ' . join ( errors ) } \" ) if warnings : for warning in warnings : logger . warning ( f \"Configuration warning: { warning } \" ) return True Dynamic Configuration \u00b6 The system supports runtime configuration updates: def update_model_config ( config_updates ): \"\"\"Update model configuration at runtime\"\"\" # Get current configuration current_config = get_model_config () # Apply updates (deep merge) updated_config = deep_merge ( current_config , config_updates ) # Validate updated configuration validate_config ( updated_config ) # Apply new configuration set_model_config ( updated_config ) # Notify components of configuration change event_bus . publish ( \"config.updated\" , { \"component\" : \"model_integration\" , \"old_config\" : current_config , \"new_config\" : updated_config }) return updated_config Configuration Loading \u00b6 The system loads configuration from multiple sources: def load_model_config (): \"\"\"Load model configuration from multiple sources\"\"\" # Start with default configuration config = default_model_config () # Load from configuration file file_config = load_config_file ( \"config/model_integration.yaml\" ) config = deep_merge ( config , file_config ) # Load from environment variables env_config = load_from_environment ( \"MODEL_\" ) config = deep_merge ( config , env_config ) # Process environment variable references config = process_env_vars ( config ) # Validate configuration validate_config ( config ) return config Complete Configuration Example \u00b6 model_integration : default_backend : \"gemini\" enable_fallbacks : true request_timeout : 15 max_retries : 2 telemetry_enabled : true cache_responses : true cache_ttl : 3600 backends : gemini : api_key : \"${GEMINI_API_KEY}\" model : \"gemini-pro\" temperature : 0.7 top_p : 0.95 top_k : 40 max_tokens : 800 timeout : 10 safety_settings : harassment : \"block_only_high\" hate_speech : \"block_medium_and_above\" sexually_explicit : \"block_high\" dangerous_content : \"block_medium_and_above\" quota_management : max_daily_requests : 5000 max_tokens_per_minute : 100000 alert_at_percentage : 80 ollama : host : \"${OLLAMA_HOST:-localhost}\" port : \"${OLLAMA_PORT:-11434}\" timeout : 15 models : - name : \"llama2\" default : true temperature : 0.7 context_window : 4096 - name : \"codellama\" temperature : 0.6 context_window : 8192 - name : \"mistral\" temperature : 0.8 context_window : 8192 concurrency : 2 startup_timeout : 60 health_check_interval : 300 transformers : model_path : \"./models/local\" default_model : \"microsoft/phi-2\" device : \"auto\" quantization : \"int8\" models : - name : \"microsoft/phi-2\" default : true - name : \"TheBloke/Llama-2-7B-GGUF\" file : \"llama-2-7b.Q4_K_M.gguf\" quantization : \"int4\" - name : \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" max_memory : \"4GB\" low_memory_mode : false cpu_threads : 4 generate_params : max_new_tokens : 512 temperature : 0.7 top_p : 0.9 top_k : 50 repetition_penalty : 1.1 features : dynamic_model_selection : enabled : true capability_weight : 0.6 performance_weight : 0.3 cost_weight : 0.1 multimodal : enabled : true preferred_backend : \"gemini\" fallback_backend : \"transformers\" context_management : max_history_items : 10 include_system_messages : true telemetry : enabled : true sampling_rate : 0.1 detailed_metrics : false Configuration Best Practices \u00b6 General Guidelines \u00b6 Use Environment Variables for Secrets Always use environment variables for API keys and sensitive information Provide clear documentation on required environment variables Provide Sensible Defaults Configure default values that work in most scenarios Document the reasoning behind default values Layer Configuration Use a layered approach to configuration (defaults \u2192 files \u2192 environment) Allow selective overrides without replacing entire configuration Validate Configuration Perform thorough validation at startup Provide clear error messages for configuration issues Model-Specific Recommendations \u00b6 Gemini \u00b6 Configure appropriate safety_settings based on your application's use case Set up quota_management to prevent unexpected billing Use a lower temperature (0.3-0.5) for factual responses, higher (0.7-0.9) for creative content Ollama \u00b6 Configure concurrency based on your hardware capabilities Use specialized models for specific tasks (CodeLlama for code, Mistral for general text) Configure health_check_interval based on your stability requirements Transformers \u00b6 Use appropriate quantization based on available memory Configure device correctly for your hardware Use the smallest model that meets your quality requirements Deployment Scenarios \u00b6 Low-Resource Environment \u00b6 model_integration : default_backend : \"transformers\" enable_fallbacks : true backends : transformers : device : \"cpu\" quantization : \"int4\" default_model : \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" low_memory_mode : true generate_params : max_new_tokens : 256 ollama : models : - name : \"tinyllama\" default : true concurrency : 1 # No Gemini configuration in low-resource scenario features : dynamic_model_selection : enabled : false # Simplify for performance High-Performance Environment \u00b6 model_integration : default_backend : \"gemini\" backends : gemini : max_tokens : 1500 ollama : host : \"ollama-server\" # Dedicated server concurrency : 4 models : - name : \"llama2:70b\" default : true transformers : device : \"cuda\" quantization : \"none\" # Full precision for quality default_model : \"meta-llama/Llama-2-13b-chat-hf\" models : - name : \"meta-llama/Llama-2-13b-chat-hf\" default : true High-Availability Environment \u00b6 model_integration : default_backend : \"gemini\" enable_fallbacks : true request_timeout : 8 # Lower timeout for faster fallback max_retries : 1 # Fewer retries, faster fallback backends : gemini : api_key : \"${GEMINI_API_KEY}\" timeout : 5 ollama : host : \"${OLLAMA_HOST}\" models : - name : \"llama2\" default : true startup_timeout : 30 health_check_interval : 60 # More frequent health checks transformers : default_model : \"microsoft/phi-2\" device : \"cuda\" features : dynamic_model_selection : enabled : true # Weight availability higher capability_weight : 0.4 performance_weight : 0.5 cost_weight : 0.1 Related Documentation \u00b6 Model Integration Architecture Backend Selection Capability Matrix Provider Integration Infrastructure Configuration Management","title":"Configuration"},{"location":"model-integration/configuration/#model-configuration","text":"","title":"Model Configuration"},{"location":"model-integration/configuration/#overview","text":"The Opossum Search model integration system provides a flexible configuration framework that allows fine-grained control over model behavior, performance characteristics, and fallback mechanisms. This document details the configuration options available for each model provider and explains how to optimize settings for different deployment scenarios.","title":"Overview"},{"location":"model-integration/configuration/#configuration-structure","text":"Model configuration follows a hierarchical structure: model_integration : # Global settings default_backend : \"gemini\" enable_fallbacks : true request_timeout : 15 # Backend-specific configurations backends : gemini : # Gemini-specific settings ollama : # Ollama-specific settings transformers : # Transformers-specific settings # Feature flags and behavior settings features : # Feature-specific settings","title":"Configuration Structure"},{"location":"model-integration/configuration/#global-configuration-options","text":"These settings apply to the entire model integration system: Option Description Default Valid Values default_backend Primary backend to use when no specific selection is made \"gemini\" \"gemini\" , \"ollama\" , \"transformers\" enable_fallbacks Whether to use fallback backends when primary fails true true , false request_timeout Global request timeout in seconds 15 Positive integer max_retries Maximum number of retries for transient errors 2 Non-negative integer telemetry_enabled Whether to collect usage telemetry true true , false cache_responses Whether to cache responses true true , false cache_ttl Time-to-live for cached responses in seconds 3600 Positive integer","title":"Global Configuration Options"},{"location":"model-integration/configuration/#provider-specific-configuration","text":"","title":"Provider-Specific Configuration"},{"location":"model-integration/configuration/#gemini-configuration","text":"gemini : api_key : \"${GEMINI_API_KEY}\" # Environment variable reference model : \"gemini-pro\" temperature : 0.7 top_p : 0.95 top_k : 40 max_tokens : 800 timeout : 10 max_retries : 2 safety_settings : harassment : \"block_only_high\" hate_speech : \"block_medium_and_above\" sexually_explicit : \"block_high\" dangerous_content : \"block_medium_and_above\" quota_management : max_daily_requests : 5000 max_tokens_per_minute : 100000 alert_at_percentage : 80","title":"Gemini Configuration"},{"location":"model-integration/configuration/#gemini-configuration-options","text":"Option Description Default Valid Values api_key Gemini API key Required Valid API key string model Gemini model to use \"gemini-pro\" \"gemini-pro\" , \"gemini-pro-vision\" temperature Randomness in generation 0.7 0.0 to 1.0 top_p Nucleus sampling parameter 0.95 0.0 to 1.0 top_k Number of top tokens to consider 40 Positive integer max_tokens Maximum tokens to generate 800 Positive integer timeout Request timeout in seconds 10 Positive integer safety_settings Content filtering settings (default values) See safety levels quota_management Settings to manage API quota (default values) See quota options","title":"Gemini Configuration Options"},{"location":"model-integration/configuration/#ollama-configuration","text":"ollama : host : \"localhost\" port : 11434 timeout : 15 models : - name : \"llama2\" default : true temperature : 0.7 context_window : 4096 - name : \"codellama\" temperature : 0.6 context_window : 8192 - name : \"mistral\" temperature : 0.8 context_window : 8192 concurrency : 2 startup_timeout : 60 health_check_interval : 300","title":"Ollama Configuration"},{"location":"model-integration/configuration/#ollama-configuration-options","text":"Option Description Default Valid Values host Ollama server hostname \"localhost\" Valid hostname or IP port Ollama server port 11434 Valid port number timeout Request timeout in seconds 15 Positive integer models List of models to use [{\"name\": \"llama2\", \"default\": true}] Array of model configs concurrency Maximum concurrent requests 2 Positive integer startup_timeout Time to wait for server startup 60 Positive integer health_check_interval Time between health checks in seconds 300 Positive integer For each model in the models array: Option Description Default Valid Values name Model name Required Valid Ollama model name default Whether this is the default model false true , false temperature Generation temperature 0.7 0.0 to 1.0 context_window Context window size Model-dependent Positive integer repeat_penalty Penalty for repeated tokens 1.1 Positive float top_p Nucleus sampling parameter 0.9 0.0 to 1.0","title":"Ollama Configuration Options"},{"location":"model-integration/configuration/#transformers-configuration","text":"transformers : model_path : \"./models/local\" default_model : \"microsoft/phi-2\" device : \"auto\" # \"auto\", \"cpu\", \"cuda\", \"mps\" quantization : \"int8\" # \"none\", \"int8\", \"int4\" models : - name : \"microsoft/phi-2\" default : true - name : \"TheBloke/Llama-2-7B-GGUF\" file : \"llama-2-7b.Q4_K_M.gguf\" quantization : \"int4\" - name : \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" max_memory : \"4GB\" low_memory_mode : false cpu_threads : 4 generate_params : max_new_tokens : 512 temperature : 0.7 top_p : 0.9 top_k : 50 repetition_penalty : 1.1","title":"Transformers Configuration"},{"location":"model-integration/configuration/#transformers-configuration-options","text":"Option Description Default Valid Values model_path Path to store model files \"./models/local\" Valid directory path default_model Default model identifier \"microsoft/phi-2\" Valid model ID or name device Compute device to use \"auto\" \"auto\" , \"cpu\" , \"cuda\" , \"mps\" quantization Quantization level \"int8\" \"none\" , \"int8\" , \"int4\" models List of models to use (default values) Array of model configs max_memory Maximum memory allocation \"4GB\" Memory size string low_memory_mode Optimize for low memory false true , false cpu_threads CPU threads for computation 4 Positive integer generate_params Generation parameters (default values) See generation options For each model in the models array: Option Description Default Valid Values name Model identifier or HF repo Required Valid model ID or name default Whether this is the default model false true , false file Specific model file for GGUF models Model-dependent Valid filename quantization Model-specific quantization Global setting \"none\" , \"int8\" , \"int4\"","title":"Transformers Configuration Options"},{"location":"model-integration/configuration/#feature-configuration","text":"features : dynamic_model_selection : enabled : true capability_weight : 0.6 performance_weight : 0.3 cost_weight : 0.1 multimodal : enabled : true preferred_backend : \"gemini\" fallback_backend : \"transformers\" context_management : max_history_items : 10 include_system_messages : true telemetry : enabled : true sampling_rate : 0.1 detailed_metrics : false","title":"Feature Configuration"},{"location":"model-integration/configuration/#environment-variables-integration","text":"The configuration system supports environment variable substitution: gemini : api_key : \"${GEMINI_API_KEY}\" ollama : host : \"${OLLAMA_HOST:-localhost}\" port : \"${OLLAMA_PORT:-11434}\" Environment variables can have default values using the :- syntax.","title":"Environment Variables Integration"},{"location":"model-integration/configuration/#configuration-validation","text":"The system validates configuration at startup: def validate_config ( config ): \"\"\"Validate model integration configuration\"\"\" errors = [] warnings = [] # Check for required settings if 'backends' not in config : errors . append ( \"Missing 'backends' section in configuration\" ) # Validate default_backend default_backend = config . get ( 'default_backend' ) if default_backend and default_backend not in config . get ( 'backends' , {}): errors . append ( f \"Default backend ' { default_backend } ' not defined in backends section\" ) # Validate Gemini settings if present if 'gemini' in config . get ( 'backends' , {}): gemini_config = config [ 'backends' ][ 'gemini' ] if not gemini_config . get ( 'api_key' ): errors . append ( \"Missing Gemini API key\" ) # Report validation results if errors : raise ConfigurationError ( f \"Configuration validation failed: { ', ' . join ( errors ) } \" ) if warnings : for warning in warnings : logger . warning ( f \"Configuration warning: { warning } \" ) return True","title":"Configuration Validation"},{"location":"model-integration/configuration/#dynamic-configuration","text":"The system supports runtime configuration updates: def update_model_config ( config_updates ): \"\"\"Update model configuration at runtime\"\"\" # Get current configuration current_config = get_model_config () # Apply updates (deep merge) updated_config = deep_merge ( current_config , config_updates ) # Validate updated configuration validate_config ( updated_config ) # Apply new configuration set_model_config ( updated_config ) # Notify components of configuration change event_bus . publish ( \"config.updated\" , { \"component\" : \"model_integration\" , \"old_config\" : current_config , \"new_config\" : updated_config }) return updated_config","title":"Dynamic Configuration"},{"location":"model-integration/configuration/#configuration-loading","text":"The system loads configuration from multiple sources: def load_model_config (): \"\"\"Load model configuration from multiple sources\"\"\" # Start with default configuration config = default_model_config () # Load from configuration file file_config = load_config_file ( \"config/model_integration.yaml\" ) config = deep_merge ( config , file_config ) # Load from environment variables env_config = load_from_environment ( \"MODEL_\" ) config = deep_merge ( config , env_config ) # Process environment variable references config = process_env_vars ( config ) # Validate configuration validate_config ( config ) return config","title":"Configuration Loading"},{"location":"model-integration/configuration/#complete-configuration-example","text":"model_integration : default_backend : \"gemini\" enable_fallbacks : true request_timeout : 15 max_retries : 2 telemetry_enabled : true cache_responses : true cache_ttl : 3600 backends : gemini : api_key : \"${GEMINI_API_KEY}\" model : \"gemini-pro\" temperature : 0.7 top_p : 0.95 top_k : 40 max_tokens : 800 timeout : 10 safety_settings : harassment : \"block_only_high\" hate_speech : \"block_medium_and_above\" sexually_explicit : \"block_high\" dangerous_content : \"block_medium_and_above\" quota_management : max_daily_requests : 5000 max_tokens_per_minute : 100000 alert_at_percentage : 80 ollama : host : \"${OLLAMA_HOST:-localhost}\" port : \"${OLLAMA_PORT:-11434}\" timeout : 15 models : - name : \"llama2\" default : true temperature : 0.7 context_window : 4096 - name : \"codellama\" temperature : 0.6 context_window : 8192 - name : \"mistral\" temperature : 0.8 context_window : 8192 concurrency : 2 startup_timeout : 60 health_check_interval : 300 transformers : model_path : \"./models/local\" default_model : \"microsoft/phi-2\" device : \"auto\" quantization : \"int8\" models : - name : \"microsoft/phi-2\" default : true - name : \"TheBloke/Llama-2-7B-GGUF\" file : \"llama-2-7b.Q4_K_M.gguf\" quantization : \"int4\" - name : \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" max_memory : \"4GB\" low_memory_mode : false cpu_threads : 4 generate_params : max_new_tokens : 512 temperature : 0.7 top_p : 0.9 top_k : 50 repetition_penalty : 1.1 features : dynamic_model_selection : enabled : true capability_weight : 0.6 performance_weight : 0.3 cost_weight : 0.1 multimodal : enabled : true preferred_backend : \"gemini\" fallback_backend : \"transformers\" context_management : max_history_items : 10 include_system_messages : true telemetry : enabled : true sampling_rate : 0.1 detailed_metrics : false","title":"Complete Configuration Example"},{"location":"model-integration/configuration/#configuration-best-practices","text":"","title":"Configuration Best Practices"},{"location":"model-integration/configuration/#general-guidelines","text":"Use Environment Variables for Secrets Always use environment variables for API keys and sensitive information Provide clear documentation on required environment variables Provide Sensible Defaults Configure default values that work in most scenarios Document the reasoning behind default values Layer Configuration Use a layered approach to configuration (defaults \u2192 files \u2192 environment) Allow selective overrides without replacing entire configuration Validate Configuration Perform thorough validation at startup Provide clear error messages for configuration issues","title":"General Guidelines"},{"location":"model-integration/configuration/#model-specific-recommendations","text":"","title":"Model-Specific Recommendations"},{"location":"model-integration/configuration/#gemini","text":"Configure appropriate safety_settings based on your application's use case Set up quota_management to prevent unexpected billing Use a lower temperature (0.3-0.5) for factual responses, higher (0.7-0.9) for creative content","title":"Gemini"},{"location":"model-integration/configuration/#ollama","text":"Configure concurrency based on your hardware capabilities Use specialized models for specific tasks (CodeLlama for code, Mistral for general text) Configure health_check_interval based on your stability requirements","title":"Ollama"},{"location":"model-integration/configuration/#transformers","text":"Use appropriate quantization based on available memory Configure device correctly for your hardware Use the smallest model that meets your quality requirements","title":"Transformers"},{"location":"model-integration/configuration/#deployment-scenarios","text":"","title":"Deployment Scenarios"},{"location":"model-integration/configuration/#low-resource-environment","text":"model_integration : default_backend : \"transformers\" enable_fallbacks : true backends : transformers : device : \"cpu\" quantization : \"int4\" default_model : \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" low_memory_mode : true generate_params : max_new_tokens : 256 ollama : models : - name : \"tinyllama\" default : true concurrency : 1 # No Gemini configuration in low-resource scenario features : dynamic_model_selection : enabled : false # Simplify for performance","title":"Low-Resource Environment"},{"location":"model-integration/configuration/#high-performance-environment","text":"model_integration : default_backend : \"gemini\" backends : gemini : max_tokens : 1500 ollama : host : \"ollama-server\" # Dedicated server concurrency : 4 models : - name : \"llama2:70b\" default : true transformers : device : \"cuda\" quantization : \"none\" # Full precision for quality default_model : \"meta-llama/Llama-2-13b-chat-hf\" models : - name : \"meta-llama/Llama-2-13b-chat-hf\" default : true","title":"High-Performance Environment"},{"location":"model-integration/configuration/#high-availability-environment","text":"model_integration : default_backend : \"gemini\" enable_fallbacks : true request_timeout : 8 # Lower timeout for faster fallback max_retries : 1 # Fewer retries, faster fallback backends : gemini : api_key : \"${GEMINI_API_KEY}\" timeout : 5 ollama : host : \"${OLLAMA_HOST}\" models : - name : \"llama2\" default : true startup_timeout : 30 health_check_interval : 60 # More frequent health checks transformers : default_model : \"microsoft/phi-2\" device : \"cuda\" features : dynamic_model_selection : enabled : true # Weight availability higher capability_weight : 0.4 performance_weight : 0.5 cost_weight : 0.1","title":"High-Availability Environment"},{"location":"model-integration/configuration/#related-documentation","text":"Model Integration Architecture Backend Selection Capability Matrix Provider Integration Infrastructure Configuration Management","title":"Related Documentation"},{"location":"model-integration/dspy-examples/","text":"DSPy Usage Examples \u00b6 Basic DSPy Pipeline \u00b6 This example demonstrates creating a basic DSPy pipeline for multi-step reasoning: # Example: Step-by-step reasoning with DSPy import dspy from app.integrations.dspy.manager import DSPyManager # Initialize DSPy dspy_manager = DSPyManager () # Define a signature for question-answering class QuestionAnswerer ( dspy . Signature ): \"\"\"Answer questions with detailed reasoning.\"\"\" question = dspy . InputField () reasoning = dspy . OutputField ( desc = \"Step-by-step reasoning process\" ) answer = dspy . OutputField ( desc = \"Final concise answer\" ) # Create a chain-of-thought module reasoner = dspy . ChainOfThought ( QuestionAnswerer ) # Use the module response = reasoner ( question = \"What characteristics make opossums adaptable?\" ) print ( f \"Reasoning: { response . reasoning } \" ) print ( f \"Answer: { response . answer } \" ) Optimizing Opossum's Prompts \u00b6 This example shows how to optimize existing prompts from the YAML configuration: # Example: Optimizing an existing prompt template from app.integrations.dspy.prompt_adapter import PromptOptimizer from app.integrations.dspy.manager import DSPyManager # Initialize components dspy_manager = DSPyManager () optimizer = PromptOptimizer ( dspy_manager ) # Optimize a specific prompt template optimized_prompt = optimizer . optimize ( template_name = \"conversation.system_prompt\" , context = \"This prompt is for general conversation with users interested in adaptable systems\" ) print ( \"Optimized Prompt:\" ) print ( optimized_prompt ) # Save the optimized prompt for future use from app.prompts.store import save_optimized_prompt save_optimized_prompt ( \"conversation.system_prompt\" , optimized_prompt ) Enhanced Chat2SVG Pipeline \u00b6 This example demonstrates using DSPy to enhance the Chat2SVG pipeline: # Example: Using the DSPy-enhanced Chat2SVG pipeline from app.models.chat2svg.dspy_pipeline import DSPyEnhancedPipeline # Initialize enhanced pipeline pipeline = DSPyEnhancedPipeline () # Process a request svg_result = pipeline . process ( description = \"A diagram showing an opossum's adaptable nature, with branches for different environments\" , style = \"minimalist\" , format = \"svg\" ) # Use the result print ( f \"SVG Result (first 100 chars): { svg_result [: 100 ] } ...\" ) Teleprompter for Automatic Optimization \u00b6 This example shows using DSPy's teleprompter for automatic prompt optimization: # Example: Using Teleprompter for automatic optimization import dspy from app.integrations.dspy.manager import DSPyManager from app.data.examples import load_examples # Initialize DSPy dspy_manager = DSPyManager () # Load training examples examples = load_examples ( \"service_visualization\" ) # Define the task signature class SVGGenerator ( dspy . Signature ): \"\"\"Generate SVG code based on a description.\"\"\" description = dspy . InputField () svg_code = dspy . OutputField () # Create a basic module basic_generator = dspy . Predict ( SVGGenerator ) # Optimize with teleprompter teleprompter = dspy . Teleprompter ( basic_generator ) optimized_generator = teleprompter . optimize ( examples ) # Use the optimized module result = optimized_generator ( description = \"A pie chart showing Opossum Search's backend usage distribution\" ) print ( result . svg_code ) Multistage Reasoning with Feedback \u00b6 This example demonstrates complex multi-stage reasoning with feedback: # Example: Multi-stage reasoning with feedback import dspy class PlanGenerator ( dspy . Signature ): \"\"\"Generate a resilience plan.\"\"\" scenario = dspy . InputField () plan = dspy . OutputField () class PlanValidator ( dspy . Signature ): \"\"\"Validate a resilience plan.\"\"\" scenario = dspy . InputField () plan = dspy . InputField () issues = dspy . OutputField () class PlanImprover ( dspy . Signature ): \"\"\"Improve a plan based on identified issues.\"\"\" scenario = dspy . InputField () plan = dspy . InputField () issues = dspy . InputField () improved_plan = dspy . OutputField () # Create a multi-stage pipeline def resilience_pipeline ( scenario ): # Stage 1: Generate initial plan generator = dspy . Predict ( PlanGenerator ) result = generator ( scenario = scenario ) initial_plan = result . plan # Stage 2: Validate the plan validator = dspy . Predict ( PlanValidator ) validation = validator ( scenario = scenario , plan = initial_plan ) issues = validation . issues # Stage 3: Improve the plan if issues exist if issues and issues . strip () != \"No issues found.\" : improver = dspy . Predict ( PlanImprover ) improved = improver ( scenario = scenario , plan = initial_plan , issues = issues ) return improved . improved_plan return initial_plan # Use the pipeline scenario = \"A service with fluctuating user demand and occasional API outages\" final_plan = resilience_pipeline ( scenario ) print ( final_plan ) \u23f1\ufe0f 10m - Setting up DSPy integration \u23f1\ufe0f 15m - Optimizing existing prompts \u23f1\ufe0f 20m - Enhancing Chat2SVG pipeline \u23f1\ufe0f 25m - Implementing advanced reasoning chains Temporal Markers \u00b6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Last updated: 2025-04-11 \u2502 \u2502 Estimated reading time: 15 minutes \u2502 \u2502 Documentation heartbeat: 0 days since last validation \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Related Documentation \u00b6 DSPy Integration Overview DSPy Technical Implementation DSPy Metrics & Performance Chat2SVG Pipeline","title":"DSPy Usage Examples"},{"location":"model-integration/dspy-examples/#dspy-usage-examples","text":"","title":"DSPy Usage Examples"},{"location":"model-integration/dspy-examples/#basic-dspy-pipeline","text":"This example demonstrates creating a basic DSPy pipeline for multi-step reasoning: # Example: Step-by-step reasoning with DSPy import dspy from app.integrations.dspy.manager import DSPyManager # Initialize DSPy dspy_manager = DSPyManager () # Define a signature for question-answering class QuestionAnswerer ( dspy . Signature ): \"\"\"Answer questions with detailed reasoning.\"\"\" question = dspy . InputField () reasoning = dspy . OutputField ( desc = \"Step-by-step reasoning process\" ) answer = dspy . OutputField ( desc = \"Final concise answer\" ) # Create a chain-of-thought module reasoner = dspy . ChainOfThought ( QuestionAnswerer ) # Use the module response = reasoner ( question = \"What characteristics make opossums adaptable?\" ) print ( f \"Reasoning: { response . reasoning } \" ) print ( f \"Answer: { response . answer } \" )","title":"Basic DSPy Pipeline"},{"location":"model-integration/dspy-examples/#optimizing-opossums-prompts","text":"This example shows how to optimize existing prompts from the YAML configuration: # Example: Optimizing an existing prompt template from app.integrations.dspy.prompt_adapter import PromptOptimizer from app.integrations.dspy.manager import DSPyManager # Initialize components dspy_manager = DSPyManager () optimizer = PromptOptimizer ( dspy_manager ) # Optimize a specific prompt template optimized_prompt = optimizer . optimize ( template_name = \"conversation.system_prompt\" , context = \"This prompt is for general conversation with users interested in adaptable systems\" ) print ( \"Optimized Prompt:\" ) print ( optimized_prompt ) # Save the optimized prompt for future use from app.prompts.store import save_optimized_prompt save_optimized_prompt ( \"conversation.system_prompt\" , optimized_prompt )","title":"Optimizing Opossum's Prompts"},{"location":"model-integration/dspy-examples/#enhanced-chat2svg-pipeline","text":"This example demonstrates using DSPy to enhance the Chat2SVG pipeline: # Example: Using the DSPy-enhanced Chat2SVG pipeline from app.models.chat2svg.dspy_pipeline import DSPyEnhancedPipeline # Initialize enhanced pipeline pipeline = DSPyEnhancedPipeline () # Process a request svg_result = pipeline . process ( description = \"A diagram showing an opossum's adaptable nature, with branches for different environments\" , style = \"minimalist\" , format = \"svg\" ) # Use the result print ( f \"SVG Result (first 100 chars): { svg_result [: 100 ] } ...\" )","title":"Enhanced Chat2SVG Pipeline"},{"location":"model-integration/dspy-examples/#teleprompter-for-automatic-optimization","text":"This example shows using DSPy's teleprompter for automatic prompt optimization: # Example: Using Teleprompter for automatic optimization import dspy from app.integrations.dspy.manager import DSPyManager from app.data.examples import load_examples # Initialize DSPy dspy_manager = DSPyManager () # Load training examples examples = load_examples ( \"service_visualization\" ) # Define the task signature class SVGGenerator ( dspy . Signature ): \"\"\"Generate SVG code based on a description.\"\"\" description = dspy . InputField () svg_code = dspy . OutputField () # Create a basic module basic_generator = dspy . Predict ( SVGGenerator ) # Optimize with teleprompter teleprompter = dspy . Teleprompter ( basic_generator ) optimized_generator = teleprompter . optimize ( examples ) # Use the optimized module result = optimized_generator ( description = \"A pie chart showing Opossum Search's backend usage distribution\" ) print ( result . svg_code )","title":"Teleprompter for Automatic Optimization"},{"location":"model-integration/dspy-examples/#multistage-reasoning-with-feedback","text":"This example demonstrates complex multi-stage reasoning with feedback: # Example: Multi-stage reasoning with feedback import dspy class PlanGenerator ( dspy . Signature ): \"\"\"Generate a resilience plan.\"\"\" scenario = dspy . InputField () plan = dspy . OutputField () class PlanValidator ( dspy . Signature ): \"\"\"Validate a resilience plan.\"\"\" scenario = dspy . InputField () plan = dspy . InputField () issues = dspy . OutputField () class PlanImprover ( dspy . Signature ): \"\"\"Improve a plan based on identified issues.\"\"\" scenario = dspy . InputField () plan = dspy . InputField () issues = dspy . InputField () improved_plan = dspy . OutputField () # Create a multi-stage pipeline def resilience_pipeline ( scenario ): # Stage 1: Generate initial plan generator = dspy . Predict ( PlanGenerator ) result = generator ( scenario = scenario ) initial_plan = result . plan # Stage 2: Validate the plan validator = dspy . Predict ( PlanValidator ) validation = validator ( scenario = scenario , plan = initial_plan ) issues = validation . issues # Stage 3: Improve the plan if issues exist if issues and issues . strip () != \"No issues found.\" : improver = dspy . Predict ( PlanImprover ) improved = improver ( scenario = scenario , plan = initial_plan , issues = issues ) return improved . improved_plan return initial_plan # Use the pipeline scenario = \"A service with fluctuating user demand and occasional API outages\" final_plan = resilience_pipeline ( scenario ) print ( final_plan ) \u23f1\ufe0f 10m - Setting up DSPy integration \u23f1\ufe0f 15m - Optimizing existing prompts \u23f1\ufe0f 20m - Enhancing Chat2SVG pipeline \u23f1\ufe0f 25m - Implementing advanced reasoning chains","title":"Multistage Reasoning with Feedback"},{"location":"model-integration/dspy-examples/#temporal-markers","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Last updated: 2025-04-11 \u2502 \u2502 Estimated reading time: 15 minutes \u2502 \u2502 Documentation heartbeat: 0 days since last validation \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Temporal Markers"},{"location":"model-integration/dspy-examples/#related-documentation","text":"DSPy Integration Overview DSPy Technical Implementation DSPy Metrics & Performance Chat2SVG Pipeline","title":"Related Documentation"},{"location":"model-integration/dspy-integration/","text":"DSPy Integration \u00b6 Overview \u00b6 Opossum Search integrates DSPy, a framework for programming Language Models (LLMs), to enhance prompt optimization and create more robust multi-stage LLM pipelines. This integration follows Opossum's core principles of adaptability and resilience while adding powerful prompt engineering capabilities. Core Benefits \u00b6 Benefit Description Implementation Area Prompt Optimization Systematic approach to tuning and refining prompts app/prompts integration Pipeline Construction Building robust multi-step reasoning chains app/models/chat2svg pipeline enhancement Evaluation Framework Metrics-driven assessment of prompt effectiveness Monitoring and development tools Few-Shot Learning Structured approach to example management Extended capabilities for existing modules Provider Abstraction Common interface across multiple LLM backends Complements existing provider system Integration Architecture \u00b6 graph TD A[Opossum Request] --> B{Request Type} B -->|Simple Query| C[Direct LLM API] B -->|Complex Reasoning| D[DSPy Pipeline] B -->|SVG Generation| E[Chat2SVG with DSPy] C --> F[Response] D --> F E --> F subgraph \"DSPy Integration Layer\" D G[DSPy Configuration] H[DSPy Teleprompter] I[DSPy Modules] G --- D H --- D I --- D end J[Prompt Templates] --> K[DSPy Optimizer] K --> L[Optimized Prompts] L --> C L --> D classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,F primary; class B,C,D,E,J,L secondary; Implementation Approach \u00b6 Opossum Search implements DSPy integration with a clock-like precision: Consistent Timing : Optimization runs at development time, with results cached for runtime Reliable Mechanics : Clear separation between optimization and execution Precision Components : Each DSPy module has a single, well-defined responsibility Self-Regulating : Performance metrics drive continuous improvement Related Documentation \u00b6 DSPy Technical Implementation DSPy Usage Examples DSPy Metrics & Performance Prompt Management Chat2SVG Pipeline","title":"DSPy Integration"},{"location":"model-integration/dspy-integration/#dspy-integration","text":"","title":"DSPy Integration"},{"location":"model-integration/dspy-integration/#overview","text":"Opossum Search integrates DSPy, a framework for programming Language Models (LLMs), to enhance prompt optimization and create more robust multi-stage LLM pipelines. This integration follows Opossum's core principles of adaptability and resilience while adding powerful prompt engineering capabilities.","title":"Overview"},{"location":"model-integration/dspy-integration/#core-benefits","text":"Benefit Description Implementation Area Prompt Optimization Systematic approach to tuning and refining prompts app/prompts integration Pipeline Construction Building robust multi-step reasoning chains app/models/chat2svg pipeline enhancement Evaluation Framework Metrics-driven assessment of prompt effectiveness Monitoring and development tools Few-Shot Learning Structured approach to example management Extended capabilities for existing modules Provider Abstraction Common interface across multiple LLM backends Complements existing provider system","title":"Core Benefits"},{"location":"model-integration/dspy-integration/#integration-architecture","text":"graph TD A[Opossum Request] --> B{Request Type} B -->|Simple Query| C[Direct LLM API] B -->|Complex Reasoning| D[DSPy Pipeline] B -->|SVG Generation| E[Chat2SVG with DSPy] C --> F[Response] D --> F E --> F subgraph \"DSPy Integration Layer\" D G[DSPy Configuration] H[DSPy Teleprompter] I[DSPy Modules] G --- D H --- D I --- D end J[Prompt Templates] --> K[DSPy Optimizer] K --> L[Optimized Prompts] L --> C L --> D classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,F primary; class B,C,D,E,J,L secondary;","title":"Integration Architecture"},{"location":"model-integration/dspy-integration/#implementation-approach","text":"Opossum Search implements DSPy integration with a clock-like precision: Consistent Timing : Optimization runs at development time, with results cached for runtime Reliable Mechanics : Clear separation between optimization and execution Precision Components : Each DSPy module has a single, well-defined responsibility Self-Regulating : Performance metrics drive continuous improvement","title":"Implementation Approach"},{"location":"model-integration/dspy-integration/#related-documentation","text":"DSPy Technical Implementation DSPy Usage Examples DSPy Metrics & Performance Prompt Management Chat2SVG Pipeline","title":"Related Documentation"},{"location":"model-integration/dspy-metrics/","text":"DSPy Metrics & Performance \u00b6 Measuring DSPy Performance \u00b6 Opossum Search implements comprehensive metrics and monitoring for DSPy integration to ensure optimal performance, quality, and resource utilization. Key Metrics \u00b6 Metric Description Target Implementation Prompt Optimization Score Effectiveness improvement of optimized prompts >15% improvement A/B testing framework Optimization Time Time required to run DSPy optimizers <60s per prompt Performance tracking Memory Consumption RAM usage during optimization <2GB peak Resource monitoring Quality Improvement Enhanced response quality measured by evaluators >20% improvement Automated evaluation Token Efficiency Token reduction while maintaining quality >10% reduction Token counter Monitoring Implementation \u00b6 # app/integrations/dspy/metrics.py import time import psutil import statistics from prometheus_client import Counter , Gauge , Histogram # Define Prometheus metrics DSPY_OPTIMIZATION_TIME = Histogram ( \"opossum_dspy_optimization_time_seconds\" , \"Time taken for DSPy optimization in seconds\" , [ \"prompt_type\" , \"optimizer\" ] ) DSPY_MEMORY_USAGE = Gauge ( \"opossum_dspy_memory_usage_mb\" , \"Memory used during DSPy optimization in MB\" , [ \"optimizer\" ] ) DSPY_QUALITY_IMPROVEMENT = Gauge ( \"opossum_dspy_quality_improvement_percent\" , \"Quality improvement percentage from DSPy optimization\" , [ \"prompt_type\" ] ) DSPY_TOKEN_REDUCTION = Gauge ( \"opossum_dspy_token_reduction_percent\" , \"Token reduction percentage from DSPy optimization\" , [ \"prompt_type\" ] ) # Helper class for tracking DSPy performance class DSPyPerformanceTracker : def __init__ ( self , prompt_type , optimizer_name ): self . prompt_type = prompt_type self . optimizer_name = optimizer_name self . start_time = None self . start_memory = None def __enter__ ( self ): self . start_time = time . time () self . start_memory = psutil . Process () . memory_info () . rss / ( 1024 * 1024 ) return self def __exit__ ( self , exc_type , exc_val , exc_tb ): if exc_type is None : # Only track successful completions duration = time . time () - self . start_time peak_memory = psutil . Process () . memory_info () . rss / ( 1024 * 1024 ) memory_used = peak_memory - self . start_memory # Record metrics DSPY_OPTIMIZATION_TIME . labels ( prompt_type = self . prompt_type , optimizer = self . optimizer_name ) . observe ( duration ) DSPY_MEMORY_USAGE . labels ( optimizer = self . optimizer_name ) . set ( memory_used ) Performance Visualization \u00b6 Opossum Search's service visualization system includes dedicated dashboards for DSPy performance monitoring: graph TB A[Raw Data Collection] --> B[Metrics Aggregation] B --> C[Time-Series DB] C --> D[Visualization Layer] subgraph \"DSPy Metrics\" E[Optimization Performance] F[Quality Metrics] G[Resource Usage] H[Token Efficiency] end D --> E D --> F D --> G D --> H classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,D primary; class B,C,E,F,G,H secondary; Quality Evaluation \u00b6 To measure the quality improvements from DSPy, Opossum implements automatic evaluation: # app/integrations/dspy/evaluation.py import dspy class ResponseQualityEvaluator : \"\"\"Evaluate the quality of responses before and after DSPy optimization.\"\"\" def __init__ ( self ): self . evaluator = dspy . Predict ( instruction = \"Evaluate the quality of the given response based on accuracy, helpfulness, and clarity.\" , input_keys = [ \"question\" , \"response\" ], output_keys = [ \"score\" , \"reasoning\" ] ) def evaluate ( self , question , response ): \"\"\"Evaluate a response on a 0-100 scale.\"\"\" result = self . evaluator ( question = question , response = response ) return { \"score\" : float ( result . score ), \"reasoning\" : result . reasoning } def compare ( self , question , baseline_response , optimized_response ): \"\"\"Compare baseline and optimized responses.\"\"\" baseline_eval = self . evaluate ( question , baseline_response ) optimized_eval = self . evaluate ( question , optimized_response ) improvement = optimized_eval [ \"score\" ] - baseline_eval [ \"score\" ] percent_improvement = ( improvement / baseline_eval [ \"score\" ]) * 100 if baseline_eval [ \"score\" ] > 0 else 0 DSPY_QUALITY_IMPROVEMENT . labels ( prompt_type = self . _detect_prompt_type ( question ) ) . set ( percent_improvement ) return { \"baseline_score\" : baseline_eval [ \"score\" ], \"optimized_score\" : optimized_eval [ \"score\" ], \"absolute_improvement\" : improvement , \"percent_improvement\" : percent_improvement , \"baseline_reasoning\" : baseline_eval [ \"reasoning\" ], \"optimized_reasoning\" : optimized_eval [ \"reasoning\" ] } Resource Optimization \u00b6 DSPy integration is designed for efficient resource utilization: Caching Strategy : All optimization results are cached to avoid redundant processing Selective Application : DSPy optimization targets high-value prompts most likely to benefit Scheduled Execution : Heavy optimization runs during off-peak hours Progressive Implementation : Gradual rollout with careful monitoring of system impact Memory Management \u00b6 To prevent memory issues during optimization: # app/integrations/dspy/memory.py import gc import torch def optimize_memory_usage (): \"\"\"Optimize memory usage before/after DSPy operations.\"\"\" # Clear Python garbage collector gc . collect () # Clear CUDA cache if available if torch . cuda . is_available (): torch . cuda . empty_cache () # Return current memory usage for monitoring import psutil return { \"ram_percent\" : psutil . virtual_memory () . percent , \"ram_used_gb\" : psutil . virtual_memory () . used / ( 1024 ** 3 ), \"process_mb\" : psutil . Process () . memory_info () . rss / ( 1024 ** 2 ) } \u25d4 Section 1/4 Complete: Metrics Framework \u25d1 Section 2/4 Complete: Monitoring Implementation \u25d5 Section 3/4 Complete: Quality Evaluation \u25cf Section 4/4 Complete: Resource Optimization Temporal Markers \u00b6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Last updated: 2025-04-11 \u2502 \u2502 Estimated reading time: 10 minutes \u2502 \u2502 Documentation heartbeat: 0 days since last validation \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Related Documentation \u00b6 DSPy Integration Overview DSPy Technical Implementation DSPy Usage Examples OpenTelemetry Integration","title":"DSPy Metrics & Performance"},{"location":"model-integration/dspy-metrics/#dspy-metrics-performance","text":"","title":"DSPy Metrics &amp; Performance"},{"location":"model-integration/dspy-metrics/#measuring-dspy-performance","text":"Opossum Search implements comprehensive metrics and monitoring for DSPy integration to ensure optimal performance, quality, and resource utilization.","title":"Measuring DSPy Performance"},{"location":"model-integration/dspy-metrics/#key-metrics","text":"Metric Description Target Implementation Prompt Optimization Score Effectiveness improvement of optimized prompts >15% improvement A/B testing framework Optimization Time Time required to run DSPy optimizers <60s per prompt Performance tracking Memory Consumption RAM usage during optimization <2GB peak Resource monitoring Quality Improvement Enhanced response quality measured by evaluators >20% improvement Automated evaluation Token Efficiency Token reduction while maintaining quality >10% reduction Token counter","title":"Key Metrics"},{"location":"model-integration/dspy-metrics/#monitoring-implementation","text":"# app/integrations/dspy/metrics.py import time import psutil import statistics from prometheus_client import Counter , Gauge , Histogram # Define Prometheus metrics DSPY_OPTIMIZATION_TIME = Histogram ( \"opossum_dspy_optimization_time_seconds\" , \"Time taken for DSPy optimization in seconds\" , [ \"prompt_type\" , \"optimizer\" ] ) DSPY_MEMORY_USAGE = Gauge ( \"opossum_dspy_memory_usage_mb\" , \"Memory used during DSPy optimization in MB\" , [ \"optimizer\" ] ) DSPY_QUALITY_IMPROVEMENT = Gauge ( \"opossum_dspy_quality_improvement_percent\" , \"Quality improvement percentage from DSPy optimization\" , [ \"prompt_type\" ] ) DSPY_TOKEN_REDUCTION = Gauge ( \"opossum_dspy_token_reduction_percent\" , \"Token reduction percentage from DSPy optimization\" , [ \"prompt_type\" ] ) # Helper class for tracking DSPy performance class DSPyPerformanceTracker : def __init__ ( self , prompt_type , optimizer_name ): self . prompt_type = prompt_type self . optimizer_name = optimizer_name self . start_time = None self . start_memory = None def __enter__ ( self ): self . start_time = time . time () self . start_memory = psutil . Process () . memory_info () . rss / ( 1024 * 1024 ) return self def __exit__ ( self , exc_type , exc_val , exc_tb ): if exc_type is None : # Only track successful completions duration = time . time () - self . start_time peak_memory = psutil . Process () . memory_info () . rss / ( 1024 * 1024 ) memory_used = peak_memory - self . start_memory # Record metrics DSPY_OPTIMIZATION_TIME . labels ( prompt_type = self . prompt_type , optimizer = self . optimizer_name ) . observe ( duration ) DSPY_MEMORY_USAGE . labels ( optimizer = self . optimizer_name ) . set ( memory_used )","title":"Monitoring Implementation"},{"location":"model-integration/dspy-metrics/#performance-visualization","text":"Opossum Search's service visualization system includes dedicated dashboards for DSPy performance monitoring: graph TB A[Raw Data Collection] --> B[Metrics Aggregation] B --> C[Time-Series DB] C --> D[Visualization Layer] subgraph \"DSPy Metrics\" E[Optimization Performance] F[Quality Metrics] G[Resource Usage] H[Token Efficiency] end D --> E D --> F D --> G D --> H classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,D primary; class B,C,E,F,G,H secondary;","title":"Performance Visualization"},{"location":"model-integration/dspy-metrics/#quality-evaluation","text":"To measure the quality improvements from DSPy, Opossum implements automatic evaluation: # app/integrations/dspy/evaluation.py import dspy class ResponseQualityEvaluator : \"\"\"Evaluate the quality of responses before and after DSPy optimization.\"\"\" def __init__ ( self ): self . evaluator = dspy . Predict ( instruction = \"Evaluate the quality of the given response based on accuracy, helpfulness, and clarity.\" , input_keys = [ \"question\" , \"response\" ], output_keys = [ \"score\" , \"reasoning\" ] ) def evaluate ( self , question , response ): \"\"\"Evaluate a response on a 0-100 scale.\"\"\" result = self . evaluator ( question = question , response = response ) return { \"score\" : float ( result . score ), \"reasoning\" : result . reasoning } def compare ( self , question , baseline_response , optimized_response ): \"\"\"Compare baseline and optimized responses.\"\"\" baseline_eval = self . evaluate ( question , baseline_response ) optimized_eval = self . evaluate ( question , optimized_response ) improvement = optimized_eval [ \"score\" ] - baseline_eval [ \"score\" ] percent_improvement = ( improvement / baseline_eval [ \"score\" ]) * 100 if baseline_eval [ \"score\" ] > 0 else 0 DSPY_QUALITY_IMPROVEMENT . labels ( prompt_type = self . _detect_prompt_type ( question ) ) . set ( percent_improvement ) return { \"baseline_score\" : baseline_eval [ \"score\" ], \"optimized_score\" : optimized_eval [ \"score\" ], \"absolute_improvement\" : improvement , \"percent_improvement\" : percent_improvement , \"baseline_reasoning\" : baseline_eval [ \"reasoning\" ], \"optimized_reasoning\" : optimized_eval [ \"reasoning\" ] }","title":"Quality Evaluation"},{"location":"model-integration/dspy-metrics/#resource-optimization","text":"DSPy integration is designed for efficient resource utilization: Caching Strategy : All optimization results are cached to avoid redundant processing Selective Application : DSPy optimization targets high-value prompts most likely to benefit Scheduled Execution : Heavy optimization runs during off-peak hours Progressive Implementation : Gradual rollout with careful monitoring of system impact","title":"Resource Optimization"},{"location":"model-integration/dspy-metrics/#memory-management","text":"To prevent memory issues during optimization: # app/integrations/dspy/memory.py import gc import torch def optimize_memory_usage (): \"\"\"Optimize memory usage before/after DSPy operations.\"\"\" # Clear Python garbage collector gc . collect () # Clear CUDA cache if available if torch . cuda . is_available (): torch . cuda . empty_cache () # Return current memory usage for monitoring import psutil return { \"ram_percent\" : psutil . virtual_memory () . percent , \"ram_used_gb\" : psutil . virtual_memory () . used / ( 1024 ** 3 ), \"process_mb\" : psutil . Process () . memory_info () . rss / ( 1024 ** 2 ) } \u25d4 Section 1/4 Complete: Metrics Framework \u25d1 Section 2/4 Complete: Monitoring Implementation \u25d5 Section 3/4 Complete: Quality Evaluation \u25cf Section 4/4 Complete: Resource Optimization","title":"Memory Management"},{"location":"model-integration/dspy-metrics/#temporal-markers","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Last updated: 2025-04-11 \u2502 \u2502 Estimated reading time: 10 minutes \u2502 \u2502 Documentation heartbeat: 0 days since last validation \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Temporal Markers"},{"location":"model-integration/dspy-metrics/#related-documentation","text":"DSPy Integration Overview DSPy Technical Implementation DSPy Usage Examples OpenTelemetry Integration","title":"Related Documentation"},{"location":"model-integration/dspy-technical/","text":"DSPy Technical Implementation \u00b6 Installation & Configuration \u00b6 Opossum Search integrates DSPy as a standard dependency rather than a vendored component. This approach ensures easy updates while maintaining the reliability expected of production systems. Setup Process \u00b6 # Add DSPy to requirements.txt pip install -U \"dspy[anthropic]\" Core Configuration \u00b6 DSPy requires explicit configuration to work with your preferred LLM provider: # app/integrations/dspy/manager.py import dspy from app.config import Config class DSPyManager : \"\"\"Centralized DSPy configuration and management.\"\"\" def __init__ ( self ): # Load configuration self . provider = Config . get ( \"DSPY_PROVIDER\" , \"google\" ) self . model = Config . get ( \"DSPY_MODEL\" , \"gemini-pro\" ) # Configure DSPy self . lm = dspy . LM ( f \" { self . provider } / { self . model } \" ) dspy . configure ( lm = self . lm ) # Track configuration metadata self . config_timestamp = self . _get_timestamp () def _get_timestamp ( self ): \"\"\"Get current timestamp for configuration tracking.\"\"\" import datetime return datetime . datetime . now () . isoformat () Integration with Existing Components \u00b6 Prompt System Integration \u00b6 DSPy integrates with Opossum's existing YAML-based prompt system: # app/integrations/dspy/prompt_adapter.py import dspy from app.prompts.loader import get_prompt_template class PromptOptimizer : \"\"\"Optimize prompts using DSPy.\"\"\" def __init__ ( self , dspy_manager ): self . dspy_manager = dspy_manager self . optimizer = self . _create_optimizer () def _create_optimizer ( self ): \"\"\"Create a DSPy optimizer for prompts.\"\"\" return dspy . ChainOfThought ( dspy . Predict ( instruction = \"Optimize the given prompt template for clarity, specificity, and effectiveness.\" , input_keys = [ \"template\" , \"context\" ], output_keys = [ \"optimized_template\" ] )) def optimize ( self , template_name , context = \"\" ): \"\"\"Optimize a prompt template.\"\"\" template = get_prompt_template ( template_name ) result = self . optimizer ( template = template , context = context ) return result . optimized_template Chat2SVG Pipeline Enhancement \u00b6 Enhance the Chat2SVG pipeline with DSPy modules: # app/models/chat2svg/dspy_pipeline.py import dspy from app.models.chat2svg.pipeline import Pipeline class DSPyEnhancedPipeline ( Pipeline ): \"\"\"Chat2SVG pipeline enhanced with DSPy capabilities.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . setup_dspy_modules () def setup_dspy_modules ( self ): \"\"\"Initialize DSPy modules for pipeline stages.\"\"\" # Template generation module self . template_generator = dspy . ChainOfThought ( dspy . Predict ( instruction = \"Generate SVG template based on the description.\" , input_keys = [ \"description\" ], output_keys = [ \"svg_template\" ] )) # Detail enhancement module self . detail_enhancer = dspy . ChainOfThought ( dspy . Predict ( instruction = \"Enhance SVG with detailed elements.\" , input_keys = [ \"svg_template\" , \"description\" ], output_keys = [ \"enhanced_svg\" ] )) # Optimizer module self . svg_optimizer = dspy . Predict ( instruction = \"Optimize SVG for rendering performance while maintaining quality.\" , input_keys = [ \"enhanced_svg\" ], output_keys = [ \"optimized_svg\" ] ) Performance Considerations \u00b6 DSPy adds computational overhead during development but creates optimized artifacts that can be cached for production use: Development-Time Optimization : Run DSPy optimization during development or CI/CD pipeline Production-Time Execution : Use optimized artifacts in production for maximum performance Caching Strategy : Cache optimized prompts and signatures for reuse Resource Management : Monitor memory and CPU usage when running optimizers CHECKPOINT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2022 DSPy installation uses pip rather than vendoring \u2022 Configuration happens in centralized manager class \u2022 Integration adapters bridge to existing systems \u2022 Pipeline enhancements preserve original architecture \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Temporal Markers \u00b6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Last updated: 2025-04-11 \u2502 \u2502 Estimated reading time: 12 minutes \u2502 \u2502 Documentation heartbeat: 0 days since last validation \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Related Documentation \u00b6 DSPy Integration Overview DSPy Usage Examples DSPy Metrics & Performance Configuration Management","title":"DSPy Technical Implementation"},{"location":"model-integration/dspy-technical/#dspy-technical-implementation","text":"","title":"DSPy Technical Implementation"},{"location":"model-integration/dspy-technical/#installation-configuration","text":"Opossum Search integrates DSPy as a standard dependency rather than a vendored component. This approach ensures easy updates while maintaining the reliability expected of production systems.","title":"Installation &amp; Configuration"},{"location":"model-integration/dspy-technical/#setup-process","text":"# Add DSPy to requirements.txt pip install -U \"dspy[anthropic]\"","title":"Setup Process"},{"location":"model-integration/dspy-technical/#core-configuration","text":"DSPy requires explicit configuration to work with your preferred LLM provider: # app/integrations/dspy/manager.py import dspy from app.config import Config class DSPyManager : \"\"\"Centralized DSPy configuration and management.\"\"\" def __init__ ( self ): # Load configuration self . provider = Config . get ( \"DSPY_PROVIDER\" , \"google\" ) self . model = Config . get ( \"DSPY_MODEL\" , \"gemini-pro\" ) # Configure DSPy self . lm = dspy . LM ( f \" { self . provider } / { self . model } \" ) dspy . configure ( lm = self . lm ) # Track configuration metadata self . config_timestamp = self . _get_timestamp () def _get_timestamp ( self ): \"\"\"Get current timestamp for configuration tracking.\"\"\" import datetime return datetime . datetime . now () . isoformat ()","title":"Core Configuration"},{"location":"model-integration/dspy-technical/#integration-with-existing-components","text":"","title":"Integration with Existing Components"},{"location":"model-integration/dspy-technical/#prompt-system-integration","text":"DSPy integrates with Opossum's existing YAML-based prompt system: # app/integrations/dspy/prompt_adapter.py import dspy from app.prompts.loader import get_prompt_template class PromptOptimizer : \"\"\"Optimize prompts using DSPy.\"\"\" def __init__ ( self , dspy_manager ): self . dspy_manager = dspy_manager self . optimizer = self . _create_optimizer () def _create_optimizer ( self ): \"\"\"Create a DSPy optimizer for prompts.\"\"\" return dspy . ChainOfThought ( dspy . Predict ( instruction = \"Optimize the given prompt template for clarity, specificity, and effectiveness.\" , input_keys = [ \"template\" , \"context\" ], output_keys = [ \"optimized_template\" ] )) def optimize ( self , template_name , context = \"\" ): \"\"\"Optimize a prompt template.\"\"\" template = get_prompt_template ( template_name ) result = self . optimizer ( template = template , context = context ) return result . optimized_template","title":"Prompt System Integration"},{"location":"model-integration/dspy-technical/#chat2svg-pipeline-enhancement","text":"Enhance the Chat2SVG pipeline with DSPy modules: # app/models/chat2svg/dspy_pipeline.py import dspy from app.models.chat2svg.pipeline import Pipeline class DSPyEnhancedPipeline ( Pipeline ): \"\"\"Chat2SVG pipeline enhanced with DSPy capabilities.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . setup_dspy_modules () def setup_dspy_modules ( self ): \"\"\"Initialize DSPy modules for pipeline stages.\"\"\" # Template generation module self . template_generator = dspy . ChainOfThought ( dspy . Predict ( instruction = \"Generate SVG template based on the description.\" , input_keys = [ \"description\" ], output_keys = [ \"svg_template\" ] )) # Detail enhancement module self . detail_enhancer = dspy . ChainOfThought ( dspy . Predict ( instruction = \"Enhance SVG with detailed elements.\" , input_keys = [ \"svg_template\" , \"description\" ], output_keys = [ \"enhanced_svg\" ] )) # Optimizer module self . svg_optimizer = dspy . Predict ( instruction = \"Optimize SVG for rendering performance while maintaining quality.\" , input_keys = [ \"enhanced_svg\" ], output_keys = [ \"optimized_svg\" ] )","title":"Chat2SVG Pipeline Enhancement"},{"location":"model-integration/dspy-technical/#performance-considerations","text":"DSPy adds computational overhead during development but creates optimized artifacts that can be cached for production use: Development-Time Optimization : Run DSPy optimization during development or CI/CD pipeline Production-Time Execution : Use optimized artifacts in production for maximum performance Caching Strategy : Cache optimized prompts and signatures for reuse Resource Management : Monitor memory and CPU usage when running optimizers CHECKPOINT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2022 DSPy installation uses pip rather than vendoring \u2022 Configuration happens in centralized manager class \u2022 Integration adapters bridge to existing systems \u2022 Pipeline enhancements preserve original architecture \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"Performance Considerations"},{"location":"model-integration/dspy-technical/#temporal-markers","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Last updated: 2025-04-11 \u2502 \u2502 Estimated reading time: 12 minutes \u2502 \u2502 Documentation heartbeat: 0 days since last validation \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Temporal Markers"},{"location":"model-integration/dspy-technical/#related-documentation","text":"DSPy Integration Overview DSPy Usage Examples DSPy Metrics & Performance Configuration Management","title":"Related Documentation"},{"location":"model-integration/providers/","text":"Provider Integration \u00b6 Overview \u00b6 Opossum Search integrates with multiple AI model providers to deliver resilient, high-quality responses. This document details the implementation specifics for each supported provider, including authentication, request/response handling, and provider-specific optimizations. Supported Providers \u00b6 The system currently integrates with these model providers: Provider Type Primary Use Cases Integration Type Gemini Cloud API Advanced reasoning, multimodal REST API Ollama Local deployment Cost-effective, private deployment HTTP API Transformers Local library Fallback, specialized models Direct library Gemini Integration \u00b6 Gemini provides state-of-the-art AI capabilities through Google's API service. Authentication \u00b6 class GeminiBackend : def __init__ ( self , config ): self . api_key = config . get ( 'api_key' ) or os . environ . get ( 'GEMINI_API_KEY' ) if not self . api_key : raise ConfigurationError ( \"Gemini API key not provided\" ) self . model = config . get ( 'model' , 'gemini-pro' ) self . timeout = config . get ( 'timeout' , 10 ) self . retries = config . get ( 'max_retries' , 2 ) # Initialize client self . client = genai . Client ( api_key = self . api_key ) Request Processing \u00b6 def process ( self , query , context = None ): \"\"\"Process a query using Gemini API\"\"\" try : # Build generation config generation_config = { \"temperature\" : self . temperature , \"top_p\" : self . top_p , \"top_k\" : self . top_k , \"max_output_tokens\" : self . max_tokens } # Prepare context if provided safety_settings = [ { \"category\" : \"HARM_CATEGORY_HARASSMENT\" , \"threshold\" : \"BLOCK_ONLY_HIGH\" }, # Other safety settings... ] # Create model instance model = self . client . get_model ( self . model ) # Build prompt with context if available content = [ query ] if context : content = context + [ query ] # Generate response response = model . generate_content ( content , generation_config = generation_config , safety_settings = safety_settings ) return response . text except Exception as e : # Handle different error types if \"rate limit\" in str ( e ) . lower (): raise RateLimitError ( f \"Gemini rate limit exceeded: { str ( e ) } \" ) elif \"quota\" in str ( e ) . lower (): raise QuotaExceededError ( f \"Gemini quota exceeded: { str ( e ) } \" ) elif \"unavailable\" in str ( e ) . lower () or \"timeout\" in str ( e ) . lower (): raise ServiceUnavailableError ( f \"Gemini service unavailable: { str ( e ) } \" ) else : raise ModelProcessingError ( f \"Error processing with Gemini: { str ( e ) } \" ) Multimodal Handling \u00b6 def process_multimodal ( self , text , images = None ): \"\"\"Process a multimodal query with text and images\"\"\" try : # Create multimodal prompt parts = [ text ] # Add images if provided if images : for image in images : if isinstance ( image , str ): # Image is a URL or file path if image . startswith (( 'http://' , 'https://' )): img_data = Image . from_url ( image ) else : img_data = Image . from_file ( image ) else : # Image is already a binary object img_data = image parts . append ( img_data ) # Generate response model = self . client . get_model ( \"gemini-pro-vision\" ) response = model . generate_content ( parts ) return response . text except Exception as e : # Handle errors logger . error ( f \"Error in multimodal processing: { str ( e ) } \" ) raise ModelProcessingError ( f \"Error processing multimodal content: { str ( e ) } \" ) Usage Tracking \u00b6 def track_usage ( self , response ): \"\"\"Track token usage for monitoring and quota management\"\"\" if hasattr ( response , 'usage_metadata' ): usage = { 'prompt_tokens' : response . usage_metadata . prompt_token_count , 'completion_tokens' : response . usage_metadata . candidates_token_count , 'total_tokens' : ( response . usage_metadata . prompt_token_count + response . usage_metadata . candidates_token_count ) } # Record usage in telemetry telemetry . record_model_usage ( 'gemini' , usage ) return usage return None Ollama Integration \u00b6 Ollama provides locally deployed open-source models with a simple API. Connection Setup \u00b6 class OllamaBackend : def __init__ ( self , config ): self . host = config . get ( 'host' , 'localhost' ) self . port = config . get ( 'port' , 11434 ) self . timeout = config . get ( 'timeout' , 15 ) # Get default model self . models = config . get ( 'models' , [{ 'name' : 'llama2' , 'default' : True }]) self . default_model = next ( ( m [ 'name' ] for m in self . models if m . get ( 'default' )), 'llama2' ) # Build base URL self . base_url = f \"http:// { self . host } : { self . port } \" # Verify connection self . _verify_connection () def _verify_connection ( self ): \"\"\"Verify connection to Ollama server\"\"\" try : response = requests . get ( f \" { self . base_url } /api/tags\" , timeout = self . timeout ) response . raise_for_status () available_models = response . json () . get ( 'models' , []) # Check if our models are available for required_model in self . models : model_name = required_model [ 'name' ] if not any ( m [ 'name' ] == model_name for m in available_models ): logger . warning ( f \"Model { model_name } not found in Ollama server\" ) except requests . exceptions . RequestException as e : logger . error ( f \"Failed to connect to Ollama server: { str ( e ) } \" ) # Don't raise here - allow system to try connection when needed Request Processing \u00b6 def process ( self , query , context = None , model = None ): \"\"\"Process a query using Ollama API\"\"\" try : model = model or self . default_model # Prepare request body request_body = { \"model\" : model , \"prompt\" : query , \"stream\" : False , \"options\" : { \"temperature\" : self . temperature , \"top_p\" : self . top_p , \"num_ctx\" : 2048 # Context window size } } # Add system context if provided if context : request_body [ \"system\" ] = context # Send request to Ollama API response = requests . post ( f \" { self . base_url } /api/generate\" , json = request_body , timeout = self . timeout ) response . raise_for_status () result = response . json () return result . get ( 'response' , '' ) except requests . exceptions . Timeout : raise ServiceUnavailableError ( \"Ollama request timed out\" ) except requests . exceptions . ConnectionError : raise ServiceUnavailableError ( \"Unable to connect to Ollama server\" ) except requests . exceptions . HTTPError as e : raise ModelProcessingError ( f \"Ollama HTTP error: { str ( e ) } \" ) except Exception as e : raise ModelProcessingError ( f \"Error processing with Ollama: { str ( e ) } \" ) Model Management \u00b6 def list_available_models ( self ): \"\"\"List models available on the Ollama server\"\"\" try : response = requests . get ( f \" { self . base_url } /api/tags\" , timeout = self . timeout ) response . raise_for_status () return response . json () . get ( 'models' , []) except Exception as e : logger . error ( f \"Error listing Ollama models: { str ( e ) } \" ) return [] def pull_model ( self , model_name ): \"\"\"Pull a model to the Ollama server if not already available\"\"\" try : # Check if model exists available_models = self . list_available_models () if any ( m [ 'name' ] == model_name for m in available_models ): logger . info ( f \"Model { model_name } already available\" ) return True # Pull model logger . info ( f \"Pulling model { model_name } to Ollama server...\" ) response = requests . post ( f \" { self . base_url } /api/pull\" , json = { \"name\" : model_name }, timeout = 300 # Longer timeout for model pulls ) response . raise_for_status () return True except Exception as e : logger . error ( f \"Error pulling model { model_name } : { str ( e ) } \" ) return False Transformers Integration \u00b6 Hugging Face Transformers provides a flexible library for running models locally. Initialization \u00b6 class TransformersBackend : def __init__ ( self , config ): self . model_path = config . get ( 'model_path' , './models/local' ) self . default_model = config . get ( 'default_model' , 'microsoft/phi-2' ) self . quantization = config . get ( 'quantization' , 'int8' ) # Determine device (CPU/GPU) self . device = config . get ( 'device' , 'auto' ) if self . device == 'auto' : self . device = 'cuda' if torch . cuda . is_available () else 'cpu' # Load default model self . models = {} self . _load_model ( self . default_model ) def _load_model ( self , model_name ): \"\"\"Load a model if not already loaded\"\"\" if model_name in self . models : return self . models [ model_name ] try : logger . info ( f \"Loading Transformers model: { model_name } \" ) # Load model with appropriate quantization if self . quantization == 'int8' : model = AutoModelForCausalLM . from_pretrained ( model_name , device_map = self . device , load_in_8bit = True ) elif self . quantization == 'int4' : model = AutoModelForCausalLM . from_pretrained ( model_name , device_map = self . device , load_in_4bit = True ) else : model = AutoModelForCausalLM . from_pretrained ( model_name , device_map = self . device ) # Load tokenizer tokenizer = AutoTokenizer . from_pretrained ( model_name ) self . models [ model_name ] = { 'model' : model , 'tokenizer' : tokenizer , 'last_used' : time . time () } return self . models [ model_name ] except Exception as e : logger . error ( f \"Error loading model { model_name } : { str ( e ) } \" ) raise ModelInitializationError ( f \"Failed to load { model_name } : { str ( e ) } \" ) Request Processing \u00b6 def process ( self , query , context = None , model_name = None ): \"\"\"Process a query using Transformers\"\"\" try : model_name = model_name or self . default_model # Load model if not already loaded if model_name not in self . models : self . _load_model ( model_name ) model_data = self . models [ model_name ] model = model_data [ 'model' ] tokenizer = model_data [ 'tokenizer' ] # Update last used timestamp model_data [ 'last_used' ] = time . time () # Prepare input text with context if provided if context : input_text = f \" { context } \\n\\n User: { query } \\n\\n Assistant:\" else : input_text = f \"User: { query } \\n\\n Assistant:\" # Tokenize input inputs = tokenizer ( input_text , return_tensors = \"pt\" ) . to ( self . device ) # Generate response with torch . no_grad (): output = model . generate ( inputs [ \"input_ids\" ], max_new_tokens = 512 , temperature = 0.7 , top_p = 0.9 , do_sample = True , pad_token_id = tokenizer . eos_token_id ) # Decode and clean response response_text = tokenizer . decode ( output [ 0 ], skip_special_tokens = True ) response_text = response_text . replace ( input_text , \"\" ) . strip () # Unload rarely used models to save memory self . _manage_loaded_models () return response_text except torch . cuda . OutOfMemoryError : logger . error ( f \"CUDA out of memory while processing with { model_name } \" ) raise ResourceExhaustedError ( \"GPU memory exhausted\" ) except Exception as e : raise ModelProcessingError ( f \"Error processing with Transformers: { str ( e ) } \" ) Memory Management \u00b6 def _manage_loaded_models ( self ): \"\"\"Unload models that haven't been used recently to free memory\"\"\" now = time . time () unload_threshold = 3600 # 1 hour # Don't unload if only one model is loaded if len ( self . models ) <= 1 : return # Check each model's last used time for model_name , model_data in list ( self . models . items ()): # Skip default model if model_name == self . default_model : continue # Unload if unused for threshold period if now - model_data [ 'last_used' ] > unload_threshold : logger . info ( f \"Unloading unused model: { model_name } \" ) # Remove from loaded models del self . models [ model_name ] # Force garbage collection if self . device == 'cuda' : torch . cuda . empty_cache () gc . collect () Error Handling \u00b6 Each provider has specific error types that require specialized handling: class ModelError ( Exception ): \"\"\"Base class for model-related errors\"\"\" pass class ModelInitializationError ( ModelError ): \"\"\"Error initializing a model\"\"\" pass class ModelProcessingError ( ModelError ): \"\"\"Error processing a request\"\"\" pass class ServiceUnavailableError ( ModelError ): \"\"\"Model service is unavailable\"\"\" pass class RateLimitError ( ModelError ): \"\"\"Rate limit exceeded\"\"\" pass class QuotaExceededError ( ModelError ): \"\"\"Usage quota exceeded\"\"\" pass class ResourceExhaustedError ( ModelError ): \"\"\"Computational resources exhausted\"\"\" pass Provider-Specific Considerations \u00b6 Gemini \u00b6 Rate Limiting : Implements exponential backoff for rate limit errors API Key Rotation : Supports multiple API keys with automatic rotation Quota Tracking : Monitors token usage to prevent quota overruns Safety Settings : Configurable content safety parameters Ollama \u00b6 Health Checks : Periodic verification of server availability Model Loading : Dynamic loading of models as needed Resource Monitoring : Tracks resource usage to prevent overloading Concurrency Control : Limits parallel requests to prevent resource contention Transformers \u00b6 GPU Management : Dynamic allocation of GPU resources Memory Optimization : Automatic model unloading to preserve memory Quantization : Support for different quantization levels (4-bit, 8-bit) Model Caching : Efficient loading and caching of model weights Related Documentation \u00b6 Model Integration Architecture Backend Selection Capability Matrix Model Configuration","title":"Provider Integration"},{"location":"model-integration/providers/#provider-integration","text":"","title":"Provider Integration"},{"location":"model-integration/providers/#overview","text":"Opossum Search integrates with multiple AI model providers to deliver resilient, high-quality responses. This document details the implementation specifics for each supported provider, including authentication, request/response handling, and provider-specific optimizations.","title":"Overview"},{"location":"model-integration/providers/#supported-providers","text":"The system currently integrates with these model providers: Provider Type Primary Use Cases Integration Type Gemini Cloud API Advanced reasoning, multimodal REST API Ollama Local deployment Cost-effective, private deployment HTTP API Transformers Local library Fallback, specialized models Direct library","title":"Supported Providers"},{"location":"model-integration/providers/#gemini-integration","text":"Gemini provides state-of-the-art AI capabilities through Google's API service.","title":"Gemini Integration"},{"location":"model-integration/providers/#authentication","text":"class GeminiBackend : def __init__ ( self , config ): self . api_key = config . get ( 'api_key' ) or os . environ . get ( 'GEMINI_API_KEY' ) if not self . api_key : raise ConfigurationError ( \"Gemini API key not provided\" ) self . model = config . get ( 'model' , 'gemini-pro' ) self . timeout = config . get ( 'timeout' , 10 ) self . retries = config . get ( 'max_retries' , 2 ) # Initialize client self . client = genai . Client ( api_key = self . api_key )","title":"Authentication"},{"location":"model-integration/providers/#request-processing","text":"def process ( self , query , context = None ): \"\"\"Process a query using Gemini API\"\"\" try : # Build generation config generation_config = { \"temperature\" : self . temperature , \"top_p\" : self . top_p , \"top_k\" : self . top_k , \"max_output_tokens\" : self . max_tokens } # Prepare context if provided safety_settings = [ { \"category\" : \"HARM_CATEGORY_HARASSMENT\" , \"threshold\" : \"BLOCK_ONLY_HIGH\" }, # Other safety settings... ] # Create model instance model = self . client . get_model ( self . model ) # Build prompt with context if available content = [ query ] if context : content = context + [ query ] # Generate response response = model . generate_content ( content , generation_config = generation_config , safety_settings = safety_settings ) return response . text except Exception as e : # Handle different error types if \"rate limit\" in str ( e ) . lower (): raise RateLimitError ( f \"Gemini rate limit exceeded: { str ( e ) } \" ) elif \"quota\" in str ( e ) . lower (): raise QuotaExceededError ( f \"Gemini quota exceeded: { str ( e ) } \" ) elif \"unavailable\" in str ( e ) . lower () or \"timeout\" in str ( e ) . lower (): raise ServiceUnavailableError ( f \"Gemini service unavailable: { str ( e ) } \" ) else : raise ModelProcessingError ( f \"Error processing with Gemini: { str ( e ) } \" )","title":"Request Processing"},{"location":"model-integration/providers/#multimodal-handling","text":"def process_multimodal ( self , text , images = None ): \"\"\"Process a multimodal query with text and images\"\"\" try : # Create multimodal prompt parts = [ text ] # Add images if provided if images : for image in images : if isinstance ( image , str ): # Image is a URL or file path if image . startswith (( 'http://' , 'https://' )): img_data = Image . from_url ( image ) else : img_data = Image . from_file ( image ) else : # Image is already a binary object img_data = image parts . append ( img_data ) # Generate response model = self . client . get_model ( \"gemini-pro-vision\" ) response = model . generate_content ( parts ) return response . text except Exception as e : # Handle errors logger . error ( f \"Error in multimodal processing: { str ( e ) } \" ) raise ModelProcessingError ( f \"Error processing multimodal content: { str ( e ) } \" )","title":"Multimodal Handling"},{"location":"model-integration/providers/#usage-tracking","text":"def track_usage ( self , response ): \"\"\"Track token usage for monitoring and quota management\"\"\" if hasattr ( response , 'usage_metadata' ): usage = { 'prompt_tokens' : response . usage_metadata . prompt_token_count , 'completion_tokens' : response . usage_metadata . candidates_token_count , 'total_tokens' : ( response . usage_metadata . prompt_token_count + response . usage_metadata . candidates_token_count ) } # Record usage in telemetry telemetry . record_model_usage ( 'gemini' , usage ) return usage return None","title":"Usage Tracking"},{"location":"model-integration/providers/#ollama-integration","text":"Ollama provides locally deployed open-source models with a simple API.","title":"Ollama Integration"},{"location":"model-integration/providers/#connection-setup","text":"class OllamaBackend : def __init__ ( self , config ): self . host = config . get ( 'host' , 'localhost' ) self . port = config . get ( 'port' , 11434 ) self . timeout = config . get ( 'timeout' , 15 ) # Get default model self . models = config . get ( 'models' , [{ 'name' : 'llama2' , 'default' : True }]) self . default_model = next ( ( m [ 'name' ] for m in self . models if m . get ( 'default' )), 'llama2' ) # Build base URL self . base_url = f \"http:// { self . host } : { self . port } \" # Verify connection self . _verify_connection () def _verify_connection ( self ): \"\"\"Verify connection to Ollama server\"\"\" try : response = requests . get ( f \" { self . base_url } /api/tags\" , timeout = self . timeout ) response . raise_for_status () available_models = response . json () . get ( 'models' , []) # Check if our models are available for required_model in self . models : model_name = required_model [ 'name' ] if not any ( m [ 'name' ] == model_name for m in available_models ): logger . warning ( f \"Model { model_name } not found in Ollama server\" ) except requests . exceptions . RequestException as e : logger . error ( f \"Failed to connect to Ollama server: { str ( e ) } \" ) # Don't raise here - allow system to try connection when needed","title":"Connection Setup"},{"location":"model-integration/providers/#request-processing_1","text":"def process ( self , query , context = None , model = None ): \"\"\"Process a query using Ollama API\"\"\" try : model = model or self . default_model # Prepare request body request_body = { \"model\" : model , \"prompt\" : query , \"stream\" : False , \"options\" : { \"temperature\" : self . temperature , \"top_p\" : self . top_p , \"num_ctx\" : 2048 # Context window size } } # Add system context if provided if context : request_body [ \"system\" ] = context # Send request to Ollama API response = requests . post ( f \" { self . base_url } /api/generate\" , json = request_body , timeout = self . timeout ) response . raise_for_status () result = response . json () return result . get ( 'response' , '' ) except requests . exceptions . Timeout : raise ServiceUnavailableError ( \"Ollama request timed out\" ) except requests . exceptions . ConnectionError : raise ServiceUnavailableError ( \"Unable to connect to Ollama server\" ) except requests . exceptions . HTTPError as e : raise ModelProcessingError ( f \"Ollama HTTP error: { str ( e ) } \" ) except Exception as e : raise ModelProcessingError ( f \"Error processing with Ollama: { str ( e ) } \" )","title":"Request Processing"},{"location":"model-integration/providers/#model-management","text":"def list_available_models ( self ): \"\"\"List models available on the Ollama server\"\"\" try : response = requests . get ( f \" { self . base_url } /api/tags\" , timeout = self . timeout ) response . raise_for_status () return response . json () . get ( 'models' , []) except Exception as e : logger . error ( f \"Error listing Ollama models: { str ( e ) } \" ) return [] def pull_model ( self , model_name ): \"\"\"Pull a model to the Ollama server if not already available\"\"\" try : # Check if model exists available_models = self . list_available_models () if any ( m [ 'name' ] == model_name for m in available_models ): logger . info ( f \"Model { model_name } already available\" ) return True # Pull model logger . info ( f \"Pulling model { model_name } to Ollama server...\" ) response = requests . post ( f \" { self . base_url } /api/pull\" , json = { \"name\" : model_name }, timeout = 300 # Longer timeout for model pulls ) response . raise_for_status () return True except Exception as e : logger . error ( f \"Error pulling model { model_name } : { str ( e ) } \" ) return False","title":"Model Management"},{"location":"model-integration/providers/#transformers-integration","text":"Hugging Face Transformers provides a flexible library for running models locally.","title":"Transformers Integration"},{"location":"model-integration/providers/#initialization","text":"class TransformersBackend : def __init__ ( self , config ): self . model_path = config . get ( 'model_path' , './models/local' ) self . default_model = config . get ( 'default_model' , 'microsoft/phi-2' ) self . quantization = config . get ( 'quantization' , 'int8' ) # Determine device (CPU/GPU) self . device = config . get ( 'device' , 'auto' ) if self . device == 'auto' : self . device = 'cuda' if torch . cuda . is_available () else 'cpu' # Load default model self . models = {} self . _load_model ( self . default_model ) def _load_model ( self , model_name ): \"\"\"Load a model if not already loaded\"\"\" if model_name in self . models : return self . models [ model_name ] try : logger . info ( f \"Loading Transformers model: { model_name } \" ) # Load model with appropriate quantization if self . quantization == 'int8' : model = AutoModelForCausalLM . from_pretrained ( model_name , device_map = self . device , load_in_8bit = True ) elif self . quantization == 'int4' : model = AutoModelForCausalLM . from_pretrained ( model_name , device_map = self . device , load_in_4bit = True ) else : model = AutoModelForCausalLM . from_pretrained ( model_name , device_map = self . device ) # Load tokenizer tokenizer = AutoTokenizer . from_pretrained ( model_name ) self . models [ model_name ] = { 'model' : model , 'tokenizer' : tokenizer , 'last_used' : time . time () } return self . models [ model_name ] except Exception as e : logger . error ( f \"Error loading model { model_name } : { str ( e ) } \" ) raise ModelInitializationError ( f \"Failed to load { model_name } : { str ( e ) } \" )","title":"Initialization"},{"location":"model-integration/providers/#request-processing_2","text":"def process ( self , query , context = None , model_name = None ): \"\"\"Process a query using Transformers\"\"\" try : model_name = model_name or self . default_model # Load model if not already loaded if model_name not in self . models : self . _load_model ( model_name ) model_data = self . models [ model_name ] model = model_data [ 'model' ] tokenizer = model_data [ 'tokenizer' ] # Update last used timestamp model_data [ 'last_used' ] = time . time () # Prepare input text with context if provided if context : input_text = f \" { context } \\n\\n User: { query } \\n\\n Assistant:\" else : input_text = f \"User: { query } \\n\\n Assistant:\" # Tokenize input inputs = tokenizer ( input_text , return_tensors = \"pt\" ) . to ( self . device ) # Generate response with torch . no_grad (): output = model . generate ( inputs [ \"input_ids\" ], max_new_tokens = 512 , temperature = 0.7 , top_p = 0.9 , do_sample = True , pad_token_id = tokenizer . eos_token_id ) # Decode and clean response response_text = tokenizer . decode ( output [ 0 ], skip_special_tokens = True ) response_text = response_text . replace ( input_text , \"\" ) . strip () # Unload rarely used models to save memory self . _manage_loaded_models () return response_text except torch . cuda . OutOfMemoryError : logger . error ( f \"CUDA out of memory while processing with { model_name } \" ) raise ResourceExhaustedError ( \"GPU memory exhausted\" ) except Exception as e : raise ModelProcessingError ( f \"Error processing with Transformers: { str ( e ) } \" )","title":"Request Processing"},{"location":"model-integration/providers/#memory-management","text":"def _manage_loaded_models ( self ): \"\"\"Unload models that haven't been used recently to free memory\"\"\" now = time . time () unload_threshold = 3600 # 1 hour # Don't unload if only one model is loaded if len ( self . models ) <= 1 : return # Check each model's last used time for model_name , model_data in list ( self . models . items ()): # Skip default model if model_name == self . default_model : continue # Unload if unused for threshold period if now - model_data [ 'last_used' ] > unload_threshold : logger . info ( f \"Unloading unused model: { model_name } \" ) # Remove from loaded models del self . models [ model_name ] # Force garbage collection if self . device == 'cuda' : torch . cuda . empty_cache () gc . collect ()","title":"Memory Management"},{"location":"model-integration/providers/#error-handling","text":"Each provider has specific error types that require specialized handling: class ModelError ( Exception ): \"\"\"Base class for model-related errors\"\"\" pass class ModelInitializationError ( ModelError ): \"\"\"Error initializing a model\"\"\" pass class ModelProcessingError ( ModelError ): \"\"\"Error processing a request\"\"\" pass class ServiceUnavailableError ( ModelError ): \"\"\"Model service is unavailable\"\"\" pass class RateLimitError ( ModelError ): \"\"\"Rate limit exceeded\"\"\" pass class QuotaExceededError ( ModelError ): \"\"\"Usage quota exceeded\"\"\" pass class ResourceExhaustedError ( ModelError ): \"\"\"Computational resources exhausted\"\"\" pass","title":"Error Handling"},{"location":"model-integration/providers/#provider-specific-considerations","text":"","title":"Provider-Specific Considerations"},{"location":"model-integration/providers/#gemini","text":"Rate Limiting : Implements exponential backoff for rate limit errors API Key Rotation : Supports multiple API keys with automatic rotation Quota Tracking : Monitors token usage to prevent quota overruns Safety Settings : Configurable content safety parameters","title":"Gemini"},{"location":"model-integration/providers/#ollama","text":"Health Checks : Periodic verification of server availability Model Loading : Dynamic loading of models as needed Resource Monitoring : Tracks resource usage to prevent overloading Concurrency Control : Limits parallel requests to prevent resource contention","title":"Ollama"},{"location":"model-integration/providers/#transformers","text":"GPU Management : Dynamic allocation of GPU resources Memory Optimization : Automatic model unloading to preserve memory Quantization : Support for different quantization levels (4-bit, 8-bit) Model Caching : Efficient loading and caching of model weights","title":"Transformers"},{"location":"model-integration/providers/#related-documentation","text":"Model Integration Architecture Backend Selection Capability Matrix Model Configuration","title":"Related Documentation"},{"location":"service-availability/architecture-constraints/","text":"Architecture Constraints \u00b6 External Service Dependencies \u00b6 Service Dependency Type Constraint Impact Gemini API Hard dependency for primary path Requires valid API key and network connectivity Service unavailability triggers failover to Ollama Google Cloud Platform Infrastructure for Gemini Subject to Google's maintenance windows and SLAs May cause temporary Gemini unavailability OpenAI API Optional fallback service Rate-limited based on subscription tier Provides additional redundancy if configured Local Service Dependencies \u00b6 Service Constraint Impact Ollama Requires local GPU for optimal performance Performance degradation on CPU-only systems Transformers Requires sufficient RAM for model loading Lower capability models used when RAM is limited Python Runtime Version 3.8+ required Application will not start on older Python versions Infrastructure Requirements \u00b6 Component Requirement Rationale CPU 4+ cores recommended Needed for concurrent service checks and model inference RAM Minimum 8GB, 16GB recommended Required for local model loading and operation Storage 10GB minimum for application and models Local models require significant storage space Network Reliable internet connection for external APIs Intermittent connectivity will affect Gemini service Docker Optional but recommended for Ollama isolation Simplifies deployment and management of Ollama service Operating Environment Constraints \u00b6 Constraint Description Mitigation Firewall Restrictions Corporate firewalls may block API calls to Google Configure proxy settings or use local services only Rate Limits Gemini API has strict rate limits Implement request queueing and smart routing Offline Operation Must function with degraded capabilities when offline Ensure Transformers models are pre-downloaded Cross-Platform Support Must run on Windows, macOS, and Linux Abstract platform-specific code and test on all platforms Technical Debt and Limitations \u00b6 Limitation Description Future Improvement Manual Failover Recovery System does not automatically recover preferred services Implement automatic service recovery detection Limited API Compatibility Different model providers have different APIs Create abstraction layer to normalize responses Fixed Check Interval Service checks occur at fixed intervals Implement adaptive check intervals based on service stability Missing Health Metrics Only binary up/down status tracked Add response time and error rate tracking Circuit Breaker Patterns Basic implementation of failure detection Implement comprehensive circuit breaker patterns","title":"Architecture Constraints"},{"location":"service-availability/architecture-constraints/#architecture-constraints","text":"","title":"Architecture Constraints"},{"location":"service-availability/architecture-constraints/#external-service-dependencies","text":"Service Dependency Type Constraint Impact Gemini API Hard dependency for primary path Requires valid API key and network connectivity Service unavailability triggers failover to Ollama Google Cloud Platform Infrastructure for Gemini Subject to Google's maintenance windows and SLAs May cause temporary Gemini unavailability OpenAI API Optional fallback service Rate-limited based on subscription tier Provides additional redundancy if configured","title":"External Service Dependencies"},{"location":"service-availability/architecture-constraints/#local-service-dependencies","text":"Service Constraint Impact Ollama Requires local GPU for optimal performance Performance degradation on CPU-only systems Transformers Requires sufficient RAM for model loading Lower capability models used when RAM is limited Python Runtime Version 3.8+ required Application will not start on older Python versions","title":"Local Service Dependencies"},{"location":"service-availability/architecture-constraints/#infrastructure-requirements","text":"Component Requirement Rationale CPU 4+ cores recommended Needed for concurrent service checks and model inference RAM Minimum 8GB, 16GB recommended Required for local model loading and operation Storage 10GB minimum for application and models Local models require significant storage space Network Reliable internet connection for external APIs Intermittent connectivity will affect Gemini service Docker Optional but recommended for Ollama isolation Simplifies deployment and management of Ollama service","title":"Infrastructure Requirements"},{"location":"service-availability/architecture-constraints/#operating-environment-constraints","text":"Constraint Description Mitigation Firewall Restrictions Corporate firewalls may block API calls to Google Configure proxy settings or use local services only Rate Limits Gemini API has strict rate limits Implement request queueing and smart routing Offline Operation Must function with degraded capabilities when offline Ensure Transformers models are pre-downloaded Cross-Platform Support Must run on Windows, macOS, and Linux Abstract platform-specific code and test on all platforms","title":"Operating Environment Constraints"},{"location":"service-availability/architecture-constraints/#technical-debt-and-limitations","text":"Limitation Description Future Improvement Manual Failover Recovery System does not automatically recover preferred services Implement automatic service recovery detection Limited API Compatibility Different model providers have different APIs Create abstraction layer to normalize responses Fixed Check Interval Service checks occur at fixed intervals Implement adaptive check intervals based on service stability Missing Health Metrics Only binary up/down status tracked Add response time and error rate tracking Circuit Breaker Patterns Basic implementation of failure detection Implement comprehensive circuit breaker patterns","title":"Technical Debt and Limitations"},{"location":"service-availability/availability-monitoring/","text":"Availability Monitoring \u00b6 Monitoring Strategy \u00b6 Component Strategy Implementation Service Availability Active polling Asynchronous health checks to all services Rate Limit Tracking Counter-based In-memory tracking with time-based resets Status Changes Event-based Status change detection and logging Failure Detection Exception handling Request timeouts and error catching Check Frequency and Scheduling \u00b6 Service Check Frequency Caching Duration Trigger Mechanism Gemini API Every 30 seconds (max) 30 seconds On-demand with caching Ollama Every 30 seconds (max) 30 seconds On-demand with caching Transformers Every 30 seconds (max) 30 seconds On-demand with caching All Services On application startup N/A Initialization check Metrics Collection \u00b6 Metric Collection Method Storage Purpose Service Status Boolean availability flag In-memory dictionary Service selection Last Check Timestamp Datetime object In-memory dictionary Throttling checks Gemini Daily Usage Counter with daily reset In-memory counter Rate limit compliance Gemini Per-Minute Usage Counter with minute reset In-memory counter Rate limit compliance Service Transitions Event logging Application logs Diagnostics and reporting Monitoring Tools \u00b6 Tool Purpose Integration Standard Logging Record availability events and transitions Python logging module Health Check API Internal HTTP endpoint for status monitoring Flask route handler Exception Tracking Capture and report service check failures Try-except blocks with logging Status Dashboard Visual representation of service availability Admin interface (planned) Implementation Details \u00b6 The monitoring system uses concurrent asynchronous checks to efficiently assess service availability: class ServiceAvailability : def __init__ ( self ): self . services_status = { \"gemini\" : { \"available\" : False , \"last_checked\" : None }, \"ollama\" : { \"available\" : False , \"last_checked\" : None }, \"transformers\" : { \"available\" : False , \"last_checked\" : None } } self . check_interval = 30 # seconds async def check_all_services ( self ): \"\"\"Check availability of all configured services\"\"\" logger . debug ( \"Beginning availability check for all services\" ) # Run all checks concurrently await asyncio . gather ( self . check_ollama_availability (), self . check_gemini_availability (), self . check_transformers_availability () ) async def check_gemini_availability ( self ): \"\"\"Check if Gemini API is available\"\"\" try : # Implement a lightweight request to Gemini API # Record result in services_status dictionary current_time = datetime . now () if self . services_status [ \"gemini\" ][ \"last_checked\" ] is None or \\ ( current_time - self . services_status [ \"gemini\" ][ \"last_checked\" ]) . seconds > self . check_interval : # Perform actual check here self . services_status [ \"gemini\" ][ \"available\" ] = True # Set based on check result self . services_status [ \"gemini\" ][ \"last_checked\" ] = current_time logger . info ( \"Gemini API is available\" ) except Exception as e : self . services_status [ \"gemini\" ][ \"available\" ] = False self . services_status [ \"gemini\" ][ \"last_checked\" ] = datetime . now () logger . error ( f \"Gemini API check failed: { str ( e ) } \" ) # Similar methods for check_ollama_availability and check_transformers_availability Availability Reporting \u00b6 Report Frequency Contents Distribution Status Change Alerts Real-time Service, previous status, new status, reason Logs Availability Summary Daily Uptime percentages, outage periods, failover counts Logs, email (planned) Rate Limit Reports Daily API usage statistics, remaining quota Logs Service Health Check On-demand Current status of all services API endpoint (planned) Note Regularly review the Availability Summary reports to identify trends and potential issues with service availability.","title":"Availability Monitoring"},{"location":"service-availability/availability-monitoring/#availability-monitoring","text":"","title":"Availability Monitoring"},{"location":"service-availability/availability-monitoring/#monitoring-strategy","text":"Component Strategy Implementation Service Availability Active polling Asynchronous health checks to all services Rate Limit Tracking Counter-based In-memory tracking with time-based resets Status Changes Event-based Status change detection and logging Failure Detection Exception handling Request timeouts and error catching","title":"Monitoring Strategy"},{"location":"service-availability/availability-monitoring/#check-frequency-and-scheduling","text":"Service Check Frequency Caching Duration Trigger Mechanism Gemini API Every 30 seconds (max) 30 seconds On-demand with caching Ollama Every 30 seconds (max) 30 seconds On-demand with caching Transformers Every 30 seconds (max) 30 seconds On-demand with caching All Services On application startup N/A Initialization check","title":"Check Frequency and Scheduling"},{"location":"service-availability/availability-monitoring/#metrics-collection","text":"Metric Collection Method Storage Purpose Service Status Boolean availability flag In-memory dictionary Service selection Last Check Timestamp Datetime object In-memory dictionary Throttling checks Gemini Daily Usage Counter with daily reset In-memory counter Rate limit compliance Gemini Per-Minute Usage Counter with minute reset In-memory counter Rate limit compliance Service Transitions Event logging Application logs Diagnostics and reporting","title":"Metrics Collection"},{"location":"service-availability/availability-monitoring/#monitoring-tools","text":"Tool Purpose Integration Standard Logging Record availability events and transitions Python logging module Health Check API Internal HTTP endpoint for status monitoring Flask route handler Exception Tracking Capture and report service check failures Try-except blocks with logging Status Dashboard Visual representation of service availability Admin interface (planned)","title":"Monitoring Tools"},{"location":"service-availability/availability-monitoring/#implementation-details","text":"The monitoring system uses concurrent asynchronous checks to efficiently assess service availability: class ServiceAvailability : def __init__ ( self ): self . services_status = { \"gemini\" : { \"available\" : False , \"last_checked\" : None }, \"ollama\" : { \"available\" : False , \"last_checked\" : None }, \"transformers\" : { \"available\" : False , \"last_checked\" : None } } self . check_interval = 30 # seconds async def check_all_services ( self ): \"\"\"Check availability of all configured services\"\"\" logger . debug ( \"Beginning availability check for all services\" ) # Run all checks concurrently await asyncio . gather ( self . check_ollama_availability (), self . check_gemini_availability (), self . check_transformers_availability () ) async def check_gemini_availability ( self ): \"\"\"Check if Gemini API is available\"\"\" try : # Implement a lightweight request to Gemini API # Record result in services_status dictionary current_time = datetime . now () if self . services_status [ \"gemini\" ][ \"last_checked\" ] is None or \\ ( current_time - self . services_status [ \"gemini\" ][ \"last_checked\" ]) . seconds > self . check_interval : # Perform actual check here self . services_status [ \"gemini\" ][ \"available\" ] = True # Set based on check result self . services_status [ \"gemini\" ][ \"last_checked\" ] = current_time logger . info ( \"Gemini API is available\" ) except Exception as e : self . services_status [ \"gemini\" ][ \"available\" ] = False self . services_status [ \"gemini\" ][ \"last_checked\" ] = datetime . now () logger . error ( f \"Gemini API check failed: { str ( e ) } \" ) # Similar methods for check_ollama_availability and check_transformers_availability","title":"Implementation Details"},{"location":"service-availability/availability-monitoring/#availability-reporting","text":"Report Frequency Contents Distribution Status Change Alerts Real-time Service, previous status, new status, reason Logs Availability Summary Daily Uptime percentages, outage periods, failover counts Logs, email (planned) Rate Limit Reports Daily API usage statistics, remaining quota Logs Service Health Check On-demand Current status of all services API endpoint (planned) Note Regularly review the Availability Summary reports to identify trends and potential issues with service availability.","title":"Availability Reporting"},{"location":"service-availability/context-and-scope/","text":"Context and Scope \u00b6 Overview \u00b6 This document describes the service availability monitoring system implemented for the Opossum Search application. The system monitors the availability of multiple AI model backends, manages rate limits, and provides automatic failover to ensure continuous operation. Services in Scope \u00b6 The following services are monitored for availability: Service Type Purpose Availability Mechanism Gemini External API High-capability AI models with API-based access API key validation and rate limit monitoring Ollama Local service Self-hosted AI models with REST API Connection health checking Transformers Local library Fallback for offline operation Always assumed available (local) Critical Paths \u00b6 The application depends on at least one service being available to respond to user queries. The dependency chain is: Primary Path : Gemini API (when available and suitable for query type) Secondary Path : Ollama service (when available and suitable for query type) Fallback Path : Transformers models (always available, used when other services fail) Note The application is designed to function even when the primary service (Gemini API) is unavailable, by automatically failing over to alternative services. System Boundaries \u00b6 The availability system: Monitors : All AI model service endpoints Records : Usage statistics for rate-limited services Logs : Service status changes and availability events Does Not : Handle network configuration or service deployment Stakeholders \u00b6 Stakeholder Interest in Service Availability End Users Uninterrupted access to AI functionality Operations Team Monitoring service health and diagnostics Developers Understanding fallback behavior and service dependencies External Interfaces \u00b6 Gemini API : Google Cloud API with rate limits and authentication Ollama REST API : Local service providing model inference Transformers Library : Local Python library for model inference Technical Context \u00b6 The service availability system is implemented as a component in the model backend selection process. It: Performs regular health checks (maximum once per 30 seconds per service) Runs checks concurrently to minimize impact on performance Influences model selection based on real-time availability Provides status information for logging and diagnostics","title":"Context and Scope"},{"location":"service-availability/context-and-scope/#context-and-scope","text":"","title":"Context and Scope"},{"location":"service-availability/context-and-scope/#overview","text":"This document describes the service availability monitoring system implemented for the Opossum Search application. The system monitors the availability of multiple AI model backends, manages rate limits, and provides automatic failover to ensure continuous operation.","title":"Overview"},{"location":"service-availability/context-and-scope/#services-in-scope","text":"The following services are monitored for availability: Service Type Purpose Availability Mechanism Gemini External API High-capability AI models with API-based access API key validation and rate limit monitoring Ollama Local service Self-hosted AI models with REST API Connection health checking Transformers Local library Fallback for offline operation Always assumed available (local)","title":"Services in Scope"},{"location":"service-availability/context-and-scope/#critical-paths","text":"The application depends on at least one service being available to respond to user queries. The dependency chain is: Primary Path : Gemini API (when available and suitable for query type) Secondary Path : Ollama service (when available and suitable for query type) Fallback Path : Transformers models (always available, used when other services fail) Note The application is designed to function even when the primary service (Gemini API) is unavailable, by automatically failing over to alternative services.","title":"Critical Paths"},{"location":"service-availability/context-and-scope/#system-boundaries","text":"The availability system: Monitors : All AI model service endpoints Records : Usage statistics for rate-limited services Logs : Service status changes and availability events Does Not : Handle network configuration or service deployment","title":"System Boundaries"},{"location":"service-availability/context-and-scope/#stakeholders","text":"Stakeholder Interest in Service Availability End Users Uninterrupted access to AI functionality Operations Team Monitoring service health and diagnostics Developers Understanding fallback behavior and service dependencies","title":"Stakeholders"},{"location":"service-availability/context-and-scope/#external-interfaces","text":"Gemini API : Google Cloud API with rate limits and authentication Ollama REST API : Local service providing model inference Transformers Library : Local Python library for model inference","title":"External Interfaces"},{"location":"service-availability/context-and-scope/#technical-context","text":"The service availability system is implemented as a component in the model backend selection process. It: Performs regular health checks (maximum once per 30 seconds per service) Runs checks concurrently to minimize impact on performance Influences model selection based on real-time availability Provides status information for logging and diagnostics","title":"Technical Context"},{"location":"service-availability/log-alerts/","text":"Logging and Alerts \u00b6 Effective logging and alerting are crucial for monitoring service availability and diagnosing issues promptly. Logging Strategy \u00b6 The application uses Python's standard logging module, configured globally via logging.dictConfig during application startup ( app/__init__.py ). Log Category Purpose Implementation Details Retention Application Logs General operational events, debug info Structured JSON format (via CustomJsonFormatter ). Includes timestamp , level , message , logger , service_name , environment , request_id . Console output format varies by environment (simple for dev, JSON for prod). Configurable (e.g., file rotation, log aggregation system) Service Status Track service availability changes INFO level logs via ServiceAvailability module. JSON format. 30 days (example) Rate Limit Monitor API usage against quotas WARNING level logs when limits approached/hit. JSON format. 90 days (example) Error Events Record service failures and exceptions Handled by ErrorHandler . Logs exceptions with stack traces (ERROR/CRITICAL level). Includes error category and context. JSON format. 14 days (example) Recovery Actions Document automatic recovery attempts INFO/WARNING logs from CircuitBreaker (state changes) and retry decorator. JSON format. 14 days (example) Performance Metrics Track response times and latency Handled via Prometheus metrics (see below). 180 days (example) Security Events Audit security-related actions Dedicated 'security' logger, potentially separate handler. JSON format. See Security Model . 90+ days (example) Log Levels \u00b6 Standard Python log levels are used: Level Usage Example DEBUG Detailed diagnostic information \"Checking Gemini API availability\", \"Extracted features: {...}\" INFO Normal operational events \"Service status changed: Gemini API now AVAILABLE\", \"Request processed successfully\" WARNING Non-critical issues, potential problems \"Approaching rate limit (85% used)\", \"Circuit breaker entering HALF_OPEN state\" ERROR Runtime errors, failed operations \"Connection to Ollama failed: Connection refused\", \"Error processing request: Validation Error\" CRITICAL System-wide failures \"All services unavailable, cannot process request\", \"Failed to initialize critical component\" Logging Implementation \u00b6 Configuration : Defined in app/logging_config.py and applied in app/__init__.py . Format : Primarily structured JSON via python-json-logger and a custom formatter. Context : A RequestContextFilter automatically injects request_id into logs generated during a request. Handlers : Configurable based on environment (Console, File, OTLP). Alert Triggers \u00b6 Alerts should be configured in your monitoring system (e.g., Prometheus Alertmanager, Grafana Alerts, Datadog Monitors) based on logs and metrics. Key triggers include: Trigger Condition Metric/Log Source Severity Example Action Service status changes to offline ServiceAvailability logs / Prometheus gauge High Page on-call engineer Circuit breaker enters OPEN state CircuitBreaker logs / Prometheus gauge Medium Notify development team channel High rate of specific error types opossum_errors_total Prometheus counter (rate) Medium Create ticket Sustained high error rate (any type) opossum_errors_total Prometheus counter (rate) High Page on-call engineer Approaching API rate limits ServiceAvailability logs / Prometheus gauge Low Log warning Hitting API rate limits ServiceAvailability logs / Prometheus gauge Medium Notify development team Critical log messages detected Log aggregation system query Critical Page on-call engineer High error handling duration opossum_error_duration_seconds Prometheus gauge Low Investigate performance Notification Channels \u00b6 Configure alerts to route to appropriate channels based on severity: Low : Logging systems, internal dashboards. Medium : Team chat channels (e.g., Slack, Teams). High/Critical : Paging systems (e.g., PagerDuty, Opsgenie), dedicated alert channels. Prometheus Metrics \u00b6 The ErrorHandler exposes the following metrics: opossum_errors_total{error_type} : Counter for total errors by ErrorCategory . opossum_error_duration_seconds{error_type} : Gauge showing the duration of the last error handling call for each ErrorCategory . Additional metrics are exposed by ServiceAvailability (see app/monitoring/availability.py ). Log Analysis \u00b6 Use a log aggregation tool (e.g., ELK Stack, Splunk, Grafana Loki, Datadog Logs) to: - Search and filter logs based on request_id , service , level , etc. - Create dashboards visualizing error rates and types. - Set up automated alerts based on log patterns.","title":"Logging and Alerts"},{"location":"service-availability/log-alerts/#logging-and-alerts","text":"Effective logging and alerting are crucial for monitoring service availability and diagnosing issues promptly.","title":"Logging and Alerts"},{"location":"service-availability/log-alerts/#logging-strategy","text":"The application uses Python's standard logging module, configured globally via logging.dictConfig during application startup ( app/__init__.py ). Log Category Purpose Implementation Details Retention Application Logs General operational events, debug info Structured JSON format (via CustomJsonFormatter ). Includes timestamp , level , message , logger , service_name , environment , request_id . Console output format varies by environment (simple for dev, JSON for prod). Configurable (e.g., file rotation, log aggregation system) Service Status Track service availability changes INFO level logs via ServiceAvailability module. JSON format. 30 days (example) Rate Limit Monitor API usage against quotas WARNING level logs when limits approached/hit. JSON format. 90 days (example) Error Events Record service failures and exceptions Handled by ErrorHandler . Logs exceptions with stack traces (ERROR/CRITICAL level). Includes error category and context. JSON format. 14 days (example) Recovery Actions Document automatic recovery attempts INFO/WARNING logs from CircuitBreaker (state changes) and retry decorator. JSON format. 14 days (example) Performance Metrics Track response times and latency Handled via Prometheus metrics (see below). 180 days (example) Security Events Audit security-related actions Dedicated 'security' logger, potentially separate handler. JSON format. See Security Model . 90+ days (example)","title":"Logging Strategy"},{"location":"service-availability/log-alerts/#log-levels","text":"Standard Python log levels are used: Level Usage Example DEBUG Detailed diagnostic information \"Checking Gemini API availability\", \"Extracted features: {...}\" INFO Normal operational events \"Service status changed: Gemini API now AVAILABLE\", \"Request processed successfully\" WARNING Non-critical issues, potential problems \"Approaching rate limit (85% used)\", \"Circuit breaker entering HALF_OPEN state\" ERROR Runtime errors, failed operations \"Connection to Ollama failed: Connection refused\", \"Error processing request: Validation Error\" CRITICAL System-wide failures \"All services unavailable, cannot process request\", \"Failed to initialize critical component\"","title":"Log Levels"},{"location":"service-availability/log-alerts/#logging-implementation","text":"Configuration : Defined in app/logging_config.py and applied in app/__init__.py . Format : Primarily structured JSON via python-json-logger and a custom formatter. Context : A RequestContextFilter automatically injects request_id into logs generated during a request. Handlers : Configurable based on environment (Console, File, OTLP).","title":"Logging Implementation"},{"location":"service-availability/log-alerts/#alert-triggers","text":"Alerts should be configured in your monitoring system (e.g., Prometheus Alertmanager, Grafana Alerts, Datadog Monitors) based on logs and metrics. Key triggers include: Trigger Condition Metric/Log Source Severity Example Action Service status changes to offline ServiceAvailability logs / Prometheus gauge High Page on-call engineer Circuit breaker enters OPEN state CircuitBreaker logs / Prometheus gauge Medium Notify development team channel High rate of specific error types opossum_errors_total Prometheus counter (rate) Medium Create ticket Sustained high error rate (any type) opossum_errors_total Prometheus counter (rate) High Page on-call engineer Approaching API rate limits ServiceAvailability logs / Prometheus gauge Low Log warning Hitting API rate limits ServiceAvailability logs / Prometheus gauge Medium Notify development team Critical log messages detected Log aggregation system query Critical Page on-call engineer High error handling duration opossum_error_duration_seconds Prometheus gauge Low Investigate performance","title":"Alert Triggers"},{"location":"service-availability/log-alerts/#notification-channels","text":"Configure alerts to route to appropriate channels based on severity: Low : Logging systems, internal dashboards. Medium : Team chat channels (e.g., Slack, Teams). High/Critical : Paging systems (e.g., PagerDuty, Opsgenie), dedicated alert channels.","title":"Notification Channels"},{"location":"service-availability/log-alerts/#prometheus-metrics","text":"The ErrorHandler exposes the following metrics: opossum_errors_total{error_type} : Counter for total errors by ErrorCategory . opossum_error_duration_seconds{error_type} : Gauge showing the duration of the last error handling call for each ErrorCategory . Additional metrics are exposed by ServiceAvailability (see app/monitoring/availability.py ).","title":"Prometheus Metrics"},{"location":"service-availability/log-alerts/#log-analysis","text":"Use a log aggregation tool (e.g., ELK Stack, Splunk, Grafana Loki, Datadog Logs) to: - Search and filter logs based on request_id , service , level , etc. - Create dashboards visualizing error rates and types. - Set up automated alerts based on log patterns.","title":"Log Analysis"},{"location":"service-availability/quality-requirements/","text":"Quality Requirements \u00b6 Availability Requirements \u00b6 Service Uptime Target Measurement Period Critical Hours Opossum Search Application 99.5% Monthly 24/7 Gemini API 98% Monthly Business hours Ollama Service 95% Weekly Business hours Transformers Fallback 99.9% Monthly 24/7 Note These availability requirements define the target uptime for each service, ensuring a reliable user experience. Performance Requirements \u00b6 Metric Target Description Service Check Response Time < 500ms Maximum time for a single service availability check Failover Detection Time < 2s Time to detect a service failure and initiate failover Failover Completion Time < 5s Time to complete transition to alternative service Service Status Cache Validity 30s Maximum age of cached service status information Recovery Time Objectives \u00b6 Scenario Recovery Time Objective (RTO) Recovery Point Objective (RPO) Gemini API Unavailable Immediate failover to Ollama No data loss Ollama Service Failure < 1 minute for auto-restart, immediate failover to Transformers No data loss All Remote Services Down < 10 seconds to activate offline mode Potential loss of latest model updates Logging and Monitoring Requirements \u00b6 Requirement Description Status Change Logging All service status changes must be logged with timestamp and reason Rate Limit Tracking Gemini API usage must be tracked with 99.99% accuracy Critical Alerts Service outages must trigger alerts within 30 seconds Availability Reports System must generate daily availability reports Quality Verification \u00b6 Verification Method Frequency Responsibility Availability Tests Daily automated tests CI/CD Pipeline Failover Tests Weekly Development Team Recovery Procedure Tests Monthly Operations Team Load Testing Quarterly QA Team","title":"Quality Requirements"},{"location":"service-availability/quality-requirements/#quality-requirements","text":"","title":"Quality Requirements"},{"location":"service-availability/quality-requirements/#availability-requirements","text":"Service Uptime Target Measurement Period Critical Hours Opossum Search Application 99.5% Monthly 24/7 Gemini API 98% Monthly Business hours Ollama Service 95% Weekly Business hours Transformers Fallback 99.9% Monthly 24/7 Note These availability requirements define the target uptime for each service, ensuring a reliable user experience.","title":"Availability Requirements"},{"location":"service-availability/quality-requirements/#performance-requirements","text":"Metric Target Description Service Check Response Time < 500ms Maximum time for a single service availability check Failover Detection Time < 2s Time to detect a service failure and initiate failover Failover Completion Time < 5s Time to complete transition to alternative service Service Status Cache Validity 30s Maximum age of cached service status information","title":"Performance Requirements"},{"location":"service-availability/quality-requirements/#recovery-time-objectives","text":"Scenario Recovery Time Objective (RTO) Recovery Point Objective (RPO) Gemini API Unavailable Immediate failover to Ollama No data loss Ollama Service Failure < 1 minute for auto-restart, immediate failover to Transformers No data loss All Remote Services Down < 10 seconds to activate offline mode Potential loss of latest model updates","title":"Recovery Time Objectives"},{"location":"service-availability/quality-requirements/#logging-and-monitoring-requirements","text":"Requirement Description Status Change Logging All service status changes must be logged with timestamp and reason Rate Limit Tracking Gemini API usage must be tracked with 99.99% accuracy Critical Alerts Service outages must trigger alerts within 30 seconds Availability Reports System must generate daily availability reports","title":"Logging and Monitoring Requirements"},{"location":"service-availability/quality-requirements/#quality-verification","text":"Verification Method Frequency Responsibility Availability Tests Daily automated tests CI/CD Pipeline Failover Tests Weekly Development Team Recovery Procedure Tests Monthly Operations Team Load Testing Quarterly QA Team","title":"Quality Verification"},{"location":"service-availability/rate-limiting-throttling/","text":"Rate Limiting and Throttling \u00b6 Related Documentation: - Technical: GraphQL API - Implementation of rate limiting in GraphQL directives - API Reference: Rate Limits - Detailed API rate limit reference documentation - Technical: Redis Caching Architecture - Redis-based rate limit tracking implementation Rate Limit Policies \u00b6 Service Rate Limit Type Quota Reset Period Priority Handling Gemini API API Calls 60 per minute Minute High-value queries prioritized Gemini API Daily Usage 60,000 per day 24 hours Resource allocation based on time of day Ollama Local Resource CPU/GPU dependent N/A Queue-based with timeout Transformers Local Resource Memory/CPU dependent N/A Simplified models for high load Detection Mechanisms \u00b6 Limit Type Detection Method Response Code Handling Strategy Pre-emptive Counter tracking N/A Redirect before limit reached Reactive HTTP 429 response 429 Too Many Requests Immediate failover to alternative service Quota Exceeded HTTP 403 response 403 Forbidden Temporary service downgrade Resource Exhaustion Exception/timeout Various Scale down model complexity Request Management \u00b6 Approach Implementation Benefit Request Queuing In-memory FIFO queue with priority Prevents request loss during high load Request Coalescing Combine similar requests Reduces total API calls Request Prioritization User interaction > Background tasks Maintains responsive UX Adaptive TTL Dynamic cache lifetime based on load Reduces API calls during peak Throttling Implementation \u00b6 class RateLimitManager : def __init__ ( self ): self . minute_usage = 0 self . daily_usage = 0 self . last_minute_reset = datetime . now () self . last_daily_reset = datetime . now () self . request_queue = asyncio . Queue () async def track_request ( self ): \"\"\"Track a new request against rate limits\"\"\" current_time = datetime . now () # Reset counters if needed if ( current_time - self . last_minute_reset ) . seconds >= 60 : self . minute_usage = 0 self . last_minute_reset = current_time if ( current_time - self . last_daily_reset ) . days >= 1 : self . daily_usage = 0 self . last_daily_reset = current_time # Increment counters self . minute_usage += 1 self . daily_usage += 1 async def can_process_request ( self ): \"\"\"Check if request can be processed within rate limits\"\"\" return self . minute_usage < 58 # Buffer of 2 requests async def process_or_queue ( self , request_func , * args ): \"\"\"Process request or queue it based on rate limits\"\"\" if await self . can_process_request (): await self . track_request () return await request_func ( * args ) else : # Queue the request or fail over return await self . handle_rate_limit ( * args ) Client-Side Adaptation \u00b6 Condition Client Behavior User Experience Server Rate Limited Switch to fallback simulation \"Using simplified mode temporarily\" Approaching Limits Batch requests Normal with slight delay Normal Operation Direct API access Full functionality Extended Outage Local-only operation Reduced capabilities with notification Tip Client-side adaptation ensures a smooth user experience even when the server is experiencing rate limits or extended outages. Balance and Optimization Strategies \u00b6 Strategy Implementation Effect Time-of-day Allocation Reserve quota for peak hours Consistent availability during business hours Request Complexity Analysis Measure token count before sending Route complex queries to appropriate backend Adaptive Backoff Exponential delay with jitter Graceful recovery during service degradation Quota Forecasting Predictive usage modeling Proactive service switching before limits reached Monitoring and Alerts \u00b6 Metric Threshold Alert Type Recipient Minute Usage >80% of limit Warning Logs Minute Usage >95% of limit Critical Operations Team Daily Usage >90% of limit Warning Operations Team Queue Size >20 requests Warning Logs Queue Size >50 requests Critical Operations Team Queue Wait Time >5 seconds Warning Logs, User Notification","title":"Rate Limiting"},{"location":"service-availability/rate-limiting-throttling/#rate-limiting-and-throttling","text":"Related Documentation: - Technical: GraphQL API - Implementation of rate limiting in GraphQL directives - API Reference: Rate Limits - Detailed API rate limit reference documentation - Technical: Redis Caching Architecture - Redis-based rate limit tracking implementation","title":"Rate Limiting and Throttling"},{"location":"service-availability/rate-limiting-throttling/#rate-limit-policies","text":"Service Rate Limit Type Quota Reset Period Priority Handling Gemini API API Calls 60 per minute Minute High-value queries prioritized Gemini API Daily Usage 60,000 per day 24 hours Resource allocation based on time of day Ollama Local Resource CPU/GPU dependent N/A Queue-based with timeout Transformers Local Resource Memory/CPU dependent N/A Simplified models for high load","title":"Rate Limit Policies"},{"location":"service-availability/rate-limiting-throttling/#detection-mechanisms","text":"Limit Type Detection Method Response Code Handling Strategy Pre-emptive Counter tracking N/A Redirect before limit reached Reactive HTTP 429 response 429 Too Many Requests Immediate failover to alternative service Quota Exceeded HTTP 403 response 403 Forbidden Temporary service downgrade Resource Exhaustion Exception/timeout Various Scale down model complexity","title":"Detection Mechanisms"},{"location":"service-availability/rate-limiting-throttling/#request-management","text":"Approach Implementation Benefit Request Queuing In-memory FIFO queue with priority Prevents request loss during high load Request Coalescing Combine similar requests Reduces total API calls Request Prioritization User interaction > Background tasks Maintains responsive UX Adaptive TTL Dynamic cache lifetime based on load Reduces API calls during peak","title":"Request Management"},{"location":"service-availability/rate-limiting-throttling/#throttling-implementation","text":"class RateLimitManager : def __init__ ( self ): self . minute_usage = 0 self . daily_usage = 0 self . last_minute_reset = datetime . now () self . last_daily_reset = datetime . now () self . request_queue = asyncio . Queue () async def track_request ( self ): \"\"\"Track a new request against rate limits\"\"\" current_time = datetime . now () # Reset counters if needed if ( current_time - self . last_minute_reset ) . seconds >= 60 : self . minute_usage = 0 self . last_minute_reset = current_time if ( current_time - self . last_daily_reset ) . days >= 1 : self . daily_usage = 0 self . last_daily_reset = current_time # Increment counters self . minute_usage += 1 self . daily_usage += 1 async def can_process_request ( self ): \"\"\"Check if request can be processed within rate limits\"\"\" return self . minute_usage < 58 # Buffer of 2 requests async def process_or_queue ( self , request_func , * args ): \"\"\"Process request or queue it based on rate limits\"\"\" if await self . can_process_request (): await self . track_request () return await request_func ( * args ) else : # Queue the request or fail over return await self . handle_rate_limit ( * args )","title":"Throttling Implementation"},{"location":"service-availability/rate-limiting-throttling/#client-side-adaptation","text":"Condition Client Behavior User Experience Server Rate Limited Switch to fallback simulation \"Using simplified mode temporarily\" Approaching Limits Batch requests Normal with slight delay Normal Operation Direct API access Full functionality Extended Outage Local-only operation Reduced capabilities with notification Tip Client-side adaptation ensures a smooth user experience even when the server is experiencing rate limits or extended outages.","title":"Client-Side Adaptation"},{"location":"service-availability/rate-limiting-throttling/#balance-and-optimization-strategies","text":"Strategy Implementation Effect Time-of-day Allocation Reserve quota for peak hours Consistent availability during business hours Request Complexity Analysis Measure token count before sending Route complex queries to appropriate backend Adaptive Backoff Exponential delay with jitter Graceful recovery during service degradation Quota Forecasting Predictive usage modeling Proactive service switching before limits reached","title":"Balance and Optimization Strategies"},{"location":"service-availability/rate-limiting-throttling/#monitoring-and-alerts","text":"Metric Threshold Alert Type Recipient Minute Usage >80% of limit Warning Logs Minute Usage >95% of limit Critical Operations Team Daily Usage >90% of limit Warning Operations Team Queue Size >20 requests Warning Logs Queue Size >50 requests Critical Operations Team Queue Wait Time >5 seconds Warning Logs, User Notification","title":"Monitoring and Alerts"},{"location":"service-availability/service-outage/","text":"Error Handling and Recovery \u00b6 Error Detection and Classification \u00b6 Error Type Detection Method Priority Example API Connection Failure Request timeout or HTTP error High Gemini API unreachable Rate Limit Reached HTTP 429 or quota tracking High Gemini API quota exceeded Authentication Failure HTTP 401/403 response High Invalid or expired API key Local Service Unreachable Socket connection failure Medium Ollama service not running Model Loading Failure Exception during model initialization Medium Insufficient resources for Transformers Slow Response Response time exceeding threshold Low Degraded performance warning Failover Strategy \u00b6 From Service To Service Trigger Transition Time Gemini API \u2192 Ollama Connection failure, rate limit, or auth error Immediate < 2s Ollama \u2192 Transformers Connection failure or initialization error Immediate < 5s Any \u2192 Client-side Fallback All server services unavailable After 3 retries < 10s Important The failover strategy ensures that the Opossum Search application remains functional even when individual services experience outages. Recovery Procedures \u00b6 Service Automatic Recovery Manual Recovery Steps Gemini API Periodic availability checks to detect when service is restored 1. Verify API key validity 2. Check quota status 3. Test connectivity to API endpoint 4. Update API configuration if needed Ollama Restart attempt after 60s of unavailability 1. Check local service process 2. Restart Ollama service 3. Verify model availability 4. Check GPU utilization if performance issues Transformers Model reloading attempt if initialization fails 1. Verify model files exist 2. Check available RAM 3. Consider loading smaller model variant 4. Update model files if corrupted Error Communication \u00b6 Audience Error Information Delivery Method End Users General error with degraded capability notice UI message: \"Using alternative model due to service availability\" Developers Detailed error including exception trace and service status Application logs with ERROR level Operations Service status changes and recovery attempts Logs and monitoring alerts Resilience Mechanisms \u00b6 Mechanism Purpose Implementation Circuit Breaker Prevent repeated calls to failing services Exponential backoff with jitter Request Caching Serve previous responses during outages In-memory cache with TTL Client-side Fallback Provide degraded functionality when server is unavailable JavaScript simulation mode in UI Service Prioritization Route requests to highest capability available service Service ranking with availability checks Graceful Degradation Maintain core functionality with limited capabilities Feature flags based on available services Recovery Monitoring \u00b6 Recovery Metric Measurement Threshold Action Recovery Time Time from failure to service restoration > RTO Alert operations team Failed Recovery Attempts Count of unsuccessful recovery attempts > 3 Escalate to manual intervention Failover Frequency Number of failovers in 24h period > 5 Investigate root cause Performance After Recovery Response time compared to baseline > 150% of baseline Flag for optimization Circuit Breaker Pattern \u00b6 Opossum implements a robust circuit breaker pattern to prevent cascading failures when services become unresponsive. The centralized implementation provides: Failure threshold detection : Configurable limits for consecutive failures Rate-based detection : Optional percentage-based failure detection for high-volume services Automatic recovery : Half-open state testing to detect when services recover Prometheus metrics : Comprehensive monitoring of circuit state changes Named instances : Service-specific circuit breakers with individual configurations Circuit breakers are managed centrally by the ErrorHandler and can be configured through environment variables: Configuration Parameter Description Default CIRCUIT_BREAKER_FAILURE_THRESHOLD Default failure count threshold 5 CIRCUIT_BREAKER_RESET_TIMEOUT Default seconds before retry 60 {SERVICE}_FAILURE_THRESHOLD Service-specific threshold Default threshold {SERVICE}_RESET_TIMEOUT Service-specific timeout Default timeout When a circuit opens, requests will be routed to fallback mechanisms or return appropriate error responses to clients.","title":"Error Handling"},{"location":"service-availability/service-outage/#error-handling-and-recovery","text":"","title":"Error Handling and Recovery"},{"location":"service-availability/service-outage/#error-detection-and-classification","text":"Error Type Detection Method Priority Example API Connection Failure Request timeout or HTTP error High Gemini API unreachable Rate Limit Reached HTTP 429 or quota tracking High Gemini API quota exceeded Authentication Failure HTTP 401/403 response High Invalid or expired API key Local Service Unreachable Socket connection failure Medium Ollama service not running Model Loading Failure Exception during model initialization Medium Insufficient resources for Transformers Slow Response Response time exceeding threshold Low Degraded performance warning","title":"Error Detection and Classification"},{"location":"service-availability/service-outage/#failover-strategy","text":"From Service To Service Trigger Transition Time Gemini API \u2192 Ollama Connection failure, rate limit, or auth error Immediate < 2s Ollama \u2192 Transformers Connection failure or initialization error Immediate < 5s Any \u2192 Client-side Fallback All server services unavailable After 3 retries < 10s Important The failover strategy ensures that the Opossum Search application remains functional even when individual services experience outages.","title":"Failover Strategy"},{"location":"service-availability/service-outage/#recovery-procedures","text":"Service Automatic Recovery Manual Recovery Steps Gemini API Periodic availability checks to detect when service is restored 1. Verify API key validity 2. Check quota status 3. Test connectivity to API endpoint 4. Update API configuration if needed Ollama Restart attempt after 60s of unavailability 1. Check local service process 2. Restart Ollama service 3. Verify model availability 4. Check GPU utilization if performance issues Transformers Model reloading attempt if initialization fails 1. Verify model files exist 2. Check available RAM 3. Consider loading smaller model variant 4. Update model files if corrupted","title":"Recovery Procedures"},{"location":"service-availability/service-outage/#error-communication","text":"Audience Error Information Delivery Method End Users General error with degraded capability notice UI message: \"Using alternative model due to service availability\" Developers Detailed error including exception trace and service status Application logs with ERROR level Operations Service status changes and recovery attempts Logs and monitoring alerts","title":"Error Communication"},{"location":"service-availability/service-outage/#resilience-mechanisms","text":"Mechanism Purpose Implementation Circuit Breaker Prevent repeated calls to failing services Exponential backoff with jitter Request Caching Serve previous responses during outages In-memory cache with TTL Client-side Fallback Provide degraded functionality when server is unavailable JavaScript simulation mode in UI Service Prioritization Route requests to highest capability available service Service ranking with availability checks Graceful Degradation Maintain core functionality with limited capabilities Feature flags based on available services","title":"Resilience Mechanisms"},{"location":"service-availability/service-outage/#recovery-monitoring","text":"Recovery Metric Measurement Threshold Action Recovery Time Time from failure to service restoration > RTO Alert operations team Failed Recovery Attempts Count of unsuccessful recovery attempts > 3 Escalate to manual intervention Failover Frequency Number of failovers in 24h period > 5 Investigate root cause Performance After Recovery Response time compared to baseline > 150% of baseline Flag for optimization","title":"Recovery Monitoring"},{"location":"service-availability/service-outage/#circuit-breaker-pattern","text":"Opossum implements a robust circuit breaker pattern to prevent cascading failures when services become unresponsive. The centralized implementation provides: Failure threshold detection : Configurable limits for consecutive failures Rate-based detection : Optional percentage-based failure detection for high-volume services Automatic recovery : Half-open state testing to detect when services recover Prometheus metrics : Comprehensive monitoring of circuit state changes Named instances : Service-specific circuit breakers with individual configurations Circuit breakers are managed centrally by the ErrorHandler and can be configured through environment variables: Configuration Parameter Description Default CIRCUIT_BREAKER_FAILURE_THRESHOLD Default failure count threshold 5 CIRCUIT_BREAKER_RESET_TIMEOUT Default seconds before retry 60 {SERVICE}_FAILURE_THRESHOLD Service-specific threshold Default threshold {SERVICE}_RESET_TIMEOUT Service-specific timeout Default timeout When a circuit opens, requests will be routed to fallback mechanisms or return appropriate error responses to clients.","title":"Circuit Breaker Pattern"},{"location":"service-availability/testing-validation/","text":"Testing and Validation - Service Availability \u00b6 Testing Strategy \u00b6 Test Type Purpose Frequency Implementation Unit Tests Verify individual components function correctly Per commit Pytest for Python components Integration Tests Verify service interaction and failover logic Daily Automated test suite with service mocks End-to-End Tests Verify complete system behavior Weekly Real-world scenarios with actual services Chaos Tests Verify resilience during unexpected failures Monthly Random service disruption testing Load Tests Verify behavior under high throughput Quarterly Simulated high volume request patterns Test Scenarios \u00b6 Scenario Test Case Validation Criteria Gemini Unavailability Simulate API timeout Successful failover to Ollama within 2s Rate Limit Exceeded Generate high request volume Preemptive failover before 429 error Authentication Failure Use invalid API key Correct error handling and failover Intermittent Failures Random request failures Circuit breaker activation after threshold Slow Response Delayed API responses Timeout detection and service degradation Recovery Detection Restore service after outage Return to primary service within check interval Validation Methods \u00b6 Method Description Tools Metrics Availability Metrics Measure uptime percentage Custom metrics collector 99.5% target uptime Failover Success Rate Measure successful transitions Test harness logs >99% success target Response Time Validation Measure end-to-end latency Request timing <5s during failover User Experience Assessment Evaluate quality of fallback responses Subjective scoring Minimal degradation Recovery Time Validation Measure time to restore optimal service Test harness logs Within RTO targets Testing Infrastructure \u00b6 Component Purpose Implementation Mock Services Simulate API responses and failures Pytest fixtures and mock HTTP servers Rate Limit Simulator Test behavior near quota limits Counter manipulation and response code injection Network Condition Simulator Test with varied connectivity Proxy with configurable delays and failures Test Harness Coordinate and execute test suites Pytest with custom plugins CI/CD Integration Automate testing on changes GitHub Actions workflows Testing Implementation \u00b6 # Example test case for failover behavior import pytest import asyncio from unittest.mock import patch , MagicMock class TestServiceFailover : @pytest . mark . asyncio async def test_gemini_to_ollama_failover ( self , availability_manager , service_router ): # Arrange - Mock Gemini as unavailable with patch . object ( availability_manager , 'get_available_services' ) as mock_get : mock_get . return_value = { \"ollama\" , \"transformers\" } # Gemini not available # Act - Attempt to route a request result = await service_router . route_request ({ \"query\" : \"test question\" }) # Assert - Request was handled by Ollama assert result [ \"fallback_used\" ] == \"ollama\" assert \"response\" in result Validation Dashboard \u00b6 Metric Visualization Threshold Alerts Service Uptime Time-series graph <99.5% Email to operations Failover Events Count and distribution >5 per day Slack notification Average Response Time Time-series by service >2s baseline Warning in dashboard Error Rate Percentage by service >1% Critical alert Fallback Distribution Pie chart of service usage >20% non-primary Weekly report Note Pay close attention to the Service Uptime and Error Rate metrics in the Validation Dashboard. These are key indicators of overall service availability. Continuous Testing \u00b6 Practice Implementation Frequency Responsible Team Automated Test Suite Full test coverage in CI pipeline Every PR Development Synthetic Monitoring Regular health checks from external locations Every 5 minutes Operations Regression Testing Verify fixed issues don't recur Every release QA Scheduled Chaos Tests Planned service disruptions Weekly off-hours SRE User Journey Tests End-to-end experience validation Bi-weekly QA","title":"Testing and Validation"},{"location":"service-availability/testing-validation/#testing-and-validation-service-availability","text":"","title":"Testing and Validation - Service Availability"},{"location":"service-availability/testing-validation/#testing-strategy","text":"Test Type Purpose Frequency Implementation Unit Tests Verify individual components function correctly Per commit Pytest for Python components Integration Tests Verify service interaction and failover logic Daily Automated test suite with service mocks End-to-End Tests Verify complete system behavior Weekly Real-world scenarios with actual services Chaos Tests Verify resilience during unexpected failures Monthly Random service disruption testing Load Tests Verify behavior under high throughput Quarterly Simulated high volume request patterns","title":"Testing Strategy"},{"location":"service-availability/testing-validation/#test-scenarios","text":"Scenario Test Case Validation Criteria Gemini Unavailability Simulate API timeout Successful failover to Ollama within 2s Rate Limit Exceeded Generate high request volume Preemptive failover before 429 error Authentication Failure Use invalid API key Correct error handling and failover Intermittent Failures Random request failures Circuit breaker activation after threshold Slow Response Delayed API responses Timeout detection and service degradation Recovery Detection Restore service after outage Return to primary service within check interval","title":"Test Scenarios"},{"location":"service-availability/testing-validation/#validation-methods","text":"Method Description Tools Metrics Availability Metrics Measure uptime percentage Custom metrics collector 99.5% target uptime Failover Success Rate Measure successful transitions Test harness logs >99% success target Response Time Validation Measure end-to-end latency Request timing <5s during failover User Experience Assessment Evaluate quality of fallback responses Subjective scoring Minimal degradation Recovery Time Validation Measure time to restore optimal service Test harness logs Within RTO targets","title":"Validation Methods"},{"location":"service-availability/testing-validation/#testing-infrastructure","text":"Component Purpose Implementation Mock Services Simulate API responses and failures Pytest fixtures and mock HTTP servers Rate Limit Simulator Test behavior near quota limits Counter manipulation and response code injection Network Condition Simulator Test with varied connectivity Proxy with configurable delays and failures Test Harness Coordinate and execute test suites Pytest with custom plugins CI/CD Integration Automate testing on changes GitHub Actions workflows","title":"Testing Infrastructure"},{"location":"service-availability/testing-validation/#testing-implementation","text":"# Example test case for failover behavior import pytest import asyncio from unittest.mock import patch , MagicMock class TestServiceFailover : @pytest . mark . asyncio async def test_gemini_to_ollama_failover ( self , availability_manager , service_router ): # Arrange - Mock Gemini as unavailable with patch . object ( availability_manager , 'get_available_services' ) as mock_get : mock_get . return_value = { \"ollama\" , \"transformers\" } # Gemini not available # Act - Attempt to route a request result = await service_router . route_request ({ \"query\" : \"test question\" }) # Assert - Request was handled by Ollama assert result [ \"fallback_used\" ] == \"ollama\" assert \"response\" in result","title":"Testing Implementation"},{"location":"service-availability/testing-validation/#validation-dashboard","text":"Metric Visualization Threshold Alerts Service Uptime Time-series graph <99.5% Email to operations Failover Events Count and distribution >5 per day Slack notification Average Response Time Time-series by service >2s baseline Warning in dashboard Error Rate Percentage by service >1% Critical alert Fallback Distribution Pie chart of service usage >20% non-primary Weekly report Note Pay close attention to the Service Uptime and Error Rate metrics in the Validation Dashboard. These are key indicators of overall service availability.","title":"Validation Dashboard"},{"location":"service-availability/testing-validation/#continuous-testing","text":"Practice Implementation Frequency Responsible Team Automated Test Suite Full test coverage in CI pipeline Every PR Development Synthetic Monitoring Regular health checks from external locations Every 5 minutes Operations Regression Testing Verify fixed issues don't recur Every release QA Scheduled Chaos Tests Planned service disruptions Weekly off-hours SRE User Journey Tests End-to-end experience validation Bi-weekly QA","title":"Continuous Testing"},{"location":"technical/bot-user-simulation/","text":"Bot User Simulation Framework \u00b6 Overview \u00b6 The Bot User Simulation Framework provides automated testing capabilities for Opossum Search by simulating realistic user behavior. This framework is particularly useful for testing: Resilience - How the system handles high load, service failures, and error cases Special Features - Testing date-specific Easter eggs and hidden commands Model Selection - Verifying that the hybrid model selection system routes queries correctly Bot Types \u00b6 The framework includes several types of bot users: Bot Type Description Main Use Cases BotUser Standard bot that sends chat messages General testing, resilience testing TimeBasedBotUser Bot that simulates specific dates Testing date-based features like National Opossum Day NationalOpossumDayTester Specialized bot for Oct 18 features Comprehensive testing of National Opossum Day ModelSelectionBot Analyzes model selection patterns Testing the hybrid model routing system Behavior Profiles \u00b6 Bots can be configured with different behavior profiles to simulate various user patterns: standard - Normal user behavior with standard queries aggressive - Rapid-fire requests with minimal delays error_prone - Sends malformed or potentially malicious queries easter_egg_hunter - Specifically tests Easter egg commands and special features Query Sets \u00b6 Each bot is pre-configured with different sets of queries: standard - Common opossum-related questions complex_reasoning - Questions requiring deeper analysis error_prone - Malformed or potentially harmful inputs easter_eggs - Known Easter egg commands and triggers Usage Examples \u00b6 Basic Bot Testing \u00b6 import asyncio from tests.bots.bot_user import BotUser async def test_basic_interaction (): bot = BotUser ( base_url = \"http://localhost:5000\" ) response = await bot . send_chat_message ( \"Tell me about opossums\" ) print ( f \"Response: { response . get ( 'response' ) } \" ) # Run a complete session with 5 messages stats = await bot . run_session ( num_messages = 5 ) print ( f \"Session stats: { stats } \" ) Testing National Opossum Day \u00b6 import asyncio from datetime import date from tests.bots.bot_user import NationalOpossumDayTester async def test_national_opossum_day (): # Tests with simulated date of October 18 bot = NationalOpossumDayTester ( base_url = \"http://localhost:5000\" ) results = await bot . test_national_opossum_day_features () # Check which features were detected print ( f \"Features detected: { results [ 'features_detected_count' ] } /4\" ) print ( f \"Features: { results [ 'special_features_detected' ] } \" ) Simulating Multiple Users \u00b6 import asyncio from tests.bots.bot_user import ConcurrentBotSimulation async def run_load_test (): # Create a simulation with 10 bots using different profiles simulation = ConcurrentBotSimulation ( base_url = \"http://localhost:5000\" , num_bots = 10 , behavior_profiles = [ \"standard\" , \"aggressive\" , \"error_prone\" ] ) # Run with 5 messages per bot, 5 bots at a time results = await simulation . run_simulation ( messages_per_bot = 5 , max_concurrency = 5 ) print ( f \"Success rate: { results [ 'success_rate' ] } \" ) print ( f \"Avg response time: { results [ 'overall_avg_response_time' ] } s\" ) Testing Model Selection \u00b6 The framework includes specialized tools for testing that queries are routed to the appropriate models: import asyncio from tests.bots.test_model_selection import ModelSelectionBot async def test_query_routing (): bot = ModelSelectionBot ( base_url = \"http://localhost:5000\" ) # Test complex reasoning queries reasoning_queries = [ \"Compare opossums to raccoons\" , \"Explain why opossums are beneficial to have around\" ] results = await bot . test_query_routing ( \"reasoning\" , reasoning_queries ) print ( f \"Model selection: { results [ 'counts' ] } \" ) Fixture Integration \u00b6 The bot framework integrates with pytest fixtures for easier test creation: base_url - Provides the API URL for testing force_service_unavailability - Temporarily makes services unavailable for testing fallback capture_model_selections - Records model selection patterns run_concurrent_bots - Runs multiple bots with configurable parameters Example usage with fixtures: import pytest @pytest . mark . asyncio async def test_model_fallback ( base_url , force_service_unavailability ): # Temporarily make Gemini unavailable force_service_unavailability ( \"gemini\" ) # Bot should now use fallback models bot = BotUser ( base_url = base_url ) response = await bot . send_chat_message ( \"Analyze the benefits of opossums\" ) # Then restore service force_service_unavailability ( \"gemini\" , make_unavailable = False ) Running the Test Suite \u00b6 To run the full bot user test suite: pytest tests/bots/ -v To run specific test types: # Test resilience pytest tests/bots/test_resilience.py -v # Test Easter eggs pytest tests/bots/test_easter_eggs.py -v # Test model selection pytest tests/bots/test_model_selection.py -v Extending the Framework \u00b6 The bot simulation framework is designed to be extensible. To create a new specialized bot: Subclass one of the existing bot classes Override methods as needed for specialized behavior Add new query types or behavior patterns Example: class CustomBot ( BotUser ): def __init__ ( self , base_url , ** kwargs ): super () . __init__ ( base_url , ** kwargs ) self . custom_feature_detected = False async def analyze_response ( self , response ): # Custom analysis logic if \"specific pattern\" in response . get ( \"response\" , \"\" ): self . custom_feature_detected = True return response Performance Considerations \u00b6 Bot users create real load on the system Consider running resource-intensive tests in isolation For load testing, start with small numbers of bots and increase gradually Set reasonable message delays for realistic traffic patterns","title":"Bot User Simulation"},{"location":"technical/bot-user-simulation/#bot-user-simulation-framework","text":"","title":"Bot User Simulation Framework"},{"location":"technical/bot-user-simulation/#overview","text":"The Bot User Simulation Framework provides automated testing capabilities for Opossum Search by simulating realistic user behavior. This framework is particularly useful for testing: Resilience - How the system handles high load, service failures, and error cases Special Features - Testing date-specific Easter eggs and hidden commands Model Selection - Verifying that the hybrid model selection system routes queries correctly","title":"Overview"},{"location":"technical/bot-user-simulation/#bot-types","text":"The framework includes several types of bot users: Bot Type Description Main Use Cases BotUser Standard bot that sends chat messages General testing, resilience testing TimeBasedBotUser Bot that simulates specific dates Testing date-based features like National Opossum Day NationalOpossumDayTester Specialized bot for Oct 18 features Comprehensive testing of National Opossum Day ModelSelectionBot Analyzes model selection patterns Testing the hybrid model routing system","title":"Bot Types"},{"location":"technical/bot-user-simulation/#behavior-profiles","text":"Bots can be configured with different behavior profiles to simulate various user patterns: standard - Normal user behavior with standard queries aggressive - Rapid-fire requests with minimal delays error_prone - Sends malformed or potentially malicious queries easter_egg_hunter - Specifically tests Easter egg commands and special features","title":"Behavior Profiles"},{"location":"technical/bot-user-simulation/#query-sets","text":"Each bot is pre-configured with different sets of queries: standard - Common opossum-related questions complex_reasoning - Questions requiring deeper analysis error_prone - Malformed or potentially harmful inputs easter_eggs - Known Easter egg commands and triggers","title":"Query Sets"},{"location":"technical/bot-user-simulation/#usage-examples","text":"","title":"Usage Examples"},{"location":"technical/bot-user-simulation/#basic-bot-testing","text":"import asyncio from tests.bots.bot_user import BotUser async def test_basic_interaction (): bot = BotUser ( base_url = \"http://localhost:5000\" ) response = await bot . send_chat_message ( \"Tell me about opossums\" ) print ( f \"Response: { response . get ( 'response' ) } \" ) # Run a complete session with 5 messages stats = await bot . run_session ( num_messages = 5 ) print ( f \"Session stats: { stats } \" )","title":"Basic Bot Testing"},{"location":"technical/bot-user-simulation/#testing-national-opossum-day","text":"import asyncio from datetime import date from tests.bots.bot_user import NationalOpossumDayTester async def test_national_opossum_day (): # Tests with simulated date of October 18 bot = NationalOpossumDayTester ( base_url = \"http://localhost:5000\" ) results = await bot . test_national_opossum_day_features () # Check which features were detected print ( f \"Features detected: { results [ 'features_detected_count' ] } /4\" ) print ( f \"Features: { results [ 'special_features_detected' ] } \" )","title":"Testing National Opossum Day"},{"location":"technical/bot-user-simulation/#simulating-multiple-users","text":"import asyncio from tests.bots.bot_user import ConcurrentBotSimulation async def run_load_test (): # Create a simulation with 10 bots using different profiles simulation = ConcurrentBotSimulation ( base_url = \"http://localhost:5000\" , num_bots = 10 , behavior_profiles = [ \"standard\" , \"aggressive\" , \"error_prone\" ] ) # Run with 5 messages per bot, 5 bots at a time results = await simulation . run_simulation ( messages_per_bot = 5 , max_concurrency = 5 ) print ( f \"Success rate: { results [ 'success_rate' ] } \" ) print ( f \"Avg response time: { results [ 'overall_avg_response_time' ] } s\" )","title":"Simulating Multiple Users"},{"location":"technical/bot-user-simulation/#testing-model-selection","text":"The framework includes specialized tools for testing that queries are routed to the appropriate models: import asyncio from tests.bots.test_model_selection import ModelSelectionBot async def test_query_routing (): bot = ModelSelectionBot ( base_url = \"http://localhost:5000\" ) # Test complex reasoning queries reasoning_queries = [ \"Compare opossums to raccoons\" , \"Explain why opossums are beneficial to have around\" ] results = await bot . test_query_routing ( \"reasoning\" , reasoning_queries ) print ( f \"Model selection: { results [ 'counts' ] } \" )","title":"Testing Model Selection"},{"location":"technical/bot-user-simulation/#fixture-integration","text":"The bot framework integrates with pytest fixtures for easier test creation: base_url - Provides the API URL for testing force_service_unavailability - Temporarily makes services unavailable for testing fallback capture_model_selections - Records model selection patterns run_concurrent_bots - Runs multiple bots with configurable parameters Example usage with fixtures: import pytest @pytest . mark . asyncio async def test_model_fallback ( base_url , force_service_unavailability ): # Temporarily make Gemini unavailable force_service_unavailability ( \"gemini\" ) # Bot should now use fallback models bot = BotUser ( base_url = base_url ) response = await bot . send_chat_message ( \"Analyze the benefits of opossums\" ) # Then restore service force_service_unavailability ( \"gemini\" , make_unavailable = False )","title":"Fixture Integration"},{"location":"technical/bot-user-simulation/#running-the-test-suite","text":"To run the full bot user test suite: pytest tests/bots/ -v To run specific test types: # Test resilience pytest tests/bots/test_resilience.py -v # Test Easter eggs pytest tests/bots/test_easter_eggs.py -v # Test model selection pytest tests/bots/test_model_selection.py -v","title":"Running the Test Suite"},{"location":"technical/bot-user-simulation/#extending-the-framework","text":"The bot simulation framework is designed to be extensible. To create a new specialized bot: Subclass one of the existing bot classes Override methods as needed for specialized behavior Add new query types or behavior patterns Example: class CustomBot ( BotUser ): def __init__ ( self , base_url , ** kwargs ): super () . __init__ ( base_url , ** kwargs ) self . custom_feature_detected = False async def analyze_response ( self , response ): # Custom analysis logic if \"specific pattern\" in response . get ( \"response\" , \"\" ): self . custom_feature_detected = True return response","title":"Extending the Framework"},{"location":"technical/bot-user-simulation/#performance-considerations","text":"Bot users create real load on the system Consider running resource-intensive tests in isolation For load testing, start with small numbers of bots and increase gradually Set reasonable message delays for realistic traffic patterns","title":"Performance Considerations"},{"location":"technical/devops-guide/","text":"Technical Documentation: Deployment & Operations Guide \u00b6 1. Deployment Overview \u00b6 This guide outlines the processes and best practices for deploying, configuring, monitoring, and maintaining Opossum Search in various environments. 2. Environment Setup \u00b6 2.1 System Requirements \u00b6 Component Minimum Recommended Notes CPU 2 cores 4+ cores 8+ cores for high-volume production RAM 8GB 16GB 32GB+ for multiple local models Storage 5GB 20GB SSD recommended for model loading Network 10Mbps 100Mbps+ Reliable connection for external APIs GPU None NVIDIA with 8GB+ VRAM For Ollama performance 2.2 Software Dependencies \u00b6 # Install system dependencies sudo apt-get update sudo apt-get install -y \\ [ python3.11 ]( https://docs.python.org/3.11/ ) \\ python3.11-dev \\ python3-pip \\ python3-venv \\ redis-server # [Redis](https://redis.io/) in-memory data structure store imagemagick # [ImageMagick](https://imagemagick.org/index.php) for image processing libmagickwand-dev build-essential # Install NVIDIA drivers and CUDA toolkit (for GPU support) # Skip this section if not using GPU sudo apt-get install -y nvidia-driver-535 nvidia-cuda-toolkit 2.3 Environment Configurations \u00b6 Environment Description Use Case development Local setup for developers Feature development, testing staging Production-like environment Integration testing, pre-release validation production Live deployment End-user service 3. Installation Procedures \u00b6 3.1 Docker-based Deployment (Recommended) \u00b6 # Clone repository git clone https://github.com/yourusername/opossum-search.git cd opossum-search # Setup environment file cp .env.example .env # Edit .env file with appropriate settings # Start with Docker Compose docker-compose up -d Docker Compose File \u00b6 # docker-compose.yml version : '3.8' services : app : build : context : . dockerfile : Dockerfile ports : - \"8000:8000\" environment : - REDIS_HOST=redis - ENV=production - GEMINI_API_KEY=${GEMINI_API_KEY} - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318/v1/traces depends_on : - redis - ollama - otel-collector volumes : - ./models:/app/models - ./config:/app/config redis : image : redis:7.0-alpine ports : - \"6379:6379\" command : redis-server --appendonly yes volumes : - redis-data:/data ollama : image : ollama/ollama:latest ports : - \"11434:11434\" volumes : - ollama-models:/root/.ollama deploy : resources : reservations : devices : - driver : nvidia count : 1 capabilities : [ gpu ] otel-collector : image : otel/opentelemetry-collector:0.97.0 command : [ \"--config=/etc/otel-config.yaml\" ] volumes : - ./otel-config.yaml:/etc/otel-config.yaml ports : - \"4318:4318\" - \"9464:9464\" jaeger : image : jaegertracing/all-in-one:latest ports : - \"16686:16686\" - \"14250:14250\" volumes : redis-data : ollama-models : 3.2 Kubernetes Deployment \u00b6 # app-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : opossum-search spec : replicas : 3 selector : matchLabels : app : opossum-search template : metadata : labels : app : opossum-search spec : containers : - name : opossum-search image : opossum-search:latest ports : - containerPort : 8000 env : - name : REDIS_HOST value : redis-service - name : ENV value : production - name : GEMINI_API_KEY valueFrom : secretKeyRef : name : api-keys key : gemini-api-key resources : requests : memory : \"512Mi\" cpu : \"500m\" limits : memory : \"2Gi\" cpu : \"2000m\" 3.3 Manual Installation \u00b6 # Clone repository git clone https://github.com/yourusername/opossum-search.git cd opossum-search # Create virtual environment python -m venv venv source venv/bin/activate # Install dependencies pip install -r requirements.txt # Setup environment cp .env.example .env # Edit .env file with appropriate settings # Run application python -m app 4. Configuration Management \u00b6 Effective configuration is crucial for adapting Opossum Search to different environments ( development , staging , production ). Configuration is primarily managed through environment variables and Python configuration files ( config/*.py ). 4.1 Environment Variables \u00b6 Variable Purpose Example ENV Deployment environment production DEBUG Enable debug mode False GEMINI_API_KEY API key for Gemini your-api-key REDIS_HOST Redis server hostname redis REDIS_PORT Redis server port 6379 REDIS_PASSWORD Redis password secure-password OLLAMA_BASE_URL Ollama server URL http://ollama:11434 OTEL_EXPORTER_OTLP_ENDPOINT OpenTelemetry collector http://otel-collector:4318 FLASK_APP Flask application entry point app.py FLASK_ENV Flask environment production DATABASE_URL Database connection URL postgres://user:password@host/db SECRET_KEY Secret key for Flask application your-secret-key LOG_LEVEL Logging level INFO OTEL_ENABLED Enable OpenTelemetry True OTEL_SERVICE_NAME OpenTelemetry service name opossum-search 4.2 Configuration Files \u00b6 Environment-specific Python files (e.g., config/development.py , config/production.py ) inherit from a base Config class and override settings. Key Configuration Settings: Logging: LOG_LEVEL : Sets the logging level (e.g., 'DEBUG', 'INFO', 'WARNING'). OpenTelemetry: OTEL_ENABLED : Boolean to enable/disable OpenTelemetry integration. OTEL_SERVICE_NAME : Name reported to the OTLP collector. OTEL_EXPORTER_OTLP_ENDPOINT : URL of the OTLP collector. Circuit Breaker Settings: CIRCUIT_BREAKER_FAILURE_THRESHOLD : Default failure count before opening (e.g., 5). CIRCUIT_BREAKER_RESET_TIMEOUT : Default seconds before attempting recovery (e.g., 60). [SERVICE_NAME]_CIRCUIT_BREAKER_ENABLED : Boolean to enable/disable breaker for 'gemini', 'ollama', 'transformers'. Defaults to True for external, False for local transformers . [SERVICE_NAME]_FAILURE_THRESHOLD : Service-specific failure threshold override. [SERVICE_NAME]_RESET_TIMEOUT : Service-specific reset timeout override. Retry Policy Settings: DEFAULT_MAX_RETRIES : Default maximum retry attempts for services (e.g., 3). DEFAULT_RETRY_DELAY : Default base delay (seconds) between retries (e.g., 1.0). [SERVICE_NAME]_MAX_RETRIES : Service-specific max retries override (e.g., 'gemini', 'ollama'). Defaults to 0 for transformers . [SERVICE_NAME]_RETRY_DELAY : Service-specific base delay override. API Keys & Endpoints: GEMINI_API_KEY , OLLAMA_HEALTH_URL , etc. Rate Limits: GEMINI_DAILY_LIMIT , GEMINI_RPM_LIMIT , etc. Model Settings: MAX_TOKENS , TEMPERATURE , TOP_P , TOP_K . USE_QUANTIZED_MODELS , MODEL_PRECISION , QUANTIZED_MODEL_DIR . Other: REQUEST_TIMEOUT , TRANSFORMERS_WORKERS , AVAILABILITY_CHECK_INTERVAL . Refer to app/config.py for the full list and default values. 4.3 Secrets Management \u00b6 API keys and other sensitive credentials should never be hardcoded in configuration files. Use environment variables or a dedicated secrets management system (like HashiCorp Vault, AWS Secrets Manager, etc.) accessed during application startup. The Config class typically reads these from the environment. # Create Kubernetes secrets kubectl create secret generic api-keys \\ --from-literal = gemini-api-key = your-gemini-api-key \\ --from-literal = redis-password = your-redis-password 5. Scaling Strategies \u00b6 5.1 Horizontal Scaling \u00b6 # Kubernetes HPA configuration apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : opossum-search-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : opossum-search minReplicas : 2 maxReplicas : 10 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 70 - type : Resource resource : name : memory target : type : Utilization averageUtilization : 80 5.2 Redis Scaling \u00b6 Single Instance Configuration \u00b6 # redis.conf for single instance maxmemory 1gb maxmemory-policy allkeys-lru appendonly yes Redis Sentinel Configuration \u00b6 # sentinel.conf sentinel monitor mymaster redis-master 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 60000 sentinel parallel-syncs mymaster 1 5.3 Model Optimization for Scale \u00b6 # Example model optimization settings for scaled deployments MODEL_CONFIGS = { \"gemini-thinking\" : { \"api_name\" : \"gemini-1.5-pro\" , \"max_tokens\" : 1024 , \"temperature\" : 0.7 , \"top_p\" : 0.95 , }, \"gemma\" : { \"top_k\" : 40 , \"temperature\" : 0.8 , \"response_format\" : { \"type\" : \"text\" }, }, \"transformers\" : { \"transformers_name\" : \"google/gemma-2b\" , \"max_length\" : 512 , \"device_map\" : \"auto\" , # Automatically select best device } } 6. Monitoring and Alerting \u00b6 6.1 Health Checks \u00b6 # app/api/health.py @app . get ( \"/health\" ) async def health_check (): \"\"\"System health check\"\"\" status = { \"status\" : \"ok\" , \"version\" : Config . VERSION , \"timestamp\" : datetime . datetime . now () . isoformat (), \"services\" : {} } # Check Redis try : redis_ping = await redis_client . ping () status [ \"services\" ][ \"redis\" ] = \"ok\" if redis_ping else \"error\" except Exception as e : status [ \"services\" ][ \"redis\" ] = f \"error: { str ( e ) } \" # Check Ollama try : async with httpx . AsyncClient () as client : response = await client . get ( f \" { Config . OLLAMA_BASE_URL } /api/tags\" ) status [ \"services\" ][ \"ollama\" ] = \"ok\" if response . status_code == 200 else \"error\" except Exception as e : status [ \"services\" ][ \"ollama\" ] = f \"error: { str ( e ) } \" # Check model availability service_monitor = ServiceMonitor () await service_monitor . check_all_services () status [ \"services\" ][ \"model_backends\" ] = { k : \"available\" if v else \"unavailable\" for k , v in service_monitor . services . items () } # Set overall status if any ( v != \"ok\" and not v . startswith ( \"available\" ) for v in [ * status [ \"services\" ] . values (), * status [ \"services\" ][ \"model_backends\" ] . values ()]): status [ \"status\" ] = \"degraded\" return status 6.2 Prometheus Metrics \u00b6 # app/monitoring/metrics.py from prometheus_client import Counter , Histogram , Gauge # Define metrics REQUESTS_TOTAL = Counter ( \"opossum_requests_total\" , \"Total number of requests\" , [ \"method\" , \"endpoint\" ] ) RESPONSE_TIME = Histogram ( \"opossum_response_time_seconds\" , \"Response time in seconds\" , [ \"method\" , \"endpoint\" ] ) MODEL_USAGE = Counter ( \"opossum_model_usage_total\" , \"Total model usage count\" , [ \"model_name\" ] ) CACHE_HITS = Counter ( \"opossum_cache_hits_total\" , \"Total cache hits\" , [ \"cache_type\" ] ) CACHE_MISSES = Counter ( \"opossum_cache_misses_total\" , \"Total cache misses\" , [ \"cache_type\" ] ) SERVICE_AVAILABILITY = Gauge ( \"opossum_service_availability\" , \"Service availability status (1=available, 0=unavailable)\" , [ \"service_name\" ] ) 6.3 Alerting Configuration \u00b6 # prometheus/alert-rules.yml groups : - name : opossum-alerts rules : - alert : HighErrorRate expr : rate(opossum_requests_total{status=\"error\"}[5m]) / rate(opossum_requests_total[5m]) > 0.05 for : 2m labels : severity : critical annotations : summary : \"High error rate detected\" description : \"Error rate is above 5% for the last 2 minutes\" - alert : ServiceUnavailable expr : opossum_service_availability == 0 for : 5m labels : severity : critical annotations : summary : \"Service unavailable\" description : \"{{ $labels.service_name }} has been unavailable for 5 minutes\" - alert : HighResponseTime expr : histogram_quantile(0.95, rate(opossum_response_time_seconds_bucket[5m])) > 2 for : 5m labels : severity : warning annotations : summary : \"High response time\" description : \"95th percentile response time is above 2 seconds for 5 minutes\" 7. Backup and Recovery \u00b6 7.1 Redis Backup \u00b6 # Automated Redis backup script #!/bin/bash BACKUP_DIR = \"/backups/redis\" TIMESTAMP = $( date +%Y%m%d%H%M%S ) BACKUP_FILE = \" $BACKUP_DIR /redis- $TIMESTAMP .rdb\" # Ensure backup directory exists mkdir -p $BACKUP_DIR # Trigger Redis save redis-cli save # Copy RDB file cp /var/lib/redis/dump.rdb $BACKUP_FILE # Compress backup gzip $BACKUP_FILE # Keep last 7 days of backups find $BACKUP_DIR -name \"redis-*.rdb.gz\" -mtime +7 -delete 7.2 Application State Backup \u00b6 # app/management/backup.py async def export_system_state (): \"\"\"Export critical system state\"\"\" state = { \"timestamp\" : datetime . datetime . now () . isoformat (), \"version\" : Config . VERSION , \"service_status\" : {}, \"model_weights\" : {}, \"cache_stats\" : {} } # Get service availability service_monitor = ServiceMonitor () await service_monitor . check_all_services () state [ \"service_status\" ] = service_monitor . services # Get model selection weights backend = HybridModelBackend () state [ \"model_weights\" ] = backend . get_model_weights () # Get cache statistics cache_stats = await redis_client . info ( \"stats\" ) state [ \"cache_stats\" ] = { \"hits\" : cache_stats . get ( \"keyspace_hits\" , 0 ), \"misses\" : cache_stats . get ( \"keyspace_misses\" , 0 ), \"keys\" : await redis_client . dbsize () } # Export to file export_path = f \"backups/state- { datetime . datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } .json\" with open ( export_path , \"w\" ) as f : json . dump ( state , f , indent = 2 ) return export_path 7.3 Recovery Procedures \u00b6 # Redis recovery #!/bin/bash BACKUP_FILE = $1 if [ -z \" $BACKUP_FILE \" ] ; then echo \"Usage: $0 backup-file.rdb.gz\" exit 1 fi # Stop Redis sudo systemctl stop redis-server # Decompress backup if needed if [[ $BACKUP_FILE == *.gz ]] ; then gunzip -c $BACKUP_FILE > /tmp/dump.rdb BACKUP_FILE = \"/tmp/dump.rdb\" fi # Replace Redis data file sudo cp $BACKUP_FILE /var/lib/redis/dump.rdb sudo chown redis:redis /var/lib/redis/dump.rdb # Start Redis sudo systemctl start redis-server # Verify Redis is running redis-cli ping 8. Maintenance Procedures \u00b6 8.1 Model Updates \u00b6 # Update Ollama models script #!/bin/bash MODELS =( \"gemma:latest\" \"llava:latest\" \"all-minilm:latest\" ) for MODEL in \" ${ MODELS [@] } \" ; do echo \"Updating $MODEL ...\" ollama pull $MODEL done # Verify models ollama list 8.2 Dependency Updates \u00b6 # Update dependencies pip install --upgrade -r requirements.txt # Check for security vulnerabilities pip-audit 8.3 Performance Tuning \u00b6 # Example memory optimization settings import torch def optimize_for_environment (): \"\"\"Apply optimal settings based on environment\"\"\" total_memory = torch . cuda . get_device_properties ( 0 ) . total_memory if torch . cuda . is_available () else 0 if torch . cuda . is_available (): if total_memory > 8 * ( 1024 ** 3 ): # More than 8GB VRAM return { \"device_map\" : \"auto\" , \"load_in_8bit\" : False , \"torch_dtype\" : torch . float16 } else : # Less than 8GB VRAM return { \"device_map\" : \"auto\" , \"load_in_8bit\" : True , \"torch_dtype\" : torch . float16 } else : # CPU only return { \"device_map\" : { \"\" : \"cpu\" }, \"low_cpu_mem_usage\" : True } 9. Troubleshooting Common Issues \u00b6 9.1 Service Unavailability \u00b6 Issue Diagnosis Resolution Gemini API unavailable Check API key validity and rate limits Verify API key and check Google Cloud console for quota Ollama service down Check logs with docker logs ollama Restart service and verify model availability Redis connection error Check Redis logs and connectivity Verify Redis is running and properly configured 9.2 Performance Issues \u00b6 Issue Diagnosis Resolution High response time Check request concurrency and model performance Scale horizontally and optimize model selection weights Memory leaks Monitor memory usage over time Implement proper garbage collection and restart services Slow image processing Check image size and transformation pipeline Optimize image processing parameters and caching 9.3 Debug Commands \u00b6 # Check service health curl http://localhost:8000/health # View service logs docker logs opossum-search-app # Check Redis performance redis-cli info stats # Monitor API performance curl http://localhost:8000/metrics # Test model availability curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"gemma\", \"prompt\": \"Hello\", \"stream\": false }' 10. Security Considerations \u00b6 10.1 API Key Rotation \u00b6 # Rotate API keys in Kubernetes kubectl create secret generic api-keys \\ --from-literal = gemini-api-key = new-gemini-api-key \\ --from-literal = redis-password = new-redis-password \\ --dry-run = client -o yaml | kubectl apply -f - # Restart pods to apply new keys kubectl rollout restart deployment opossum-search 10.2 SSL Configuration \u00b6 # ingress.yaml for TLS apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : opossum-search-ingress annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : letsencrypt-prod spec : tls : - hosts : - api.opossum-search.com secretName : opossum-search-tls rules : - host : api.opossum-search.com http : paths : - path : / pathType : Prefix backend : service : name : opossum-search-service port : number : 8000 This Deployment & Operations Guide provides comprehensive instructions for deploying, configuring, monitoring, and maintaining the Opossum Search platform across various environments. Follow these best practices to ensure reliable operation and optimal performance of your deployment.","title":"Developer Quickstart"},{"location":"technical/devops-guide/#technical-documentation-deployment-operations-guide","text":"","title":"Technical Documentation: Deployment &amp; Operations Guide"},{"location":"technical/devops-guide/#1-deployment-overview","text":"This guide outlines the processes and best practices for deploying, configuring, monitoring, and maintaining Opossum Search in various environments.","title":"1. Deployment Overview"},{"location":"technical/devops-guide/#2-environment-setup","text":"","title":"2. Environment Setup"},{"location":"technical/devops-guide/#21-system-requirements","text":"Component Minimum Recommended Notes CPU 2 cores 4+ cores 8+ cores for high-volume production RAM 8GB 16GB 32GB+ for multiple local models Storage 5GB 20GB SSD recommended for model loading Network 10Mbps 100Mbps+ Reliable connection for external APIs GPU None NVIDIA with 8GB+ VRAM For Ollama performance","title":"2.1 System Requirements"},{"location":"technical/devops-guide/#22-software-dependencies","text":"# Install system dependencies sudo apt-get update sudo apt-get install -y \\ [ python3.11 ]( https://docs.python.org/3.11/ ) \\ python3.11-dev \\ python3-pip \\ python3-venv \\ redis-server # [Redis](https://redis.io/) in-memory data structure store imagemagick # [ImageMagick](https://imagemagick.org/index.php) for image processing libmagickwand-dev build-essential # Install NVIDIA drivers and CUDA toolkit (for GPU support) # Skip this section if not using GPU sudo apt-get install -y nvidia-driver-535 nvidia-cuda-toolkit","title":"2.2 Software Dependencies"},{"location":"technical/devops-guide/#23-environment-configurations","text":"Environment Description Use Case development Local setup for developers Feature development, testing staging Production-like environment Integration testing, pre-release validation production Live deployment End-user service","title":"2.3 Environment Configurations"},{"location":"technical/devops-guide/#3-installation-procedures","text":"","title":"3. Installation Procedures"},{"location":"technical/devops-guide/#31-docker-based-deployment-recommended","text":"# Clone repository git clone https://github.com/yourusername/opossum-search.git cd opossum-search # Setup environment file cp .env.example .env # Edit .env file with appropriate settings # Start with Docker Compose docker-compose up -d","title":"3.1 Docker-based Deployment (Recommended)"},{"location":"technical/devops-guide/#docker-compose-file","text":"# docker-compose.yml version : '3.8' services : app : build : context : . dockerfile : Dockerfile ports : - \"8000:8000\" environment : - REDIS_HOST=redis - ENV=production - GEMINI_API_KEY=${GEMINI_API_KEY} - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318/v1/traces depends_on : - redis - ollama - otel-collector volumes : - ./models:/app/models - ./config:/app/config redis : image : redis:7.0-alpine ports : - \"6379:6379\" command : redis-server --appendonly yes volumes : - redis-data:/data ollama : image : ollama/ollama:latest ports : - \"11434:11434\" volumes : - ollama-models:/root/.ollama deploy : resources : reservations : devices : - driver : nvidia count : 1 capabilities : [ gpu ] otel-collector : image : otel/opentelemetry-collector:0.97.0 command : [ \"--config=/etc/otel-config.yaml\" ] volumes : - ./otel-config.yaml:/etc/otel-config.yaml ports : - \"4318:4318\" - \"9464:9464\" jaeger : image : jaegertracing/all-in-one:latest ports : - \"16686:16686\" - \"14250:14250\" volumes : redis-data : ollama-models :","title":"Docker Compose File"},{"location":"technical/devops-guide/#32-kubernetes-deployment","text":"# app-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : opossum-search spec : replicas : 3 selector : matchLabels : app : opossum-search template : metadata : labels : app : opossum-search spec : containers : - name : opossum-search image : opossum-search:latest ports : - containerPort : 8000 env : - name : REDIS_HOST value : redis-service - name : ENV value : production - name : GEMINI_API_KEY valueFrom : secretKeyRef : name : api-keys key : gemini-api-key resources : requests : memory : \"512Mi\" cpu : \"500m\" limits : memory : \"2Gi\" cpu : \"2000m\"","title":"3.2 Kubernetes Deployment"},{"location":"technical/devops-guide/#33-manual-installation","text":"# Clone repository git clone https://github.com/yourusername/opossum-search.git cd opossum-search # Create virtual environment python -m venv venv source venv/bin/activate # Install dependencies pip install -r requirements.txt # Setup environment cp .env.example .env # Edit .env file with appropriate settings # Run application python -m app","title":"3.3 Manual Installation"},{"location":"technical/devops-guide/#4-configuration-management","text":"Effective configuration is crucial for adapting Opossum Search to different environments ( development , staging , production ). Configuration is primarily managed through environment variables and Python configuration files ( config/*.py ).","title":"4. Configuration Management"},{"location":"technical/devops-guide/#41-environment-variables","text":"Variable Purpose Example ENV Deployment environment production DEBUG Enable debug mode False GEMINI_API_KEY API key for Gemini your-api-key REDIS_HOST Redis server hostname redis REDIS_PORT Redis server port 6379 REDIS_PASSWORD Redis password secure-password OLLAMA_BASE_URL Ollama server URL http://ollama:11434 OTEL_EXPORTER_OTLP_ENDPOINT OpenTelemetry collector http://otel-collector:4318 FLASK_APP Flask application entry point app.py FLASK_ENV Flask environment production DATABASE_URL Database connection URL postgres://user:password@host/db SECRET_KEY Secret key for Flask application your-secret-key LOG_LEVEL Logging level INFO OTEL_ENABLED Enable OpenTelemetry True OTEL_SERVICE_NAME OpenTelemetry service name opossum-search","title":"4.1 Environment Variables"},{"location":"technical/devops-guide/#42-configuration-files","text":"Environment-specific Python files (e.g., config/development.py , config/production.py ) inherit from a base Config class and override settings. Key Configuration Settings: Logging: LOG_LEVEL : Sets the logging level (e.g., 'DEBUG', 'INFO', 'WARNING'). OpenTelemetry: OTEL_ENABLED : Boolean to enable/disable OpenTelemetry integration. OTEL_SERVICE_NAME : Name reported to the OTLP collector. OTEL_EXPORTER_OTLP_ENDPOINT : URL of the OTLP collector. Circuit Breaker Settings: CIRCUIT_BREAKER_FAILURE_THRESHOLD : Default failure count before opening (e.g., 5). CIRCUIT_BREAKER_RESET_TIMEOUT : Default seconds before attempting recovery (e.g., 60). [SERVICE_NAME]_CIRCUIT_BREAKER_ENABLED : Boolean to enable/disable breaker for 'gemini', 'ollama', 'transformers'. Defaults to True for external, False for local transformers . [SERVICE_NAME]_FAILURE_THRESHOLD : Service-specific failure threshold override. [SERVICE_NAME]_RESET_TIMEOUT : Service-specific reset timeout override. Retry Policy Settings: DEFAULT_MAX_RETRIES : Default maximum retry attempts for services (e.g., 3). DEFAULT_RETRY_DELAY : Default base delay (seconds) between retries (e.g., 1.0). [SERVICE_NAME]_MAX_RETRIES : Service-specific max retries override (e.g., 'gemini', 'ollama'). Defaults to 0 for transformers . [SERVICE_NAME]_RETRY_DELAY : Service-specific base delay override. API Keys & Endpoints: GEMINI_API_KEY , OLLAMA_HEALTH_URL , etc. Rate Limits: GEMINI_DAILY_LIMIT , GEMINI_RPM_LIMIT , etc. Model Settings: MAX_TOKENS , TEMPERATURE , TOP_P , TOP_K . USE_QUANTIZED_MODELS , MODEL_PRECISION , QUANTIZED_MODEL_DIR . Other: REQUEST_TIMEOUT , TRANSFORMERS_WORKERS , AVAILABILITY_CHECK_INTERVAL . Refer to app/config.py for the full list and default values.","title":"4.2 Configuration Files"},{"location":"technical/devops-guide/#43-secrets-management","text":"API keys and other sensitive credentials should never be hardcoded in configuration files. Use environment variables or a dedicated secrets management system (like HashiCorp Vault, AWS Secrets Manager, etc.) accessed during application startup. The Config class typically reads these from the environment. # Create Kubernetes secrets kubectl create secret generic api-keys \\ --from-literal = gemini-api-key = your-gemini-api-key \\ --from-literal = redis-password = your-redis-password","title":"4.3 Secrets Management"},{"location":"technical/devops-guide/#5-scaling-strategies","text":"","title":"5. Scaling Strategies"},{"location":"technical/devops-guide/#51-horizontal-scaling","text":"# Kubernetes HPA configuration apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : opossum-search-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : opossum-search minReplicas : 2 maxReplicas : 10 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 70 - type : Resource resource : name : memory target : type : Utilization averageUtilization : 80","title":"5.1 Horizontal Scaling"},{"location":"technical/devops-guide/#52-redis-scaling","text":"","title":"5.2 Redis Scaling"},{"location":"technical/devops-guide/#single-instance-configuration","text":"# redis.conf for single instance maxmemory 1gb maxmemory-policy allkeys-lru appendonly yes","title":"Single Instance Configuration"},{"location":"technical/devops-guide/#redis-sentinel-configuration","text":"# sentinel.conf sentinel monitor mymaster redis-master 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 60000 sentinel parallel-syncs mymaster 1","title":"Redis Sentinel Configuration"},{"location":"technical/devops-guide/#53-model-optimization-for-scale","text":"# Example model optimization settings for scaled deployments MODEL_CONFIGS = { \"gemini-thinking\" : { \"api_name\" : \"gemini-1.5-pro\" , \"max_tokens\" : 1024 , \"temperature\" : 0.7 , \"top_p\" : 0.95 , }, \"gemma\" : { \"top_k\" : 40 , \"temperature\" : 0.8 , \"response_format\" : { \"type\" : \"text\" }, }, \"transformers\" : { \"transformers_name\" : \"google/gemma-2b\" , \"max_length\" : 512 , \"device_map\" : \"auto\" , # Automatically select best device } }","title":"5.3 Model Optimization for Scale"},{"location":"technical/devops-guide/#6-monitoring-and-alerting","text":"","title":"6. Monitoring and Alerting"},{"location":"technical/devops-guide/#61-health-checks","text":"# app/api/health.py @app . get ( \"/health\" ) async def health_check (): \"\"\"System health check\"\"\" status = { \"status\" : \"ok\" , \"version\" : Config . VERSION , \"timestamp\" : datetime . datetime . now () . isoformat (), \"services\" : {} } # Check Redis try : redis_ping = await redis_client . ping () status [ \"services\" ][ \"redis\" ] = \"ok\" if redis_ping else \"error\" except Exception as e : status [ \"services\" ][ \"redis\" ] = f \"error: { str ( e ) } \" # Check Ollama try : async with httpx . AsyncClient () as client : response = await client . get ( f \" { Config . OLLAMA_BASE_URL } /api/tags\" ) status [ \"services\" ][ \"ollama\" ] = \"ok\" if response . status_code == 200 else \"error\" except Exception as e : status [ \"services\" ][ \"ollama\" ] = f \"error: { str ( e ) } \" # Check model availability service_monitor = ServiceMonitor () await service_monitor . check_all_services () status [ \"services\" ][ \"model_backends\" ] = { k : \"available\" if v else \"unavailable\" for k , v in service_monitor . services . items () } # Set overall status if any ( v != \"ok\" and not v . startswith ( \"available\" ) for v in [ * status [ \"services\" ] . values (), * status [ \"services\" ][ \"model_backends\" ] . values ()]): status [ \"status\" ] = \"degraded\" return status","title":"6.1 Health Checks"},{"location":"technical/devops-guide/#62-prometheus-metrics","text":"# app/monitoring/metrics.py from prometheus_client import Counter , Histogram , Gauge # Define metrics REQUESTS_TOTAL = Counter ( \"opossum_requests_total\" , \"Total number of requests\" , [ \"method\" , \"endpoint\" ] ) RESPONSE_TIME = Histogram ( \"opossum_response_time_seconds\" , \"Response time in seconds\" , [ \"method\" , \"endpoint\" ] ) MODEL_USAGE = Counter ( \"opossum_model_usage_total\" , \"Total model usage count\" , [ \"model_name\" ] ) CACHE_HITS = Counter ( \"opossum_cache_hits_total\" , \"Total cache hits\" , [ \"cache_type\" ] ) CACHE_MISSES = Counter ( \"opossum_cache_misses_total\" , \"Total cache misses\" , [ \"cache_type\" ] ) SERVICE_AVAILABILITY = Gauge ( \"opossum_service_availability\" , \"Service availability status (1=available, 0=unavailable)\" , [ \"service_name\" ] )","title":"6.2 Prometheus Metrics"},{"location":"technical/devops-guide/#63-alerting-configuration","text":"# prometheus/alert-rules.yml groups : - name : opossum-alerts rules : - alert : HighErrorRate expr : rate(opossum_requests_total{status=\"error\"}[5m]) / rate(opossum_requests_total[5m]) > 0.05 for : 2m labels : severity : critical annotations : summary : \"High error rate detected\" description : \"Error rate is above 5% for the last 2 minutes\" - alert : ServiceUnavailable expr : opossum_service_availability == 0 for : 5m labels : severity : critical annotations : summary : \"Service unavailable\" description : \"{{ $labels.service_name }} has been unavailable for 5 minutes\" - alert : HighResponseTime expr : histogram_quantile(0.95, rate(opossum_response_time_seconds_bucket[5m])) > 2 for : 5m labels : severity : warning annotations : summary : \"High response time\" description : \"95th percentile response time is above 2 seconds for 5 minutes\"","title":"6.3 Alerting Configuration"},{"location":"technical/devops-guide/#7-backup-and-recovery","text":"","title":"7. Backup and Recovery"},{"location":"technical/devops-guide/#71-redis-backup","text":"# Automated Redis backup script #!/bin/bash BACKUP_DIR = \"/backups/redis\" TIMESTAMP = $( date +%Y%m%d%H%M%S ) BACKUP_FILE = \" $BACKUP_DIR /redis- $TIMESTAMP .rdb\" # Ensure backup directory exists mkdir -p $BACKUP_DIR # Trigger Redis save redis-cli save # Copy RDB file cp /var/lib/redis/dump.rdb $BACKUP_FILE # Compress backup gzip $BACKUP_FILE # Keep last 7 days of backups find $BACKUP_DIR -name \"redis-*.rdb.gz\" -mtime +7 -delete","title":"7.1 Redis Backup"},{"location":"technical/devops-guide/#72-application-state-backup","text":"# app/management/backup.py async def export_system_state (): \"\"\"Export critical system state\"\"\" state = { \"timestamp\" : datetime . datetime . now () . isoformat (), \"version\" : Config . VERSION , \"service_status\" : {}, \"model_weights\" : {}, \"cache_stats\" : {} } # Get service availability service_monitor = ServiceMonitor () await service_monitor . check_all_services () state [ \"service_status\" ] = service_monitor . services # Get model selection weights backend = HybridModelBackend () state [ \"model_weights\" ] = backend . get_model_weights () # Get cache statistics cache_stats = await redis_client . info ( \"stats\" ) state [ \"cache_stats\" ] = { \"hits\" : cache_stats . get ( \"keyspace_hits\" , 0 ), \"misses\" : cache_stats . get ( \"keyspace_misses\" , 0 ), \"keys\" : await redis_client . dbsize () } # Export to file export_path = f \"backups/state- { datetime . datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } .json\" with open ( export_path , \"w\" ) as f : json . dump ( state , f , indent = 2 ) return export_path","title":"7.2 Application State Backup"},{"location":"technical/devops-guide/#73-recovery-procedures","text":"# Redis recovery #!/bin/bash BACKUP_FILE = $1 if [ -z \" $BACKUP_FILE \" ] ; then echo \"Usage: $0 backup-file.rdb.gz\" exit 1 fi # Stop Redis sudo systemctl stop redis-server # Decompress backup if needed if [[ $BACKUP_FILE == *.gz ]] ; then gunzip -c $BACKUP_FILE > /tmp/dump.rdb BACKUP_FILE = \"/tmp/dump.rdb\" fi # Replace Redis data file sudo cp $BACKUP_FILE /var/lib/redis/dump.rdb sudo chown redis:redis /var/lib/redis/dump.rdb # Start Redis sudo systemctl start redis-server # Verify Redis is running redis-cli ping","title":"7.3 Recovery Procedures"},{"location":"technical/devops-guide/#8-maintenance-procedures","text":"","title":"8. Maintenance Procedures"},{"location":"technical/devops-guide/#81-model-updates","text":"# Update Ollama models script #!/bin/bash MODELS =( \"gemma:latest\" \"llava:latest\" \"all-minilm:latest\" ) for MODEL in \" ${ MODELS [@] } \" ; do echo \"Updating $MODEL ...\" ollama pull $MODEL done # Verify models ollama list","title":"8.1 Model Updates"},{"location":"technical/devops-guide/#82-dependency-updates","text":"# Update dependencies pip install --upgrade -r requirements.txt # Check for security vulnerabilities pip-audit","title":"8.2 Dependency Updates"},{"location":"technical/devops-guide/#83-performance-tuning","text":"# Example memory optimization settings import torch def optimize_for_environment (): \"\"\"Apply optimal settings based on environment\"\"\" total_memory = torch . cuda . get_device_properties ( 0 ) . total_memory if torch . cuda . is_available () else 0 if torch . cuda . is_available (): if total_memory > 8 * ( 1024 ** 3 ): # More than 8GB VRAM return { \"device_map\" : \"auto\" , \"load_in_8bit\" : False , \"torch_dtype\" : torch . float16 } else : # Less than 8GB VRAM return { \"device_map\" : \"auto\" , \"load_in_8bit\" : True , \"torch_dtype\" : torch . float16 } else : # CPU only return { \"device_map\" : { \"\" : \"cpu\" }, \"low_cpu_mem_usage\" : True }","title":"8.3 Performance Tuning"},{"location":"technical/devops-guide/#9-troubleshooting-common-issues","text":"","title":"9. Troubleshooting Common Issues"},{"location":"technical/devops-guide/#91-service-unavailability","text":"Issue Diagnosis Resolution Gemini API unavailable Check API key validity and rate limits Verify API key and check Google Cloud console for quota Ollama service down Check logs with docker logs ollama Restart service and verify model availability Redis connection error Check Redis logs and connectivity Verify Redis is running and properly configured","title":"9.1 Service Unavailability"},{"location":"technical/devops-guide/#92-performance-issues","text":"Issue Diagnosis Resolution High response time Check request concurrency and model performance Scale horizontally and optimize model selection weights Memory leaks Monitor memory usage over time Implement proper garbage collection and restart services Slow image processing Check image size and transformation pipeline Optimize image processing parameters and caching","title":"9.2 Performance Issues"},{"location":"technical/devops-guide/#93-debug-commands","text":"# Check service health curl http://localhost:8000/health # View service logs docker logs opossum-search-app # Check Redis performance redis-cli info stats # Monitor API performance curl http://localhost:8000/metrics # Test model availability curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"gemma\", \"prompt\": \"Hello\", \"stream\": false }'","title":"9.3 Debug Commands"},{"location":"technical/devops-guide/#10-security-considerations","text":"","title":"10. Security Considerations"},{"location":"technical/devops-guide/#101-api-key-rotation","text":"# Rotate API keys in Kubernetes kubectl create secret generic api-keys \\ --from-literal = gemini-api-key = new-gemini-api-key \\ --from-literal = redis-password = new-redis-password \\ --dry-run = client -o yaml | kubectl apply -f - # Restart pods to apply new keys kubectl rollout restart deployment opossum-search","title":"10.1 API Key Rotation"},{"location":"technical/devops-guide/#102-ssl-configuration","text":"# ingress.yaml for TLS apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : opossum-search-ingress annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : letsencrypt-prod spec : tls : - hosts : - api.opossum-search.com secretName : opossum-search-tls rules : - host : api.opossum-search.com http : paths : - path : / pathType : Prefix backend : service : name : opossum-search-service port : number : 8000 This Deployment & Operations Guide provides comprehensive instructions for deploying, configuring, monitoring, and maintaining the Opossum Search platform across various environments. Follow these best practices to ensure reliable operation and optimal performance of your deployment.","title":"10.2 SSL Configuration"},{"location":"technical/error-handling-resilience/","text":"Error Handling & Resilience \u00b6 The Opossum Search system employs a centralized error handling mechanism and resilience patterns to ensure graceful degradation and robust operation even when external services or internal components fail. Centralized Error Handler ( app.utils.error_handler.ErrorHandler ) \u00b6 The ErrorHandler class provides a single point for capturing, classifying, logging, and responding to exceptions throughout the application. Key Features: \u00b6 Global Exception Capture : Integrates with Flask to handle uncaught exceptions. Error Classification : Categorizes errors (e.g., API_CONNECTION , RATE_LIMIT , VALIDATION , SYSTEM ) using the ErrorCategory enum for consistent handling. Standardized JSON Responses : Formats errors into a consistent JSON structure using the ErrorResponse Pydantic model, including an error code, message, details, request ID, and timestamp. Contextual Logging : Logs errors using the globally configured structured logger, including relevant context provided during the error handling call. Circuit Breaker Integration : Interacts with CircuitBreaker instances for external services ( gemini , ollama , transformers ). Failures are recorded via record_failure , and successes via record_success . Circuit breakers can be configured per service, including enabling/disabling them. Retry Policy Integration : Reads retry policies (max retries, base delay) configured per service ( gemini , ollama , transformers ). Note: The actual retry logic is typically applied via decorators (like retry_with_exponential_backoff ) before the error handler is invoked. Fallback Mechanism : Supports registering and invoking fallback functions for specific services if the primary operation fails. Prometheus Metrics : Tracks error occurrences ( opossum_errors_total counter) and handling duration ( opossum_error_duration_seconds gauge) categorized by error type. Workflow: \u00b6 An exception occurs and is caught or bubbles up to the Flask app. The ErrorHandler.handle_error(error, context) method is invoked. The error is classified ( _classify_error ). Error metrics are updated. If the error context includes a service name, the corresponding circuit breaker's record_failure method is called. The handler checks if a fallback function is registered for the service and attempts to execute it ( _get_fallback_response ). If no fallback succeeds, a standardized JSON error response is formatted ( _format_error_response ) based on the error type and details. The error is logged with appropriate severity and context ( _log_error ). The formatted JSON response with the correct HTTP status code is returned. Configuration: \u00b6 Error handling behavior, circuit breakers, and retry policies are configured via the Flask application config (see Deployment & Operations Guide ). Circuit Breaker Pattern ( app.monitoring.circuit_breaker.CircuitBreaker ) \u00b6 Used to prevent repeated calls to failing external services. States : CLOSED (normal), OPEN (failing, calls blocked), HALF_OPEN (testing recovery). Configuration : Failure threshold, reset timeout (configurable per service). Integration : The ErrorHandler records failures. Decorators or direct checks use the breaker's state ( is_open , should_use_fallback ) to decide whether to call the service. record_success resets the failure count or closes the breaker. Retry Mechanism ( app.utils.retry.retry_with_exponential_backoff ) \u00b6 A decorator used to automatically retry failing operations (typically API calls) with increasing delays. Features : Exponential backoff, jitter, configurable max retries, specific exception handling. Configuration : Max retries, base delay (configurable per service). Integration : Applied directly to functions making potentially failing calls. Retries happen before the error handler is invoked for a final failure. Fallback Strategies \u00b6 When a primary service fails (and retries are exhausted, or the circuit breaker is open), the system attempts to use fallback mechanisms: Registered Fallbacks : The ErrorHandler can invoke specific Python functions registered per service. Model Selection Router : The ServiceRouter inherently handles fallbacks by selecting the next best available model based on scores and availability if the primary choice fails during the routing process. Default Local Model : The transformers backend often serves as the ultimate fallback if all external services are unavailable. Chat2SVG Pipeline Resilience \u00b6 The Chat2SVG pipeline implements dedicated resilience patterns to ensure reliable SVG generation even under suboptimal conditions. Circuit Breaker Pattern \u00b6 The pipeline uses a circuit breaker pattern to prevent cascading failures: # Initialize global components _circuit_breaker = CircuitBreaker () async def generate_svg_request ( prompt : str , style : Optional [ str ] = None , priority : float = 0.5 ) -> Dict [ str , Any ]: # Check circuit breaker if _circuit_breaker . should_use_fallback (): logger . warning ( \"Circuit breaker active, using fallback processing\" ) state . fallback_used = True return await _process_with_fallback ( state ) # Normal processing... SVG Generation Fallback Strategy \u00b6 When the main pipeline encounters errors or when the circuit breaker is open: Template-Only Mode : Fallback uses simplified template generation only Resource Monitoring : Continues to monitor available resources Degraded Output : Returns a basic SVG with essential elements Circuit Recovery : Success/failure tracking to automatically reset the circuit Integration with Global Error Handling \u00b6 The Chat2SVG module integrates with the application's centralized error handling system while maintaining domain-specific fallback mechanisms: Re-exports the central CircuitBreaker component Records success/failure metrics to the central monitoring system Provides detailed error context for centralized logging Fallback Mechanisms \u00b6 Fallback Hierarchy \u00b6 Priority Service Type Capabilities Limitations 1 (Primary) Gemini API External API Full model capabilities, high intelligence Rate limited, requires internet 2 (Secondary) Ollama Local service Good capabilities, custom models Requires GPU for performance, local deployment 3 (Tertiary) Transformers Local library Basic capabilities, offline operation Higher latency, limited model size 4 (SVG) Chat2SVG Pipeline SVG generation with optimization Resource-intensive for enhanced details 5 (Emergency) Client-side JavaScript Basic scripted responses Very limited capabilities, no real AI Hybrid Model Implementation \u00b6 The system implements an intelligent hybrid model backend that dynamically selects and combines different model capabilities based on availability and requirements: Backend Selection Strategy \u00b6 The hybrid model uses a weighted scoring system to select the most appropriate backend: Capability Weight Description Text Processing 0.3 Basic text understanding and generation Reasoning 0.3 Complex reasoning and inference capabilities Multimodal 0.2 Ability to process images with text Latency 0.1 Response time importance Resource Usage 0.1 System resource consumption consideration Service Capabilities \u00b6 Each service is scored based on its capabilities: Gemini API \u00b6 Text Processing: 0.9 Reasoning: 0.95 Multimodal: 1.0 Latency: 0.6 Resource Usage: 0.2 Ollama \u00b6 Text Processing: 0.8 Reasoning: 0.7 Multimodal: 0.0 Latency: 0.8 Resource Usage: 0.6 Transformers \u00b6 Text Processing: 0.7 Reasoning: 0.5 Multimodal: 0.0 Latency: 0.4 Resource Usage: 0.8 Implementation Features \u00b6 The hybrid model implementation includes: Lazy Backend Initialization Backends are created only when needed Reduces resource usage and startup time Real-time Availability Checks Integrates with ServiceAvailability monitoring Considers service health in selection decisions Intelligent Routing Fast path for image-related queries to Gemini Weighted capability scoring for text queries Automatic fallback to next best available service Error Handling Graceful degradation when services fail Automatic fallback to Transformers as last resort Clear error logging and user communication Selection Process \u00b6 Check service availability status Quick check for image processing needs Route directly to Gemini if available Calculate capability scores for available services Select highest scoring available backend Initialize backend if needed Monitor response and handle failures Recovery and Resilience \u00b6 The hybrid implementation provides several layers of resilience: Automatic failover to next best service Lazy initialization to prevent cascading failures Clear logging for debugging and monitoring Graceful degradation of capabilities User-friendly error messages Configuration \u00b6 Service configuration is managed through: Config.MODEL_CONFIGS for model-specific settings Capability weights in HybridModelBackend ServiceAvailability check intervals Backend-specific timeouts and retry settings Fallback Implementation \u00b6 class ServiceRouter : def __init__ ( self , availability_manager ): self . availability_manager = availability_manager async def route_request ( self , user_request ): \"\"\"Route request to best available service\"\"\" services = await self . availability_manager . get_available_services () if \"gemini\" in services and not self . will_exceed_rate_limit ( user_request ): return await self . process_with_gemini ( user_request ) elif \"ollama\" in services : return await self . process_with_ollama ( user_request ) elif \"transformers\" in services : return await self . process_with_transformers ( user_request ) else : return { \"response\" : \"Sorry, no AI services are currently available.\" , \"fallback_used\" : \"none\" , \"service_status\" : \"unavailable\" } Client-Side Fallback \u00b6 The frontend implements a JavaScript-based fallback that simulates basic responses when server services are unavailable: // Excerpt from client-side fallback function getBotResponse ( userMessage ) { // Fallback simulation function userMessage = userMessage . toLowerCase (). trim (); let botMessage = \"\" ; switch ( conversationStage ) { case \"greeting\" : if ( userMessage . includes ( \"hi\" ) || userMessage . includes ( \"hello\" )) { botMessage = \"Greetings! I am your Opossum Information Assistant. How can I help you?\" ; conversationStage = \"initial_query\" ; } else { botMessage = \"Sorry, I didn't catch that. Perhaps start with a friendly 'Hello'?\" ; } break ; // Additional conversation stages and responses... default : botMessage = \"I'm in simulation mode. Please ask something about opossums.\" ; } return botMessage ; } Capability Degradation \u00b6 Service Capability Level Features Available Features Limited Gemini API Full Complete AI capabilities, image analysis None Ollama High Near-complete AI capabilities Some specialized models, slower image processing Transformers Medium Basic Q&A, text completion Complex reasoning, image processing Client-side Minimal Scripted responses only All AI capabilities User Experience During Fallback \u00b6 Fallback Scenario User Notification Experience Impact Gemini \u2192 Ollama \"Using alternative AI service\" Minimal impact, slight latency increase Ollama \u2192 Transformers \"Using simplified model\" Noticeable capability reduction Server \u2192 Client \"Using simplified mode temporarily\" Severely limited capabilities Temporary Outage Loading indicator, retry message Brief delay before fallback activates Recovery Mechanisms \u00b6 Recovery Type Detection Implementation User Experience Automatic Periodic health checks Service switching when preferred service returns Seamless transition to better service Semi-Automatic Service status monitoring Manual approval of service transition Brief service interruption Adaptive Sensitivity analysis Dynamic resource allocation and pipeline tuning Quality adjustment based on resources Manual Administrator intervention Configuration update and service restart Temporary unavailability during restart Resource Sensitivity Analysis \u00b6 The Chat2SVG pipeline uses a SensitivityAnalyzer component to enhance system resilience through resource optimization: Analyzer Capabilities \u00b6 Resource Bottleneck Detection : Identifies which resources (CPU, GPU, memory) are limiting factors Parameter Impact Analysis : Quantifies how configuration changes affect output quality Solution Space Exploration : Evaluates alternative execution paths for optimal resource usage Recommendation Generation : Produces actionable suggestions for pipeline optimization Integration with Error Recovery \u00b6 The sensitivity analyzer's insights are used to recover from resource-related failures: # When processing a request sensitivity_data = await _analyzer . analyze_solution ( solution , resources ) # Data can inform future pipeline configurations to avoid similar failures Adaptive Resource Management \u00b6 Based on sensitivity analysis: Dynamic Resource Allocation : Shifts resources to bottlenecked components Quality-Performance Tradeoffs : Adjusts detail levels based on available resources Predictive Failure Avoidance : Detects potential issues before they cause failures","title":"Error Handling"},{"location":"technical/error-handling-resilience/#error-handling-resilience","text":"The Opossum Search system employs a centralized error handling mechanism and resilience patterns to ensure graceful degradation and robust operation even when external services or internal components fail.","title":"Error Handling &amp; Resilience"},{"location":"technical/error-handling-resilience/#centralized-error-handler-apputilserror_handlererrorhandler","text":"The ErrorHandler class provides a single point for capturing, classifying, logging, and responding to exceptions throughout the application.","title":"Centralized Error Handler (app.utils.error_handler.ErrorHandler)"},{"location":"technical/error-handling-resilience/#key-features","text":"Global Exception Capture : Integrates with Flask to handle uncaught exceptions. Error Classification : Categorizes errors (e.g., API_CONNECTION , RATE_LIMIT , VALIDATION , SYSTEM ) using the ErrorCategory enum for consistent handling. Standardized JSON Responses : Formats errors into a consistent JSON structure using the ErrorResponse Pydantic model, including an error code, message, details, request ID, and timestamp. Contextual Logging : Logs errors using the globally configured structured logger, including relevant context provided during the error handling call. Circuit Breaker Integration : Interacts with CircuitBreaker instances for external services ( gemini , ollama , transformers ). Failures are recorded via record_failure , and successes via record_success . Circuit breakers can be configured per service, including enabling/disabling them. Retry Policy Integration : Reads retry policies (max retries, base delay) configured per service ( gemini , ollama , transformers ). Note: The actual retry logic is typically applied via decorators (like retry_with_exponential_backoff ) before the error handler is invoked. Fallback Mechanism : Supports registering and invoking fallback functions for specific services if the primary operation fails. Prometheus Metrics : Tracks error occurrences ( opossum_errors_total counter) and handling duration ( opossum_error_duration_seconds gauge) categorized by error type.","title":"Key Features:"},{"location":"technical/error-handling-resilience/#workflow","text":"An exception occurs and is caught or bubbles up to the Flask app. The ErrorHandler.handle_error(error, context) method is invoked. The error is classified ( _classify_error ). Error metrics are updated. If the error context includes a service name, the corresponding circuit breaker's record_failure method is called. The handler checks if a fallback function is registered for the service and attempts to execute it ( _get_fallback_response ). If no fallback succeeds, a standardized JSON error response is formatted ( _format_error_response ) based on the error type and details. The error is logged with appropriate severity and context ( _log_error ). The formatted JSON response with the correct HTTP status code is returned.","title":"Workflow:"},{"location":"technical/error-handling-resilience/#configuration","text":"Error handling behavior, circuit breakers, and retry policies are configured via the Flask application config (see Deployment & Operations Guide ).","title":"Configuration:"},{"location":"technical/error-handling-resilience/#circuit-breaker-pattern-appmonitoringcircuit_breakercircuitbreaker","text":"Used to prevent repeated calls to failing external services. States : CLOSED (normal), OPEN (failing, calls blocked), HALF_OPEN (testing recovery). Configuration : Failure threshold, reset timeout (configurable per service). Integration : The ErrorHandler records failures. Decorators or direct checks use the breaker's state ( is_open , should_use_fallback ) to decide whether to call the service. record_success resets the failure count or closes the breaker.","title":"Circuit Breaker Pattern (app.monitoring.circuit_breaker.CircuitBreaker)"},{"location":"technical/error-handling-resilience/#retry-mechanism-apputilsretryretry_with_exponential_backoff","text":"A decorator used to automatically retry failing operations (typically API calls) with increasing delays. Features : Exponential backoff, jitter, configurable max retries, specific exception handling. Configuration : Max retries, base delay (configurable per service). Integration : Applied directly to functions making potentially failing calls. Retries happen before the error handler is invoked for a final failure.","title":"Retry Mechanism (app.utils.retry.retry_with_exponential_backoff)"},{"location":"technical/error-handling-resilience/#fallback-strategies","text":"When a primary service fails (and retries are exhausted, or the circuit breaker is open), the system attempts to use fallback mechanisms: Registered Fallbacks : The ErrorHandler can invoke specific Python functions registered per service. Model Selection Router : The ServiceRouter inherently handles fallbacks by selecting the next best available model based on scores and availability if the primary choice fails during the routing process. Default Local Model : The transformers backend often serves as the ultimate fallback if all external services are unavailable.","title":"Fallback Strategies"},{"location":"technical/error-handling-resilience/#chat2svg-pipeline-resilience","text":"The Chat2SVG pipeline implements dedicated resilience patterns to ensure reliable SVG generation even under suboptimal conditions.","title":"Chat2SVG Pipeline Resilience"},{"location":"technical/error-handling-resilience/#circuit-breaker-pattern","text":"The pipeline uses a circuit breaker pattern to prevent cascading failures: # Initialize global components _circuit_breaker = CircuitBreaker () async def generate_svg_request ( prompt : str , style : Optional [ str ] = None , priority : float = 0.5 ) -> Dict [ str , Any ]: # Check circuit breaker if _circuit_breaker . should_use_fallback (): logger . warning ( \"Circuit breaker active, using fallback processing\" ) state . fallback_used = True return await _process_with_fallback ( state ) # Normal processing...","title":"Circuit Breaker Pattern"},{"location":"technical/error-handling-resilience/#svg-generation-fallback-strategy","text":"When the main pipeline encounters errors or when the circuit breaker is open: Template-Only Mode : Fallback uses simplified template generation only Resource Monitoring : Continues to monitor available resources Degraded Output : Returns a basic SVG with essential elements Circuit Recovery : Success/failure tracking to automatically reset the circuit","title":"SVG Generation Fallback Strategy"},{"location":"technical/error-handling-resilience/#integration-with-global-error-handling","text":"The Chat2SVG module integrates with the application's centralized error handling system while maintaining domain-specific fallback mechanisms: Re-exports the central CircuitBreaker component Records success/failure metrics to the central monitoring system Provides detailed error context for centralized logging","title":"Integration with Global Error Handling"},{"location":"technical/error-handling-resilience/#fallback-mechanisms","text":"","title":"Fallback Mechanisms"},{"location":"technical/error-handling-resilience/#fallback-hierarchy","text":"Priority Service Type Capabilities Limitations 1 (Primary) Gemini API External API Full model capabilities, high intelligence Rate limited, requires internet 2 (Secondary) Ollama Local service Good capabilities, custom models Requires GPU for performance, local deployment 3 (Tertiary) Transformers Local library Basic capabilities, offline operation Higher latency, limited model size 4 (SVG) Chat2SVG Pipeline SVG generation with optimization Resource-intensive for enhanced details 5 (Emergency) Client-side JavaScript Basic scripted responses Very limited capabilities, no real AI","title":"Fallback Hierarchy"},{"location":"technical/error-handling-resilience/#hybrid-model-implementation","text":"The system implements an intelligent hybrid model backend that dynamically selects and combines different model capabilities based on availability and requirements:","title":"Hybrid Model Implementation"},{"location":"technical/error-handling-resilience/#backend-selection-strategy","text":"The hybrid model uses a weighted scoring system to select the most appropriate backend: Capability Weight Description Text Processing 0.3 Basic text understanding and generation Reasoning 0.3 Complex reasoning and inference capabilities Multimodal 0.2 Ability to process images with text Latency 0.1 Response time importance Resource Usage 0.1 System resource consumption consideration","title":"Backend Selection Strategy"},{"location":"technical/error-handling-resilience/#service-capabilities","text":"Each service is scored based on its capabilities:","title":"Service Capabilities"},{"location":"technical/error-handling-resilience/#gemini-api","text":"Text Processing: 0.9 Reasoning: 0.95 Multimodal: 1.0 Latency: 0.6 Resource Usage: 0.2","title":"Gemini API"},{"location":"technical/error-handling-resilience/#ollama","text":"Text Processing: 0.8 Reasoning: 0.7 Multimodal: 0.0 Latency: 0.8 Resource Usage: 0.6","title":"Ollama"},{"location":"technical/error-handling-resilience/#transformers","text":"Text Processing: 0.7 Reasoning: 0.5 Multimodal: 0.0 Latency: 0.4 Resource Usage: 0.8","title":"Transformers"},{"location":"technical/error-handling-resilience/#implementation-features","text":"The hybrid model implementation includes: Lazy Backend Initialization Backends are created only when needed Reduces resource usage and startup time Real-time Availability Checks Integrates with ServiceAvailability monitoring Considers service health in selection decisions Intelligent Routing Fast path for image-related queries to Gemini Weighted capability scoring for text queries Automatic fallback to next best available service Error Handling Graceful degradation when services fail Automatic fallback to Transformers as last resort Clear error logging and user communication","title":"Implementation Features"},{"location":"technical/error-handling-resilience/#selection-process","text":"Check service availability status Quick check for image processing needs Route directly to Gemini if available Calculate capability scores for available services Select highest scoring available backend Initialize backend if needed Monitor response and handle failures","title":"Selection Process"},{"location":"technical/error-handling-resilience/#recovery-and-resilience","text":"The hybrid implementation provides several layers of resilience: Automatic failover to next best service Lazy initialization to prevent cascading failures Clear logging for debugging and monitoring Graceful degradation of capabilities User-friendly error messages","title":"Recovery and Resilience"},{"location":"technical/error-handling-resilience/#configuration_1","text":"Service configuration is managed through: Config.MODEL_CONFIGS for model-specific settings Capability weights in HybridModelBackend ServiceAvailability check intervals Backend-specific timeouts and retry settings","title":"Configuration"},{"location":"technical/error-handling-resilience/#fallback-implementation","text":"class ServiceRouter : def __init__ ( self , availability_manager ): self . availability_manager = availability_manager async def route_request ( self , user_request ): \"\"\"Route request to best available service\"\"\" services = await self . availability_manager . get_available_services () if \"gemini\" in services and not self . will_exceed_rate_limit ( user_request ): return await self . process_with_gemini ( user_request ) elif \"ollama\" in services : return await self . process_with_ollama ( user_request ) elif \"transformers\" in services : return await self . process_with_transformers ( user_request ) else : return { \"response\" : \"Sorry, no AI services are currently available.\" , \"fallback_used\" : \"none\" , \"service_status\" : \"unavailable\" }","title":"Fallback Implementation"},{"location":"technical/error-handling-resilience/#client-side-fallback","text":"The frontend implements a JavaScript-based fallback that simulates basic responses when server services are unavailable: // Excerpt from client-side fallback function getBotResponse ( userMessage ) { // Fallback simulation function userMessage = userMessage . toLowerCase (). trim (); let botMessage = \"\" ; switch ( conversationStage ) { case \"greeting\" : if ( userMessage . includes ( \"hi\" ) || userMessage . includes ( \"hello\" )) { botMessage = \"Greetings! I am your Opossum Information Assistant. How can I help you?\" ; conversationStage = \"initial_query\" ; } else { botMessage = \"Sorry, I didn't catch that. Perhaps start with a friendly 'Hello'?\" ; } break ; // Additional conversation stages and responses... default : botMessage = \"I'm in simulation mode. Please ask something about opossums.\" ; } return botMessage ; }","title":"Client-Side Fallback"},{"location":"technical/error-handling-resilience/#capability-degradation","text":"Service Capability Level Features Available Features Limited Gemini API Full Complete AI capabilities, image analysis None Ollama High Near-complete AI capabilities Some specialized models, slower image processing Transformers Medium Basic Q&A, text completion Complex reasoning, image processing Client-side Minimal Scripted responses only All AI capabilities","title":"Capability Degradation"},{"location":"technical/error-handling-resilience/#user-experience-during-fallback","text":"Fallback Scenario User Notification Experience Impact Gemini \u2192 Ollama \"Using alternative AI service\" Minimal impact, slight latency increase Ollama \u2192 Transformers \"Using simplified model\" Noticeable capability reduction Server \u2192 Client \"Using simplified mode temporarily\" Severely limited capabilities Temporary Outage Loading indicator, retry message Brief delay before fallback activates","title":"User Experience During Fallback"},{"location":"technical/error-handling-resilience/#recovery-mechanisms","text":"Recovery Type Detection Implementation User Experience Automatic Periodic health checks Service switching when preferred service returns Seamless transition to better service Semi-Automatic Service status monitoring Manual approval of service transition Brief service interruption Adaptive Sensitivity analysis Dynamic resource allocation and pipeline tuning Quality adjustment based on resources Manual Administrator intervention Configuration update and service restart Temporary unavailability during restart","title":"Recovery Mechanisms"},{"location":"technical/error-handling-resilience/#resource-sensitivity-analysis","text":"The Chat2SVG pipeline uses a SensitivityAnalyzer component to enhance system resilience through resource optimization:","title":"Resource Sensitivity Analysis"},{"location":"technical/error-handling-resilience/#analyzer-capabilities","text":"Resource Bottleneck Detection : Identifies which resources (CPU, GPU, memory) are limiting factors Parameter Impact Analysis : Quantifies how configuration changes affect output quality Solution Space Exploration : Evaluates alternative execution paths for optimal resource usage Recommendation Generation : Produces actionable suggestions for pipeline optimization","title":"Analyzer Capabilities"},{"location":"technical/error-handling-resilience/#integration-with-error-recovery","text":"The sensitivity analyzer's insights are used to recover from resource-related failures: # When processing a request sensitivity_data = await _analyzer . analyze_solution ( solution , resources ) # Data can inform future pipeline configurations to avoid similar failures","title":"Integration with Error Recovery"},{"location":"technical/error-handling-resilience/#adaptive-resource-management","text":"Based on sensitivity analysis: Dynamic Resource Allocation : Shifts resources to bottlenecked components Quality-Performance Tradeoffs : Adjusts detail levels based on available resources Predictive Failure Avoidance : Detects potential issues before they cause failures","title":"Adaptive Resource Management"},{"location":"technical/getting-started/","text":"Technical Documentation Index \u00b6 Overview \u00b6 The technical documentation section provides in-depth information about Opossum Search's core systems, architectures, and implementation details. This section is intended for developers, system architects, and technical stakeholders who need to understand how the system works under the hood. Core Architectures \u00b6 Opossum Search is built on several key architectural components that work together to provide a resilient, performant search experience: Redis Caching Architecture - Our multi-level caching system for optimizing performance and reducing API costs Hybrid Model Selection - How Opossum intelligently routes queries to appropriate AI backends Image Processing Pipeline - End-to-end processing of images for multimodal understanding GraphQL API - Our flexible, type-safe API implementation with security controls OpenTelemetry Integration - Comprehensive observability implementation across the system SVG Markup - Secure generation of vector graphics for visualizations System Architecture Overview - High-level view of how all components interact Deployment & Operations Guide - Guide for deploying and maintaining the system Security Model - Comprehensive security controls and practices Error Handling & Resilience - How the system handles failures gracefully Key Concepts \u00b6 Multi-Level Resilience \u00b6 Opossum Search implements multiple layers of resilience, including service monitoring, circuit breakers, fallback chains, and graceful degradation. This ensures the system remains operational even when individual components fail. Dynamic Model Selection \u00b6 Rather than relying on a single AI model, Opossum uses a sophisticated scoring system to route queries to the most appropriate backend based on capability, availability, and performance characteristics. Observability-First Design \u00b6 Every component is instrumented with telemetry, providing deep visibility into system behavior, performance bottlenecks, and error patterns through distributed tracing, metrics, and structured logging. When to Use This Documentation \u00b6 Planning integrations with Opossum's backend systems Troubleshooting complex technical issues Understanding performance characteristics of different components Implementing similar architectures in your own systems Contributing to Opossum's development Related Sections \u00b6 Service Availability - Higher-level documentation about system availability Development - Practical guides for developers working with the codebase Infrastructure - Details about the supporting infrastructure","title":"First Steps"},{"location":"technical/getting-started/#technical-documentation-index","text":"","title":"Technical Documentation Index"},{"location":"technical/getting-started/#overview","text":"The technical documentation section provides in-depth information about Opossum Search's core systems, architectures, and implementation details. This section is intended for developers, system architects, and technical stakeholders who need to understand how the system works under the hood.","title":"Overview"},{"location":"technical/getting-started/#core-architectures","text":"Opossum Search is built on several key architectural components that work together to provide a resilient, performant search experience: Redis Caching Architecture - Our multi-level caching system for optimizing performance and reducing API costs Hybrid Model Selection - How Opossum intelligently routes queries to appropriate AI backends Image Processing Pipeline - End-to-end processing of images for multimodal understanding GraphQL API - Our flexible, type-safe API implementation with security controls OpenTelemetry Integration - Comprehensive observability implementation across the system SVG Markup - Secure generation of vector graphics for visualizations System Architecture Overview - High-level view of how all components interact Deployment & Operations Guide - Guide for deploying and maintaining the system Security Model - Comprehensive security controls and practices Error Handling & Resilience - How the system handles failures gracefully","title":"Core Architectures"},{"location":"technical/getting-started/#key-concepts","text":"","title":"Key Concepts"},{"location":"technical/getting-started/#multi-level-resilience","text":"Opossum Search implements multiple layers of resilience, including service monitoring, circuit breakers, fallback chains, and graceful degradation. This ensures the system remains operational even when individual components fail.","title":"Multi-Level Resilience"},{"location":"technical/getting-started/#dynamic-model-selection","text":"Rather than relying on a single AI model, Opossum uses a sophisticated scoring system to route queries to the most appropriate backend based on capability, availability, and performance characteristics.","title":"Dynamic Model Selection"},{"location":"technical/getting-started/#observability-first-design","text":"Every component is instrumented with telemetry, providing deep visibility into system behavior, performance bottlenecks, and error patterns through distributed tracing, metrics, and structured logging.","title":"Observability-First Design"},{"location":"technical/getting-started/#when-to-use-this-documentation","text":"Planning integrations with Opossum's backend systems Troubleshooting complex technical issues Understanding performance characteristics of different components Implementing similar architectures in your own systems Contributing to Opossum's development","title":"When to Use This Documentation"},{"location":"technical/getting-started/#related-sections","text":"Service Availability - Higher-level documentation about system availability Development - Practical guides for developers working with the codebase Infrastructure - Details about the supporting infrastructure","title":"Related Sections"},{"location":"technical/graphql-api/","text":"Technical Documentation: GraphQL API \u00b6 1. Overview \u00b6 Opossum Search uses GraphQL as its primary API layer, providing a flexible, type-safe interface for clients to interact with the system. The GraphQL implementation leverages Ariadne for schema-first development and includes advanced features such as rate limiting, request costing, authentication, and integration with Apollo Studio. Related Documentation: - GraphQL API: Getting Started - User guide for integrating with the GraphQL API - Service Availability: Rate Limiting - How rate limiting is implemented at the API level - API Reference: Routes - REST API routes that complement the GraphQL API 2. Schema Design \u00b6 2.1 Core Types \u00b6 The GraphQL schema defines several core types: type Query { # Status and health checking service_status : ServiceStatus ! # Search-related queries search ( query : String ! , options : SearchOptions ) : SearchResult ! # Utility queries generate_gibberish ( prompt : String ! ) : String @cost ( value : 2 ) } type Mutation { # Interactive chat chat ( message : String ! , conversation_id : ID ) : ChatResponse ! @rateLimit ( limit : \"50 per day, 10 per hour\" ) @cost ( value : 10 ) @auth ( requires : USER ) # Image processing process_image ( image : Upload ! , prompt : String ) : ImageAnalysisResult ! @rateLimit ( limit : \"20 per day, 5 per hour\" ) @cost ( value : 15 ) @auth ( requires : USER ) # Service management force_service_check : ServiceStatus ! @rateLimit ( limit : \"10 per hour\" ) @auth ( requires : ADMIN ) } type ServiceStatus { gemini_available : Boolean ! ollama_available : Boolean ! transformers_available : Boolean ! all_services_available : Boolean ! last_checked : String ! } type ChatResponse { message : String ! conversation_id : ID ! backend_used : String tokens_used : Int } type ImageAnalysisResult { description : String ! tags : [ String !] confidence : Float backend_used : String } type SearchResult { results : [ SearchItem !]! total : Int ! query_time_ms : Int ! } type SearchItem { title : String ! url : String snippet : String score : Float } input SearchOptions { limit : Int offset : Int filter : String } 2.2 Schema Registration \u00b6 # Schema registration with Ariadne from ariadne import load_schema_from_path , make_executable_schema from ariadne.asgi import GraphQL # Load schema from file type_defs = load_schema_from_path ( \"app/api/schema.graphql\" ) # Create executable schema with resolvers and directives schema = make_executable_schema ( type_defs , query_resolvers , mutation_resolvers , cost_directive , rate_limit_directive , auth_directive , caching_directive ) # Create GraphQL application graphql_app = GraphQL ( schema , debug = Config . DEBUG , introspection = Config . GRAPHQL_GRAPHIQL , validation_rules = [ cost_validation_rule , depth_limit_rule ] ) 3. Directives \u00b6 3.1 Cost Directive \u00b6 Controls query complexity and prevents resource abuse: # Cost directive implementation from ariadne import SchemaDirectiveVisitor from graphql import GraphQLField class CostDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve cost_value = self . args . get ( \"value\" , 1 ) async def resolve_with_cost ( obj , info , ** kwargs ): # Add cost to request context if not hasattr ( info . context , \"cost\" ): info . context . cost = 0 info . context . cost += cost_value # Apply cost limit validation if info . context . cost > Config . GRAPHQL_COMPLEXITY_THRESHOLD : raise Exception ( f \"Query exceeded cost limit: { info . context . cost } \" ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_cost return field 3.2 Rate Limit Directive \u00b6 Prevents API abuse by enforcing request rate limits: # Rate limit directive implementation class RateLimitDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve limit = self . args . get ( \"limit\" , \"100 per day\" ) async def resolve_with_rate_limit ( obj , info , ** kwargs ): # Get client identifier client_id = self . _get_client_id ( info . context ) # Check rate limit limiter = RateLimiter ( redis_client ) is_limited = await limiter . is_rate_limited ( client_id , limit ) if is_limited : raise Exception ( \"Rate limit exceeded. Try again later.\" ) # Increment rate counter await limiter . increment_rate_counter ( client_id , limit ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_rate_limit return field def _get_client_id ( self , context ): \"\"\"Extract client identifier from request context\"\"\" # Try authenticated user ID first if hasattr ( context , \"user\" ) and context . user : return f \"user: { context . user . id } \" # Fall back to IP address return f \"ip: { context . request . client . host } \" 3.3 Auth Directive \u00b6 Enforces authentication requirements for protected operations: # Auth directive implementation class AuthDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve required_role = self . args . get ( \"requires\" , \"USER\" ) async def resolve_with_auth ( obj , info , ** kwargs ): # Check if user is authenticated if not hasattr ( info . context , \"user\" ) or not info . context . user : raise Exception ( \"Authentication required\" ) # Check role if specified if required_role == \"ADMIN\" and not info . context . user . is_admin : raise Exception ( \"Admin access required\" ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_auth return field 3.4 Caching Directive \u00b6 Enables resolver-level caching for expensive operations: # Caching directive implementation class CachingDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve ttl = self . args . get ( \"ttl\" , 60 ) # Default 60 seconds async def resolve_with_caching ( obj , info , ** kwargs ): # Generate cache key cache_key = f \"graphql:cache: { info . field_name } : { hash ( frozenset ( kwargs . items ())) } \" # Check cache cached = await redis_client . get ( cache_key ) if cached : return json . loads ( cached ) # Call original resolver result = await original_resolver ( obj , info , ** kwargs ) # Cache result await redis_client . setex ( cache_key , ttl , json . dumps ( result , default = serialize_for_json ) ) return result field . resolve = resolve_with_caching return field 4. Query Examples \u00b6 4.1 Service Status Check \u00b6 query CheckStatus { service_status { gemini_available ollama_available transformers_available all_services_available last_checked } } 4.2 Text Chat \u00b6 mutation SendChatMessage { chat ( message : \"What is the natural habitat of opossums?\" conversation_id : \"conv_12345\" ) { message conversation_id backend_used tokens_used } } 4.3 Image Processing \u00b6 mutation ProcessImage ( $image : Upload !, $prompt : String ) { process_image ( image : $image prompt : \"Describe what you see in this image\" ) { description tags confidence backend_used } } 4.4 Search Query \u00b6 query Search ( $query : String !, $options : SearchOptions ) { search ( query : \"opossum diet in urban environments\" options : { limit : 10 filter : \"recent\" } ) { results { title url snippet score } total query_time_ms } } 5. Apollo Studio Integration \u00b6 5.1 Schema Reporting \u00b6 The GraphQL API reports its schema to Apollo Studio for monitoring and exploration: # Apollo Studio integration from ariadne.contrib.apollo import ApolloTracingExtension from ariadne.contrib.federation import FederatedObjectType , make_federated_schema # Setup Apollo extensions if enabled extensions = [] if Config . APOLLO_STUDIO_ENABLED : extensions . append ( ApolloTracingExtension ) # Create GraphQL application with Apollo integration graphql_app = GraphQL ( schema , debug = Config . DEBUG , introspection = Config . GRAPHQL_GRAPHIQL , extensions = extensions ) # Apollo Studio configuration in settings APOLLO_KEY = os . getenv ( \"APOLLO_KEY\" ) APOLLO_GRAPH_REF = os . getenv ( \"APOLLO_GRAPH_REF\" , \"opossum-search@current\" ) APOLLO_SCHEMA_REPORTING = os . getenv ( \"APOLLO_SCHEMA_REPORTING\" , \"True\" ) . lower () == \"true\" 5.2 Performance Tracing \u00b6 Apollo tracing provides performance insights: # Apollo tracing middleware @app . middleware ( \"http\" ) async def apollo_tracing_middleware ( request , call_next ): # Skip non-GraphQL requests if not request . url . path == Config . GRAPHQL_ENDPOINT : return await call_next ( request ) # Add Apollo tracing header response = await call_next ( request ) response . headers [ \"apollo-federation-include-trace\" ] = ( \"ftv1\" if Config . APOLLO_INCLUDE_TRACES else \"false\" ) return response 6. Security and Performance \u00b6 6.1 Depth Limiting \u00b6 Prevents deeply nested queries that could consume excessive resources: # GraphQL depth limitation from graphql import validation from graphql.language.ast import FieldNode , FragmentSpreadNode , InlineFragmentNode class DepthLimitRule ( validation . ValidationRule ): def __init__ ( self , max_depth ): super () . __init__ () self . max_depth = max_depth def enter_operation_definition ( self , node , key , parent , path , ancestors ): self . depth_map = {} return node def enter_field ( self , node , key , parent , path , ancestors ): depth = self . _get_depth ( ancestors ) if depth > self . max_depth : self . report_error ( f \"Query exceeds maximum depth of { self . max_depth } \" ) return node def _get_depth ( self , ancestors ): # Calculate current depth based on ancestry depth = 0 for ancestor in ancestors : if isinstance ( ancestor , ( FieldNode , FragmentSpreadNode , InlineFragmentNode )): depth += 1 return depth # Create depth limit rule depth_limit_rule = DepthLimitRule ( Config . GRAPHQL_DEPTH_LIMIT ) 6.2 CORS Configuration \u00b6 Cross-Origin Resource Sharing settings for browser security: # CORS middleware configuration from starlette.middleware.cors import CORSMiddleware # Add CORS middleware app . add_middleware ( CORSMiddleware , allow_origins = Config . CORS_ALLOWED_ORIGINS , allow_credentials = Config . CORS_ALLOW_CREDENTIALS , allow_methods = [ \"GET\" , \"POST\" , \"OPTIONS\" ], allow_headers = Config . CORS_ALLOW_HEADERS , expose_headers = Config . CORS_EXPOSE_HEADERS ) 6.3 Response Compression \u00b6 Reduces bandwidth usage for API responses: # Compression middleware from starlette.middleware.gzip import GZipMiddleware # Add compression middleware app . add_middleware ( GZipMiddleware , minimum_size = Config . COMPRESS_MIN_SIZE , compresslevel = Config . COMPRESS_LEVEL ) 6.4 API Key Authentication \u00b6 Secures the GraphQL API with API key validation: # API key authentication middleware @app . middleware ( \"http\" ) async def api_key_middleware ( request , call_next ): # Skip if API key auth is disabled if not Config . API_KEY_REQUIRED : return await call_next ( request ) # Skip non-GraphQL requests if not request . url . path == Config . GRAPHQL_ENDPOINT : return await call_next ( request ) # Check for API key in headers or query parameters api_key = request . headers . get ( \"x-api-key\" ) if not api_key : # Try query parameter query_params = request . query_params api_key = query_params . get ( \"api_key\" ) # Validate API key if not api_key or api_key != Config . API_KEY : return JSONResponse ( status_code = 401 , content = { \"error\" : \"Invalid or missing API key\" } ) # Proceed with request return await call_next ( request ) 7. Integration with GraphQL Voyager \u00b6 For schema visualization and exploration, the application includes GraphQL Voyager: # Voyager integration from fastapi import FastAPI , Request from fastapi.responses import HTMLResponse from fastapi.templating import Jinja2Templates # Setup templates for Voyager templates = Jinja2Templates ( directory = \"templates\" ) # Add Voyager endpoint @app . get ( \"/voyager\" , response_class = HTMLResponse ) async def voyager ( request : Request ): \"\"\"Serve GraphQL Voyager interface\"\"\" if not Config . VOYAGER_ENABLED : return { \"error\" : \"GraphQL Voyager is disabled\" } return templates . TemplateResponse ( Config . VOYAGER_PATH , { \"request\" : request , \"graphql_endpoint\" : Config . GRAPHQL_ENDPOINT } ) 8. Resolver Implementation \u00b6 Sample resolver implementation for the core queries: # Query resolvers query_resolvers = QueryType () @query_resolvers . field ( \"service_status\" ) async def resolve_service_status ( _ , info ): \"\"\"Resolve service status query\"\"\" service_monitor = ServiceMonitor () await service_monitor . check_all_services () return { \"gemini_available\" : service_monitor . services [ \"gemini\" ], \"ollama_available\" : service_monitor . services [ \"ollama\" ], \"transformers_available\" : service_monitor . services [ \"transformers\" ], \"all_services_available\" : all ( service_monitor . services . values ()), \"last_checked\" : datetime . datetime . now () . isoformat () } # Mutation resolvers mutation_resolvers = MutationType () @mutation_resolvers . field ( \"chat\" ) async def resolve_chat ( _ , info , message , conversation_id = None ): \"\"\"Resolve chat mutation\"\"\" # Create or retrieve conversation if not conversation_id : conversation_id = str ( uuid . uuid4 ()) # Get backend backend = HybridModelBackend () # Generate response response , metadata = await backend . generate_response ( message , conversation_id = conversation_id ) return { \"message\" : response , \"conversation_id\" : conversation_id , \"backend_used\" : metadata . get ( \"backend\" ), \"tokens_used\" : metadata . get ( \"tokens\" , 0 ) } The GraphQL API serves as the primary interface for Opossum Search, providing a flexible, secure, and performant way for clients to interact with the system. The schema-first approach with Ariadne enables clean separation of concerns, while directives handle cross-cutting concerns like authentication, rate limiting, and caching.","title":"GraphQL API"},{"location":"technical/graphql-api/#technical-documentation-graphql-api","text":"","title":"Technical Documentation: GraphQL API"},{"location":"technical/graphql-api/#1-overview","text":"Opossum Search uses GraphQL as its primary API layer, providing a flexible, type-safe interface for clients to interact with the system. The GraphQL implementation leverages Ariadne for schema-first development and includes advanced features such as rate limiting, request costing, authentication, and integration with Apollo Studio. Related Documentation: - GraphQL API: Getting Started - User guide for integrating with the GraphQL API - Service Availability: Rate Limiting - How rate limiting is implemented at the API level - API Reference: Routes - REST API routes that complement the GraphQL API","title":"1. Overview"},{"location":"technical/graphql-api/#2-schema-design","text":"","title":"2. Schema Design"},{"location":"technical/graphql-api/#21-core-types","text":"The GraphQL schema defines several core types: type Query { # Status and health checking service_status : ServiceStatus ! # Search-related queries search ( query : String ! , options : SearchOptions ) : SearchResult ! # Utility queries generate_gibberish ( prompt : String ! ) : String @cost ( value : 2 ) } type Mutation { # Interactive chat chat ( message : String ! , conversation_id : ID ) : ChatResponse ! @rateLimit ( limit : \"50 per day, 10 per hour\" ) @cost ( value : 10 ) @auth ( requires : USER ) # Image processing process_image ( image : Upload ! , prompt : String ) : ImageAnalysisResult ! @rateLimit ( limit : \"20 per day, 5 per hour\" ) @cost ( value : 15 ) @auth ( requires : USER ) # Service management force_service_check : ServiceStatus ! @rateLimit ( limit : \"10 per hour\" ) @auth ( requires : ADMIN ) } type ServiceStatus { gemini_available : Boolean ! ollama_available : Boolean ! transformers_available : Boolean ! all_services_available : Boolean ! last_checked : String ! } type ChatResponse { message : String ! conversation_id : ID ! backend_used : String tokens_used : Int } type ImageAnalysisResult { description : String ! tags : [ String !] confidence : Float backend_used : String } type SearchResult { results : [ SearchItem !]! total : Int ! query_time_ms : Int ! } type SearchItem { title : String ! url : String snippet : String score : Float } input SearchOptions { limit : Int offset : Int filter : String }","title":"2.1 Core Types"},{"location":"technical/graphql-api/#22-schema-registration","text":"# Schema registration with Ariadne from ariadne import load_schema_from_path , make_executable_schema from ariadne.asgi import GraphQL # Load schema from file type_defs = load_schema_from_path ( \"app/api/schema.graphql\" ) # Create executable schema with resolvers and directives schema = make_executable_schema ( type_defs , query_resolvers , mutation_resolvers , cost_directive , rate_limit_directive , auth_directive , caching_directive ) # Create GraphQL application graphql_app = GraphQL ( schema , debug = Config . DEBUG , introspection = Config . GRAPHQL_GRAPHIQL , validation_rules = [ cost_validation_rule , depth_limit_rule ] )","title":"2.2 Schema Registration"},{"location":"technical/graphql-api/#3-directives","text":"","title":"3. Directives"},{"location":"technical/graphql-api/#31-cost-directive","text":"Controls query complexity and prevents resource abuse: # Cost directive implementation from ariadne import SchemaDirectiveVisitor from graphql import GraphQLField class CostDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve cost_value = self . args . get ( \"value\" , 1 ) async def resolve_with_cost ( obj , info , ** kwargs ): # Add cost to request context if not hasattr ( info . context , \"cost\" ): info . context . cost = 0 info . context . cost += cost_value # Apply cost limit validation if info . context . cost > Config . GRAPHQL_COMPLEXITY_THRESHOLD : raise Exception ( f \"Query exceeded cost limit: { info . context . cost } \" ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_cost return field","title":"3.1 Cost Directive"},{"location":"technical/graphql-api/#32-rate-limit-directive","text":"Prevents API abuse by enforcing request rate limits: # Rate limit directive implementation class RateLimitDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve limit = self . args . get ( \"limit\" , \"100 per day\" ) async def resolve_with_rate_limit ( obj , info , ** kwargs ): # Get client identifier client_id = self . _get_client_id ( info . context ) # Check rate limit limiter = RateLimiter ( redis_client ) is_limited = await limiter . is_rate_limited ( client_id , limit ) if is_limited : raise Exception ( \"Rate limit exceeded. Try again later.\" ) # Increment rate counter await limiter . increment_rate_counter ( client_id , limit ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_rate_limit return field def _get_client_id ( self , context ): \"\"\"Extract client identifier from request context\"\"\" # Try authenticated user ID first if hasattr ( context , \"user\" ) and context . user : return f \"user: { context . user . id } \" # Fall back to IP address return f \"ip: { context . request . client . host } \"","title":"3.2 Rate Limit Directive"},{"location":"technical/graphql-api/#33-auth-directive","text":"Enforces authentication requirements for protected operations: # Auth directive implementation class AuthDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve required_role = self . args . get ( \"requires\" , \"USER\" ) async def resolve_with_auth ( obj , info , ** kwargs ): # Check if user is authenticated if not hasattr ( info . context , \"user\" ) or not info . context . user : raise Exception ( \"Authentication required\" ) # Check role if specified if required_role == \"ADMIN\" and not info . context . user . is_admin : raise Exception ( \"Admin access required\" ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_auth return field","title":"3.3 Auth Directive"},{"location":"technical/graphql-api/#34-caching-directive","text":"Enables resolver-level caching for expensive operations: # Caching directive implementation class CachingDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve ttl = self . args . get ( \"ttl\" , 60 ) # Default 60 seconds async def resolve_with_caching ( obj , info , ** kwargs ): # Generate cache key cache_key = f \"graphql:cache: { info . field_name } : { hash ( frozenset ( kwargs . items ())) } \" # Check cache cached = await redis_client . get ( cache_key ) if cached : return json . loads ( cached ) # Call original resolver result = await original_resolver ( obj , info , ** kwargs ) # Cache result await redis_client . setex ( cache_key , ttl , json . dumps ( result , default = serialize_for_json ) ) return result field . resolve = resolve_with_caching return field","title":"3.4 Caching Directive"},{"location":"technical/graphql-api/#4-query-examples","text":"","title":"4. Query Examples"},{"location":"technical/graphql-api/#41-service-status-check","text":"query CheckStatus { service_status { gemini_available ollama_available transformers_available all_services_available last_checked } }","title":"4.1 Service Status Check"},{"location":"technical/graphql-api/#42-text-chat","text":"mutation SendChatMessage { chat ( message : \"What is the natural habitat of opossums?\" conversation_id : \"conv_12345\" ) { message conversation_id backend_used tokens_used } }","title":"4.2 Text Chat"},{"location":"technical/graphql-api/#43-image-processing","text":"mutation ProcessImage ( $image : Upload !, $prompt : String ) { process_image ( image : $image prompt : \"Describe what you see in this image\" ) { description tags confidence backend_used } }","title":"4.3 Image Processing"},{"location":"technical/graphql-api/#44-search-query","text":"query Search ( $query : String !, $options : SearchOptions ) { search ( query : \"opossum diet in urban environments\" options : { limit : 10 filter : \"recent\" } ) { results { title url snippet score } total query_time_ms } }","title":"4.4 Search Query"},{"location":"technical/graphql-api/#5-apollo-studio-integration","text":"","title":"5. Apollo Studio Integration"},{"location":"technical/graphql-api/#51-schema-reporting","text":"The GraphQL API reports its schema to Apollo Studio for monitoring and exploration: # Apollo Studio integration from ariadne.contrib.apollo import ApolloTracingExtension from ariadne.contrib.federation import FederatedObjectType , make_federated_schema # Setup Apollo extensions if enabled extensions = [] if Config . APOLLO_STUDIO_ENABLED : extensions . append ( ApolloTracingExtension ) # Create GraphQL application with Apollo integration graphql_app = GraphQL ( schema , debug = Config . DEBUG , introspection = Config . GRAPHQL_GRAPHIQL , extensions = extensions ) # Apollo Studio configuration in settings APOLLO_KEY = os . getenv ( \"APOLLO_KEY\" ) APOLLO_GRAPH_REF = os . getenv ( \"APOLLO_GRAPH_REF\" , \"opossum-search@current\" ) APOLLO_SCHEMA_REPORTING = os . getenv ( \"APOLLO_SCHEMA_REPORTING\" , \"True\" ) . lower () == \"true\"","title":"5.1 Schema Reporting"},{"location":"technical/graphql-api/#52-performance-tracing","text":"Apollo tracing provides performance insights: # Apollo tracing middleware @app . middleware ( \"http\" ) async def apollo_tracing_middleware ( request , call_next ): # Skip non-GraphQL requests if not request . url . path == Config . GRAPHQL_ENDPOINT : return await call_next ( request ) # Add Apollo tracing header response = await call_next ( request ) response . headers [ \"apollo-federation-include-trace\" ] = ( \"ftv1\" if Config . APOLLO_INCLUDE_TRACES else \"false\" ) return response","title":"5.2 Performance Tracing"},{"location":"technical/graphql-api/#6-security-and-performance","text":"","title":"6. Security and Performance"},{"location":"technical/graphql-api/#61-depth-limiting","text":"Prevents deeply nested queries that could consume excessive resources: # GraphQL depth limitation from graphql import validation from graphql.language.ast import FieldNode , FragmentSpreadNode , InlineFragmentNode class DepthLimitRule ( validation . ValidationRule ): def __init__ ( self , max_depth ): super () . __init__ () self . max_depth = max_depth def enter_operation_definition ( self , node , key , parent , path , ancestors ): self . depth_map = {} return node def enter_field ( self , node , key , parent , path , ancestors ): depth = self . _get_depth ( ancestors ) if depth > self . max_depth : self . report_error ( f \"Query exceeds maximum depth of { self . max_depth } \" ) return node def _get_depth ( self , ancestors ): # Calculate current depth based on ancestry depth = 0 for ancestor in ancestors : if isinstance ( ancestor , ( FieldNode , FragmentSpreadNode , InlineFragmentNode )): depth += 1 return depth # Create depth limit rule depth_limit_rule = DepthLimitRule ( Config . GRAPHQL_DEPTH_LIMIT )","title":"6.1 Depth Limiting"},{"location":"technical/graphql-api/#62-cors-configuration","text":"Cross-Origin Resource Sharing settings for browser security: # CORS middleware configuration from starlette.middleware.cors import CORSMiddleware # Add CORS middleware app . add_middleware ( CORSMiddleware , allow_origins = Config . CORS_ALLOWED_ORIGINS , allow_credentials = Config . CORS_ALLOW_CREDENTIALS , allow_methods = [ \"GET\" , \"POST\" , \"OPTIONS\" ], allow_headers = Config . CORS_ALLOW_HEADERS , expose_headers = Config . CORS_EXPOSE_HEADERS )","title":"6.2 CORS Configuration"},{"location":"technical/graphql-api/#63-response-compression","text":"Reduces bandwidth usage for API responses: # Compression middleware from starlette.middleware.gzip import GZipMiddleware # Add compression middleware app . add_middleware ( GZipMiddleware , minimum_size = Config . COMPRESS_MIN_SIZE , compresslevel = Config . COMPRESS_LEVEL )","title":"6.3 Response Compression"},{"location":"technical/graphql-api/#64-api-key-authentication","text":"Secures the GraphQL API with API key validation: # API key authentication middleware @app . middleware ( \"http\" ) async def api_key_middleware ( request , call_next ): # Skip if API key auth is disabled if not Config . API_KEY_REQUIRED : return await call_next ( request ) # Skip non-GraphQL requests if not request . url . path == Config . GRAPHQL_ENDPOINT : return await call_next ( request ) # Check for API key in headers or query parameters api_key = request . headers . get ( \"x-api-key\" ) if not api_key : # Try query parameter query_params = request . query_params api_key = query_params . get ( \"api_key\" ) # Validate API key if not api_key or api_key != Config . API_KEY : return JSONResponse ( status_code = 401 , content = { \"error\" : \"Invalid or missing API key\" } ) # Proceed with request return await call_next ( request )","title":"6.4 API Key Authentication"},{"location":"technical/graphql-api/#7-integration-with-graphql-voyager","text":"For schema visualization and exploration, the application includes GraphQL Voyager: # Voyager integration from fastapi import FastAPI , Request from fastapi.responses import HTMLResponse from fastapi.templating import Jinja2Templates # Setup templates for Voyager templates = Jinja2Templates ( directory = \"templates\" ) # Add Voyager endpoint @app . get ( \"/voyager\" , response_class = HTMLResponse ) async def voyager ( request : Request ): \"\"\"Serve GraphQL Voyager interface\"\"\" if not Config . VOYAGER_ENABLED : return { \"error\" : \"GraphQL Voyager is disabled\" } return templates . TemplateResponse ( Config . VOYAGER_PATH , { \"request\" : request , \"graphql_endpoint\" : Config . GRAPHQL_ENDPOINT } )","title":"7. Integration with GraphQL Voyager"},{"location":"technical/graphql-api/#8-resolver-implementation","text":"Sample resolver implementation for the core queries: # Query resolvers query_resolvers = QueryType () @query_resolvers . field ( \"service_status\" ) async def resolve_service_status ( _ , info ): \"\"\"Resolve service status query\"\"\" service_monitor = ServiceMonitor () await service_monitor . check_all_services () return { \"gemini_available\" : service_monitor . services [ \"gemini\" ], \"ollama_available\" : service_monitor . services [ \"ollama\" ], \"transformers_available\" : service_monitor . services [ \"transformers\" ], \"all_services_available\" : all ( service_monitor . services . values ()), \"last_checked\" : datetime . datetime . now () . isoformat () } # Mutation resolvers mutation_resolvers = MutationType () @mutation_resolvers . field ( \"chat\" ) async def resolve_chat ( _ , info , message , conversation_id = None ): \"\"\"Resolve chat mutation\"\"\" # Create or retrieve conversation if not conversation_id : conversation_id = str ( uuid . uuid4 ()) # Get backend backend = HybridModelBackend () # Generate response response , metadata = await backend . generate_response ( message , conversation_id = conversation_id ) return { \"message\" : response , \"conversation_id\" : conversation_id , \"backend_used\" : metadata . get ( \"backend\" ), \"tokens_used\" : metadata . get ( \"tokens\" , 0 ) } The GraphQL API serves as the primary interface for Opossum Search, providing a flexible, secure, and performant way for clients to interact with the system. The schema-first approach with Ariadne enables clean separation of concerns, while directives handle cross-cutting concerns like authentication, rate limiting, and caching.","title":"8. Resolver Implementation"},{"location":"technical/hybrid-model-selection/","text":"Hybrid Model Architecture \u00b6 Overview \u00b6 The hybrid model architecture provides an intelligent layer for combining multiple model backends based on availability, capability, and request requirements. This document outlines the technical implementation and decision-making process. Related Documentation: - Technical: Hybrid Model Selection System - Detailed technical implementation of the selection algorithm - Model Integration: Backend Selection - Additional details on backend selection criteria - Technical: Redis Caching Architecture - How Redis supports the hybrid model architecture Selection Process Diagram \u00b6 graph TD A[Incoming Request] --> B{Has Image?} B -- Yes --> C{Gemini Available?} C -- Yes --> D[Use Gemini] C -- No --> E[Calculate Scores] B -- No --> E E --> F[Check Gemini Score] E --> G[Check Ollama Score] E --> H[Check Transformers Score] F --> I{Select Best Score} G --> I H --> I I --> J[Initialize Selected Backend] J --> K{Success?} K -- Yes --> L[Generate Response] K -- No --> M[Use Fallback] M --> N[Transformers Fallback] style D fill:#34a853,color:white style N fill:#ea4335,color:white style A fill:#4285f4,color:white Implementation Details \u00b6 Capability Scoring \u00b6 The hybrid model uses a weighted scoring system to evaluate each backend: CAPABILITY_WEIGHTS = { 'text_processing' : 0.3 , 'reasoning' : 0.3 , 'multimodal' : 0.2 , 'latency' : 0.1 , 'resource_usage' : 0.1 } Each backend is scored based on its capabilities in these areas, with the final selection going to the highest scoring available backend. Backend Management \u00b6 Backends are managed through lazy initialization: Backends are only created when needed Failed backends are not retried until next service check Successful backends are cached for reuse Resource cleanup happens automatically Error Handling \u00b6 The system implements multiple layers of error handling: Service availability monitoring Backend initialization retry logic Graceful degradation paths Clear error logging and reporting User-friendly error messages Configuration \u00b6 Backend settings are managed through: MODEL_CONFIGS = { \"gemini-thinking\" : { \"api_name\" : \"gemini-2.0-flash-thinking-exp-01-21\" , \"max_tokens\" : 1024 , \"temperature\" : 0.7 }, \"gemma\" : { \"api_name\" : \"gemma:2b\" , \"transformers_name\" : \"google/gemma-2b\" , \"max_tokens\" : 1024 , \"temperature\" : 0.6 } } Performance Characteristics \u00b6 Metric Target Actual (P95) Backend Selection Time < 50ms 35ms Initialization Time < 500ms 450ms Fallback Trigger Time < 2000ms 1800ms Cache Hit Response Time < 100ms 85ms Monitoring and Metrics \u00b6 The hybrid implementation exposes several key metrics: Backend selection times Initialization success rates Fallback frequency Cache hit rates Error rates by backend These metrics are available through the standard monitoring interfaces and can be used for alerting and optimization.","title":"Hybrid Selection"},{"location":"technical/hybrid-model-selection/#hybrid-model-architecture","text":"","title":"Hybrid Model Architecture"},{"location":"technical/hybrid-model-selection/#overview","text":"The hybrid model architecture provides an intelligent layer for combining multiple model backends based on availability, capability, and request requirements. This document outlines the technical implementation and decision-making process. Related Documentation: - Technical: Hybrid Model Selection System - Detailed technical implementation of the selection algorithm - Model Integration: Backend Selection - Additional details on backend selection criteria - Technical: Redis Caching Architecture - How Redis supports the hybrid model architecture","title":"Overview"},{"location":"technical/hybrid-model-selection/#selection-process-diagram","text":"graph TD A[Incoming Request] --> B{Has Image?} B -- Yes --> C{Gemini Available?} C -- Yes --> D[Use Gemini] C -- No --> E[Calculate Scores] B -- No --> E E --> F[Check Gemini Score] E --> G[Check Ollama Score] E --> H[Check Transformers Score] F --> I{Select Best Score} G --> I H --> I I --> J[Initialize Selected Backend] J --> K{Success?} K -- Yes --> L[Generate Response] K -- No --> M[Use Fallback] M --> N[Transformers Fallback] style D fill:#34a853,color:white style N fill:#ea4335,color:white style A fill:#4285f4,color:white","title":"Selection Process Diagram"},{"location":"technical/hybrid-model-selection/#implementation-details","text":"","title":"Implementation Details"},{"location":"technical/hybrid-model-selection/#capability-scoring","text":"The hybrid model uses a weighted scoring system to evaluate each backend: CAPABILITY_WEIGHTS = { 'text_processing' : 0.3 , 'reasoning' : 0.3 , 'multimodal' : 0.2 , 'latency' : 0.1 , 'resource_usage' : 0.1 } Each backend is scored based on its capabilities in these areas, with the final selection going to the highest scoring available backend.","title":"Capability Scoring"},{"location":"technical/hybrid-model-selection/#backend-management","text":"Backends are managed through lazy initialization: Backends are only created when needed Failed backends are not retried until next service check Successful backends are cached for reuse Resource cleanup happens automatically","title":"Backend Management"},{"location":"technical/hybrid-model-selection/#error-handling","text":"The system implements multiple layers of error handling: Service availability monitoring Backend initialization retry logic Graceful degradation paths Clear error logging and reporting User-friendly error messages","title":"Error Handling"},{"location":"technical/hybrid-model-selection/#configuration","text":"Backend settings are managed through: MODEL_CONFIGS = { \"gemini-thinking\" : { \"api_name\" : \"gemini-2.0-flash-thinking-exp-01-21\" , \"max_tokens\" : 1024 , \"temperature\" : 0.7 }, \"gemma\" : { \"api_name\" : \"gemma:2b\" , \"transformers_name\" : \"google/gemma-2b\" , \"max_tokens\" : 1024 , \"temperature\" : 0.6 } }","title":"Configuration"},{"location":"technical/hybrid-model-selection/#performance-characteristics","text":"Metric Target Actual (P95) Backend Selection Time < 50ms 35ms Initialization Time < 500ms 450ms Fallback Trigger Time < 2000ms 1800ms Cache Hit Response Time < 100ms 85ms","title":"Performance Characteristics"},{"location":"technical/hybrid-model-selection/#monitoring-and-metrics","text":"The hybrid implementation exposes several key metrics: Backend selection times Initialization success rates Fallback frequency Cache hit rates Error rates by backend These metrics are available through the standard monitoring interfaces and can be used for alerting and optimization.","title":"Monitoring and Metrics"},{"location":"technical/image-processing-pipeline/","text":"Technical Documentation: Image Processing Pipeline \u00b6 1. Overview \u00b6 The Image Processing Pipeline in Opossum Search handles multimodal queries containing images, preparing them for analysis by various AI backends. This pipeline ensures optimal image formatting for different models, performs necessary transformations, and extracts relevant metadata to enhance multimodal understanding. 2. Processing Stages \u00b6 2.1 Initial Processing \u00b6 async def process_incoming_image ( self , image_data , image_type = None ): \"\"\"Process incoming image data for multimodal analysis\"\"\" # Detect format if not provided if not image_type : image_type = self . _detect_image_format ( image_data ) # Validate image for security if not self . _validate_image ( image_data , image_type ): raise InvalidImageError ( \"Invalid or corrupted image data\" ) # Extract basic metadata metadata = await self . _extract_metadata ( image_data ) # Create processing context context = { \"original_size\" : metadata . get ( \"size\" , 0 ), \"dimensions\" : metadata . get ( \"dimensions\" , ( 0 , 0 )), \"format\" : image_type , \"content_type\" : f \"image/ { image_type } \" , \"requires_resize\" : any ( dim > Config . MAX_IMAGE_DIMENSION for dim in metadata . get ( \"dimensions\" , ( 0 , 0 ))) } return { \"image_data\" : image_data , \"metadata\" : metadata , \"context\" : context } Format Detection \u00b6 def _detect_image_format ( self , image_data ): \"\"\"Detect image format from binary data\"\"\" # Check magic bytes for common formats if image_data . startswith ( b ' \\xFF\\xD8\\xFF ' ): return \"jpeg\" elif image_data . startswith ( b ' \\x89 PNG \\r\\n\\x1a\\n ' ): return \"png\" elif image_data . startswith ( b 'GIF87a' ) or image_data . startswith ( b 'GIF89a' ): return \"gif\" elif image_data . startswith ( b 'RIFF' ) and image_data [ 8 : 12 ] == b 'WEBP' : return \"webp\" # Use ImageMagick for more complex detection with wand . image . Image ( blob = image_data ) as img : return img . format . lower () Metadata Extraction \u00b6 async def _extract_metadata ( self , image_data ): \"\"\"Extract metadata from image\"\"\" metadata = {} with wand . image . Image ( blob = image_data ) as img : # Basic properties metadata [ \"dimensions\" ] = ( img . width , img . height ) metadata [ \"format\" ] = img . format . lower () metadata [ \"size\" ] = len ( image_data ) metadata [ \"colorspace\" ] = str ( img . colorspace ) metadata [ \"depth\" ] = img . depth # Extract EXIF if available if hasattr ( img , 'metadata' ): exif = {} for k , v in img . metadata . items (): if k . startswith ( 'exif:' ): exif_key = k . split ( ':' , 1 )[ 1 ] exif [ exif_key ] = v if exif : metadata [ \"exif\" ] = exif return metadata 2.2 Transformation Pipeline \u00b6 async def transform_for_model ( self , image_data , target_model , context = None ): \"\"\"Transform image for specific model requirements\"\"\" context = context or {} transformations = [] # Determine required transformations based on model if target_model == \"gemini-thinking\" : transformations = [ self . _resize_image ( max_dimension = 1024 ), self . _convert_format ( target_format = \"jpeg\" , quality = 90 ) ] elif target_model == \"llava\" : transformations = [ self . _resize_image ( max_dimension = 512 ), self . _convert_format ( target_format = \"jpeg\" , quality = 80 ) ] else : # Default transformations transformations = [ self . _resize_image ( max_dimension = 768 ), self . _convert_format ( target_format = \"jpeg\" , quality = 85 ) ] # Apply transformations in sequence processed_image = image_data for transform_fn in transformations : processed_image = await transform_fn ( processed_image ) # Update context with transformed image details with wand . image . Image ( blob = processed_image ) as img : context [ \"transformed_size\" ] = len ( processed_image ) context [ \"transformed_dimensions\" ] = ( img . width , img . height ) return processed_image , context Resize Function \u00b6 def _resize_image ( self , max_dimension = 768 ): \"\"\"Create a resize transformation function\"\"\" async def resize_transform ( image_data ): with wand . image . Image ( blob = image_data ) as img : # Check if resize needed if img . width <= max_dimension and img . height <= max_dimension : return image_data # Calculate new dimensions maintaining aspect ratio ratio = min ( max_dimension / img . width , max_dimension / img . height ) new_width = int ( img . width * ratio ) new_height = int ( img . height * ratio ) # Perform resize img . resize ( new_width , new_height ) # Return resized image return img . make_blob () return resize_transform Format Conversion \u00b6 def _convert_format ( self , target_format = \"jpeg\" , quality = 85 ): \"\"\"Create a format conversion transformation function\"\"\" async def format_transform ( image_data ): with wand . image . Image ( blob = image_data ) as img : # Set format and quality img . format = target_format if hasattr ( img , 'compression_quality' ): img . compression_quality = quality # Handle transparency for JPEG conversion if target_format . lower () == 'jpeg' and img . alpha_channel : # Add white background for transparent images with wand . image . Image ( width = img . width , height = img . height , background = wand . color . Color ( 'white' ) ) as bg : bg . composite ( img , 0 , 0 ) return bg . make_blob ( format = target_format ) # Standard conversion return img . make_blob ( format = target_format ) return format_transform 2.3 Parallel Processing \u00b6 For efficiency, the system processes multiple transformations in parallel: async def parallel_image_processing ( self , image_data ): \"\"\"Process image with parallel transformations\"\"\" # Define processing tasks tasks = [ self . _resize_image ( max_dimension = 512 )( image_data ), # Standard size self . _extract_metadata ( image_data ), # Metadata extraction self . _generate_thumbnail ( image_data ) # Thumbnail generation ] # Run processing tasks in parallel resized , metadata , thumbnail = await asyncio . gather ( * tasks ) # Combine results return { \"image\" : resized , \"metadata\" : metadata , \"thumbnail\" : thumbnail } 3. Multimodal Routing \u00b6 3.1 Model Capability Assessment \u00b6 The system routes images to appropriate models based on their capabilities: async def select_model_for_image ( self , image_context , query_text = None ): \"\"\"Select appropriate model for image processing\"\"\" # Check for image-specific features has_text = image_context . get ( \"has_text\" , False ) is_chart = image_context . get ( \"is_chart\" , False ) needs_detail = image_context . get ( \"high_detail\" , False ) # Score each available model scores = {} if self . is_available ( \"gemini-thinking\" ): scores [ \"gemini-thinking\" ] = 10 # Base score if needs_detail : scores [ \"gemini-thinking\" ] += 5 # Good with details if self . is_available ( \"llava\" ): scores [ \"llava\" ] = 8 # Base score if has_text : scores [ \"llava\" ] += 3 # Good with text in images # Get highest scoring model if not scores : return Config . DEFAULT_MODEL # No multimodal models available return max ( scores . items (), key = lambda x : x [ 1 ])[ 0 ] 3.2 Image Analysis Pre-processing \u00b6 Before sending to model, images may undergo analysis to guide model selection: async def analyze_image_content ( self , image_data ): \"\"\"Analyze image to determine key characteristics\"\"\" analysis = { \"has_text\" : False , \"is_chart\" : False , \"is_photo\" : False , \"high_detail\" : False } # Use MiniLM for quick embedding-based classification if self . minilm_embedder : # Extract image features features = await self . _extract_image_features ( image_data ) # Compare with feature vectors for different image types text_score = cosine_similarity ( features , self . text_image_embedding ) chart_score = cosine_similarity ( features , self . chart_image_embedding ) photo_score = cosine_similarity ( features , self . photo_image_embedding ) # Set flags based on similarity scores if text_score > 0.65 : analysis [ \"has_text\" ] = True if chart_score > 0.7 : analysis [ \"is_chart\" ] = True if photo_score > 0.75 : analysis [ \"is_photo\" ] = True # Check for high detail based on entropy analysis [ \"high_detail\" ] = await self . _check_image_detail ( image_data ) return analysis 4. Image-to-Base64 Conversion \u00b6 For API transmission, images are encoded to base64: def encode_image_for_api ( self , image_data , prefix = True ): \"\"\"Encode image data as base64 for API transmission\"\"\" # Get image format if available image_format = \"jpeg\" # Default try : with wand . image . Image ( blob = image_data ) as img : image_format = img . format . lower () except Exception : pass # Base64 encode b64_data = base64 . b64encode ( image_data ) . decode ( 'utf-8' ) # Add data URI prefix if requested if prefix : return f \"data:image/ { image_format } ;base64, { b64_data } \" return b64_data 5. Security Considerations \u00b6 5.1 Image Validation \u00b6 def _validate_image ( self , image_data , image_type = None ): \"\"\"Validate image for security and integrity\"\"\" # Size validation if len ( image_data ) > Config . MAX_IMAGE_SIZE : logger . warning ( f \"Image exceeds maximum size: { len ( image_data ) } bytes\" ) return False # Format validation allowed_formats = [ 'jpeg' , 'jpg' , 'png' , 'gif' , 'webp' ] if image_type and image_type . lower () not in allowed_formats : logger . warning ( f \"Image format not allowed: { image_type } \" ) return False # Integrity check - attempt to open and process try : with wand . image . Image ( blob = image_data ) as img : # Check for potential dangerous sizes if img . width > 10000 or img . height > 10000 : logger . warning ( f \"Suspicious image dimensions: { img . width } x { img . height } \" ) return False # Additional validation can be performed here # Image passed all checks return True except Exception as e : logger . warning ( f \"Image validation failed: { e } \" ) return False 5.2 Malware Scanning \u00b6 For production environments, the system can integrate with ClamAV: async def scan_image_for_malware ( self , image_data ): \"\"\"Scan image for malware (requires ClamAV)\"\"\" # Skip if scanning disabled if not Config . ENABLE_MALWARE_SCAN : return True # Connect to ClamAV daemon try : clamd = clamd . ClamdNetworkSocket ( host = Config . CLAMAV_HOST , port = Config . CLAMAV_PORT ) # Scan the image data scan_result = clamd . instream ( io . BytesIO ( image_data )) # Check result status = scan_result [ 'stream' ][ 0 ] if status == 'OK' : return True else : logger . warning ( f \"Malware scan detected: { status } \" ) return False except Exception as e : logger . error ( f \"Malware scan error: { e } \" ) # Fail open or closed based on config return not Config . FAIL_CLOSED_ON_SCAN_ERROR 6. Performance Optimization \u00b6 6.1 Image Processing Caching \u00b6 async def get_processed_image ( self , image_hash , target_model ): \"\"\"Get processed image from cache or process it\"\"\" cache_key = f \"image:processed: { image_hash } : { target_model } \" # Check cache cached = await self . redis . get ( cache_key ) if cached : return pickle . loads ( cached ) # Process image if not cached original = await self . get_original_image ( image_hash ) if not original : return None # Process for target model processed , context = await self . transform_for_model ( original , target_model ) # Cache processed version await self . redis . setex ( cache_key , Config . IMAGE_CACHE_TTL , pickle . dumps (( processed , context )) ) return processed , context 6.2 Image Compression Strategies \u00b6 async def optimize_for_bandwidth ( self , image_data , context ): \"\"\"Optimize image based on network conditions\"\"\" # Get network context bandwidth = context . get ( \"bandwidth\" , \"high\" ) if bandwidth == \"low\" : # Aggressive optimization for low bandwidth return await self . _apply_transformations ( image_data , [ self . _resize_image ( max_dimension = 384 ), self . _convert_format ( target_format = \"jpeg\" , quality = 65 ) ]) elif bandwidth == \"medium\" : # Moderate optimization return await self . _apply_transformations ( image_data , [ self . _resize_image ( max_dimension = 512 ), self . _convert_format ( target_format = \"jpeg\" , quality = 75 ) ]) else : # High bandwidth - minimal optimization return await self . _apply_transformations ( image_data , [ self . _resize_image ( max_dimension = 768 ), self . _convert_format ( target_format = \"jpeg\" , quality = 85 ) ]) 7. Integration with LLM Pipeline \u00b6 7.1 Multimodal Message Construction \u00b6 def construct_multimodal_request ( self , text_query , image_data , model ): \"\"\"Construct multimodal message for different backends\"\"\" if model == \"gemini-thinking\" : # Gemini API format return { \"contents\" : [ { \"role\" : \"user\" , \"parts\" : [ { \"text\" : text_query }, { \"inline_data\" : { \"mime_type\" : \"image/jpeg\" , \"data\" : self . encode_image_for_api ( image_data , prefix = False ) }} ] } ] } elif model == \"llava\" : # Ollama format for LLaVA return { \"model\" : Config . LLAVA_MODEL , \"prompt\" : text_query , \"images\" : [ self . encode_image_for_api ( image_data , prefix = False )] } else : # Default format return { \"query\" : text_query , \"image\" : self . encode_image_for_api ( image_data ) } 8. Error Handling \u00b6 class ImageProcessingError ( Exception ): \"\"\"Base class for image processing errors\"\"\" pass class InvalidImageError ( ImageProcessingError ): \"\"\"Invalid image data or format\"\"\" pass class ImageTooLargeError ( ImageProcessingError ): \"\"\"Image exceeds size limits\"\"\" pass class ProcessingFailedError ( ImageProcessingError ): \"\"\"Generic processing failure\"\"\" pass async def handle_processing_error ( self , error , image_context ): \"\"\"Handle image processing errors gracefully\"\"\" if isinstance ( error , InvalidImageError ): return { \"error\" : \"Invalid image format or corrupted data\" } elif isinstance ( error , ImageTooLargeError ): return { \"error\" : f \"Image too large. Maximum size: { Config . MAX_IMAGE_SIZE / 1024 / 1024 } MB\" } else : # Log unexpected errors logger . error ( f \"Image processing error: { error } \" , exc_info = True ) return { \"error\" : \"Failed to process image\" } 9. Future Enhancements \u00b6 The image processing pipeline roadmap includes: Content-Aware Processing : Adjusting processing based on image content (text, faces, etc.) Progressive Image Loading : Sending low-res versions first, then upgrading Format-Specific Optimizations : Custom processing for WebP, AVIF, etc. ML-Based Image Analysis : Pre-classification of images before sending to LLMs Animated GIF/WebP Support : Handling of animated content This image processing pipeline provides Opossum Search with robust multimodal capabilities while ensuring optimal performance, security, and compatibility with various LLM backends.","title":"Image Pipeline"},{"location":"technical/image-processing-pipeline/#technical-documentation-image-processing-pipeline","text":"","title":"Technical Documentation: Image Processing Pipeline"},{"location":"technical/image-processing-pipeline/#1-overview","text":"The Image Processing Pipeline in Opossum Search handles multimodal queries containing images, preparing them for analysis by various AI backends. This pipeline ensures optimal image formatting for different models, performs necessary transformations, and extracts relevant metadata to enhance multimodal understanding.","title":"1. Overview"},{"location":"technical/image-processing-pipeline/#2-processing-stages","text":"","title":"2. Processing Stages"},{"location":"technical/image-processing-pipeline/#21-initial-processing","text":"async def process_incoming_image ( self , image_data , image_type = None ): \"\"\"Process incoming image data for multimodal analysis\"\"\" # Detect format if not provided if not image_type : image_type = self . _detect_image_format ( image_data ) # Validate image for security if not self . _validate_image ( image_data , image_type ): raise InvalidImageError ( \"Invalid or corrupted image data\" ) # Extract basic metadata metadata = await self . _extract_metadata ( image_data ) # Create processing context context = { \"original_size\" : metadata . get ( \"size\" , 0 ), \"dimensions\" : metadata . get ( \"dimensions\" , ( 0 , 0 )), \"format\" : image_type , \"content_type\" : f \"image/ { image_type } \" , \"requires_resize\" : any ( dim > Config . MAX_IMAGE_DIMENSION for dim in metadata . get ( \"dimensions\" , ( 0 , 0 ))) } return { \"image_data\" : image_data , \"metadata\" : metadata , \"context\" : context }","title":"2.1 Initial Processing"},{"location":"technical/image-processing-pipeline/#format-detection","text":"def _detect_image_format ( self , image_data ): \"\"\"Detect image format from binary data\"\"\" # Check magic bytes for common formats if image_data . startswith ( b ' \\xFF\\xD8\\xFF ' ): return \"jpeg\" elif image_data . startswith ( b ' \\x89 PNG \\r\\n\\x1a\\n ' ): return \"png\" elif image_data . startswith ( b 'GIF87a' ) or image_data . startswith ( b 'GIF89a' ): return \"gif\" elif image_data . startswith ( b 'RIFF' ) and image_data [ 8 : 12 ] == b 'WEBP' : return \"webp\" # Use ImageMagick for more complex detection with wand . image . Image ( blob = image_data ) as img : return img . format . lower ()","title":"Format Detection"},{"location":"technical/image-processing-pipeline/#metadata-extraction","text":"async def _extract_metadata ( self , image_data ): \"\"\"Extract metadata from image\"\"\" metadata = {} with wand . image . Image ( blob = image_data ) as img : # Basic properties metadata [ \"dimensions\" ] = ( img . width , img . height ) metadata [ \"format\" ] = img . format . lower () metadata [ \"size\" ] = len ( image_data ) metadata [ \"colorspace\" ] = str ( img . colorspace ) metadata [ \"depth\" ] = img . depth # Extract EXIF if available if hasattr ( img , 'metadata' ): exif = {} for k , v in img . metadata . items (): if k . startswith ( 'exif:' ): exif_key = k . split ( ':' , 1 )[ 1 ] exif [ exif_key ] = v if exif : metadata [ \"exif\" ] = exif return metadata","title":"Metadata Extraction"},{"location":"technical/image-processing-pipeline/#22-transformation-pipeline","text":"async def transform_for_model ( self , image_data , target_model , context = None ): \"\"\"Transform image for specific model requirements\"\"\" context = context or {} transformations = [] # Determine required transformations based on model if target_model == \"gemini-thinking\" : transformations = [ self . _resize_image ( max_dimension = 1024 ), self . _convert_format ( target_format = \"jpeg\" , quality = 90 ) ] elif target_model == \"llava\" : transformations = [ self . _resize_image ( max_dimension = 512 ), self . _convert_format ( target_format = \"jpeg\" , quality = 80 ) ] else : # Default transformations transformations = [ self . _resize_image ( max_dimension = 768 ), self . _convert_format ( target_format = \"jpeg\" , quality = 85 ) ] # Apply transformations in sequence processed_image = image_data for transform_fn in transformations : processed_image = await transform_fn ( processed_image ) # Update context with transformed image details with wand . image . Image ( blob = processed_image ) as img : context [ \"transformed_size\" ] = len ( processed_image ) context [ \"transformed_dimensions\" ] = ( img . width , img . height ) return processed_image , context","title":"2.2 Transformation Pipeline"},{"location":"technical/image-processing-pipeline/#resize-function","text":"def _resize_image ( self , max_dimension = 768 ): \"\"\"Create a resize transformation function\"\"\" async def resize_transform ( image_data ): with wand . image . Image ( blob = image_data ) as img : # Check if resize needed if img . width <= max_dimension and img . height <= max_dimension : return image_data # Calculate new dimensions maintaining aspect ratio ratio = min ( max_dimension / img . width , max_dimension / img . height ) new_width = int ( img . width * ratio ) new_height = int ( img . height * ratio ) # Perform resize img . resize ( new_width , new_height ) # Return resized image return img . make_blob () return resize_transform","title":"Resize Function"},{"location":"technical/image-processing-pipeline/#format-conversion","text":"def _convert_format ( self , target_format = \"jpeg\" , quality = 85 ): \"\"\"Create a format conversion transformation function\"\"\" async def format_transform ( image_data ): with wand . image . Image ( blob = image_data ) as img : # Set format and quality img . format = target_format if hasattr ( img , 'compression_quality' ): img . compression_quality = quality # Handle transparency for JPEG conversion if target_format . lower () == 'jpeg' and img . alpha_channel : # Add white background for transparent images with wand . image . Image ( width = img . width , height = img . height , background = wand . color . Color ( 'white' ) ) as bg : bg . composite ( img , 0 , 0 ) return bg . make_blob ( format = target_format ) # Standard conversion return img . make_blob ( format = target_format ) return format_transform","title":"Format Conversion"},{"location":"technical/image-processing-pipeline/#23-parallel-processing","text":"For efficiency, the system processes multiple transformations in parallel: async def parallel_image_processing ( self , image_data ): \"\"\"Process image with parallel transformations\"\"\" # Define processing tasks tasks = [ self . _resize_image ( max_dimension = 512 )( image_data ), # Standard size self . _extract_metadata ( image_data ), # Metadata extraction self . _generate_thumbnail ( image_data ) # Thumbnail generation ] # Run processing tasks in parallel resized , metadata , thumbnail = await asyncio . gather ( * tasks ) # Combine results return { \"image\" : resized , \"metadata\" : metadata , \"thumbnail\" : thumbnail }","title":"2.3 Parallel Processing"},{"location":"technical/image-processing-pipeline/#3-multimodal-routing","text":"","title":"3. Multimodal Routing"},{"location":"technical/image-processing-pipeline/#31-model-capability-assessment","text":"The system routes images to appropriate models based on their capabilities: async def select_model_for_image ( self , image_context , query_text = None ): \"\"\"Select appropriate model for image processing\"\"\" # Check for image-specific features has_text = image_context . get ( \"has_text\" , False ) is_chart = image_context . get ( \"is_chart\" , False ) needs_detail = image_context . get ( \"high_detail\" , False ) # Score each available model scores = {} if self . is_available ( \"gemini-thinking\" ): scores [ \"gemini-thinking\" ] = 10 # Base score if needs_detail : scores [ \"gemini-thinking\" ] += 5 # Good with details if self . is_available ( \"llava\" ): scores [ \"llava\" ] = 8 # Base score if has_text : scores [ \"llava\" ] += 3 # Good with text in images # Get highest scoring model if not scores : return Config . DEFAULT_MODEL # No multimodal models available return max ( scores . items (), key = lambda x : x [ 1 ])[ 0 ]","title":"3.1 Model Capability Assessment"},{"location":"technical/image-processing-pipeline/#32-image-analysis-pre-processing","text":"Before sending to model, images may undergo analysis to guide model selection: async def analyze_image_content ( self , image_data ): \"\"\"Analyze image to determine key characteristics\"\"\" analysis = { \"has_text\" : False , \"is_chart\" : False , \"is_photo\" : False , \"high_detail\" : False } # Use MiniLM for quick embedding-based classification if self . minilm_embedder : # Extract image features features = await self . _extract_image_features ( image_data ) # Compare with feature vectors for different image types text_score = cosine_similarity ( features , self . text_image_embedding ) chart_score = cosine_similarity ( features , self . chart_image_embedding ) photo_score = cosine_similarity ( features , self . photo_image_embedding ) # Set flags based on similarity scores if text_score > 0.65 : analysis [ \"has_text\" ] = True if chart_score > 0.7 : analysis [ \"is_chart\" ] = True if photo_score > 0.75 : analysis [ \"is_photo\" ] = True # Check for high detail based on entropy analysis [ \"high_detail\" ] = await self . _check_image_detail ( image_data ) return analysis","title":"3.2 Image Analysis Pre-processing"},{"location":"technical/image-processing-pipeline/#4-image-to-base64-conversion","text":"For API transmission, images are encoded to base64: def encode_image_for_api ( self , image_data , prefix = True ): \"\"\"Encode image data as base64 for API transmission\"\"\" # Get image format if available image_format = \"jpeg\" # Default try : with wand . image . Image ( blob = image_data ) as img : image_format = img . format . lower () except Exception : pass # Base64 encode b64_data = base64 . b64encode ( image_data ) . decode ( 'utf-8' ) # Add data URI prefix if requested if prefix : return f \"data:image/ { image_format } ;base64, { b64_data } \" return b64_data","title":"4. Image-to-Base64 Conversion"},{"location":"technical/image-processing-pipeline/#5-security-considerations","text":"","title":"5. Security Considerations"},{"location":"technical/image-processing-pipeline/#51-image-validation","text":"def _validate_image ( self , image_data , image_type = None ): \"\"\"Validate image for security and integrity\"\"\" # Size validation if len ( image_data ) > Config . MAX_IMAGE_SIZE : logger . warning ( f \"Image exceeds maximum size: { len ( image_data ) } bytes\" ) return False # Format validation allowed_formats = [ 'jpeg' , 'jpg' , 'png' , 'gif' , 'webp' ] if image_type and image_type . lower () not in allowed_formats : logger . warning ( f \"Image format not allowed: { image_type } \" ) return False # Integrity check - attempt to open and process try : with wand . image . Image ( blob = image_data ) as img : # Check for potential dangerous sizes if img . width > 10000 or img . height > 10000 : logger . warning ( f \"Suspicious image dimensions: { img . width } x { img . height } \" ) return False # Additional validation can be performed here # Image passed all checks return True except Exception as e : logger . warning ( f \"Image validation failed: { e } \" ) return False","title":"5.1 Image Validation"},{"location":"technical/image-processing-pipeline/#52-malware-scanning","text":"For production environments, the system can integrate with ClamAV: async def scan_image_for_malware ( self , image_data ): \"\"\"Scan image for malware (requires ClamAV)\"\"\" # Skip if scanning disabled if not Config . ENABLE_MALWARE_SCAN : return True # Connect to ClamAV daemon try : clamd = clamd . ClamdNetworkSocket ( host = Config . CLAMAV_HOST , port = Config . CLAMAV_PORT ) # Scan the image data scan_result = clamd . instream ( io . BytesIO ( image_data )) # Check result status = scan_result [ 'stream' ][ 0 ] if status == 'OK' : return True else : logger . warning ( f \"Malware scan detected: { status } \" ) return False except Exception as e : logger . error ( f \"Malware scan error: { e } \" ) # Fail open or closed based on config return not Config . FAIL_CLOSED_ON_SCAN_ERROR","title":"5.2 Malware Scanning"},{"location":"technical/image-processing-pipeline/#6-performance-optimization","text":"","title":"6. Performance Optimization"},{"location":"technical/image-processing-pipeline/#61-image-processing-caching","text":"async def get_processed_image ( self , image_hash , target_model ): \"\"\"Get processed image from cache or process it\"\"\" cache_key = f \"image:processed: { image_hash } : { target_model } \" # Check cache cached = await self . redis . get ( cache_key ) if cached : return pickle . loads ( cached ) # Process image if not cached original = await self . get_original_image ( image_hash ) if not original : return None # Process for target model processed , context = await self . transform_for_model ( original , target_model ) # Cache processed version await self . redis . setex ( cache_key , Config . IMAGE_CACHE_TTL , pickle . dumps (( processed , context )) ) return processed , context","title":"6.1 Image Processing Caching"},{"location":"technical/image-processing-pipeline/#62-image-compression-strategies","text":"async def optimize_for_bandwidth ( self , image_data , context ): \"\"\"Optimize image based on network conditions\"\"\" # Get network context bandwidth = context . get ( \"bandwidth\" , \"high\" ) if bandwidth == \"low\" : # Aggressive optimization for low bandwidth return await self . _apply_transformations ( image_data , [ self . _resize_image ( max_dimension = 384 ), self . _convert_format ( target_format = \"jpeg\" , quality = 65 ) ]) elif bandwidth == \"medium\" : # Moderate optimization return await self . _apply_transformations ( image_data , [ self . _resize_image ( max_dimension = 512 ), self . _convert_format ( target_format = \"jpeg\" , quality = 75 ) ]) else : # High bandwidth - minimal optimization return await self . _apply_transformations ( image_data , [ self . _resize_image ( max_dimension = 768 ), self . _convert_format ( target_format = \"jpeg\" , quality = 85 ) ])","title":"6.2 Image Compression Strategies"},{"location":"technical/image-processing-pipeline/#7-integration-with-llm-pipeline","text":"","title":"7. Integration with LLM Pipeline"},{"location":"technical/image-processing-pipeline/#71-multimodal-message-construction","text":"def construct_multimodal_request ( self , text_query , image_data , model ): \"\"\"Construct multimodal message for different backends\"\"\" if model == \"gemini-thinking\" : # Gemini API format return { \"contents\" : [ { \"role\" : \"user\" , \"parts\" : [ { \"text\" : text_query }, { \"inline_data\" : { \"mime_type\" : \"image/jpeg\" , \"data\" : self . encode_image_for_api ( image_data , prefix = False ) }} ] } ] } elif model == \"llava\" : # Ollama format for LLaVA return { \"model\" : Config . LLAVA_MODEL , \"prompt\" : text_query , \"images\" : [ self . encode_image_for_api ( image_data , prefix = False )] } else : # Default format return { \"query\" : text_query , \"image\" : self . encode_image_for_api ( image_data ) }","title":"7.1 Multimodal Message Construction"},{"location":"technical/image-processing-pipeline/#8-error-handling","text":"class ImageProcessingError ( Exception ): \"\"\"Base class for image processing errors\"\"\" pass class InvalidImageError ( ImageProcessingError ): \"\"\"Invalid image data or format\"\"\" pass class ImageTooLargeError ( ImageProcessingError ): \"\"\"Image exceeds size limits\"\"\" pass class ProcessingFailedError ( ImageProcessingError ): \"\"\"Generic processing failure\"\"\" pass async def handle_processing_error ( self , error , image_context ): \"\"\"Handle image processing errors gracefully\"\"\" if isinstance ( error , InvalidImageError ): return { \"error\" : \"Invalid image format or corrupted data\" } elif isinstance ( error , ImageTooLargeError ): return { \"error\" : f \"Image too large. Maximum size: { Config . MAX_IMAGE_SIZE / 1024 / 1024 } MB\" } else : # Log unexpected errors logger . error ( f \"Image processing error: { error } \" , exc_info = True ) return { \"error\" : \"Failed to process image\" }","title":"8. Error Handling"},{"location":"technical/image-processing-pipeline/#9-future-enhancements","text":"The image processing pipeline roadmap includes: Content-Aware Processing : Adjusting processing based on image content (text, faces, etc.) Progressive Image Loading : Sending low-res versions first, then upgrading Format-Specific Optimizations : Custom processing for WebP, AVIF, etc. ML-Based Image Analysis : Pre-classification of images before sending to LLMs Animated GIF/WebP Support : Handling of animated content This image processing pipeline provides Opossum Search with robust multimodal capabilities while ensuring optimal performance, security, and compatibility with various LLM backends.","title":"9. Future Enhancements"},{"location":"technical/opentelemetry-integration/","text":"Technical Documentation: OpenTelemetry Integration \u00b6 1. Overview \u00b6 Opossum Search implements comprehensive observability using OpenTelemetry to monitor application performance, track request flows, and diagnose issues across its distributed architecture. This integration enables detailed visibility into system behavior, performance bottlenecks, and service dependencies. 2. OpenTelemetry Configuration \u00b6 # OpenTelemetry Configuration OTEL_EXPORTER_OTLP_ENDPOINT = os . getenv ( \"OTEL_EXPORTER_OTLP_ENDPOINT\" , \"http://localhost:4318/v1/traces\" ) OTEL_SERVICE_NAME = os . getenv ( \"OTEL_SERVICE_NAME\" , \"opossum-search\" ) OTEL_ENABLED = os . getenv ( \"OTEL_ENABLED\" , \"True\" ) . lower () == \"true\" 2.1 Initialization \u00b6 from opentelemetry import trace from opentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.trace.export import BatchSpanProcessor from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter from opentelemetry.sdk.resources import Resource from opentelemetry.semconv.resource import ResourceAttributes def initialize_telemetry (): \"\"\"Initialize OpenTelemetry for distributed tracing\"\"\" if not Config . OTEL_ENABLED : return # Create resource information resource = Resource . create ({ ResourceAttributes . SERVICE_NAME : Config . OTEL_SERVICE_NAME , ResourceAttributes . SERVICE_VERSION : \"1.0.0\" , \"environment\" : Config . ENV }) # Set up tracer provider provider = TracerProvider ( resource = resource ) # Create OTLP exporter otlp_exporter = OTLPSpanExporter ( endpoint = Config . OTEL_EXPORTER_OTLP_ENDPOINT ) span_processor = BatchSpanProcessor ( otlp_exporter ) provider . add_span_processor ( span_processor ) # Set global tracer provider trace . set_tracer_provider ( provider ) # Create global tracer return trace . get_tracer ( __name__ ) 3. Tracing Request Flow \u00b6 3.1 GraphQL Request Tracing \u00b6 from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor # Instrument FastAPI application def instrument_fastapi ( app ): \"\"\"Instrument FastAPI with OpenTelemetry\"\"\" if Config . OTEL_ENABLED : FastAPIInstrumentor . instrument_app ( app , excluded_urls = \"/health,/metrics\" , tracer_provider = trace . get_tracer_provider () ) 3.2 Model Backend Tracing \u00b6 async def generate_response ( self , prompt , model_name ): \"\"\"Generate response with OpenTelemetry tracing\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"model.generate\" , attributes = { \"prompt.length\" : len ( prompt ), \"model.name\" : model_name } ) as span : try : # Record start time start_time = time . time () # Call model backend response = await self . _call_model_backend ( prompt , model_name ) # Calculate and record duration duration = time . time () - start_time span . set_attribute ( \"duration_seconds\" , duration ) span . set_attribute ( \"response.length\" , len ( response )) return response except Exception as e : # Record error information span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) raise 4. Custom Metrics and Events \u00b6 4.1 Service Availability Monitoring \u00b6 async def check_service ( self , service_name ): \"\"\"Check service availability with metrics recording\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"service.check\" , attributes = { \"service.name\" : service_name } ) as span : # Record check attempt span . add_event ( \"check_started\" ) try : # Perform actual check is_available = await self . _perform_actual_check ( service_name ) # Record result span . set_attribute ( \"service.available\" , is_available ) span . add_event ( \"check_completed\" , { \"result\" : \"available\" if is_available else \"unavailable\" } ) return is_available except Exception as e : # Record error span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) span . add_event ( \"check_failed\" , { \"error\" : str ( e )}) return False 4.2 Cache Hit/Miss Monitoring \u00b6 async def get_cached_item ( self , key ): \"\"\"Get cached item with telemetry\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"cache.get\" , attributes = { \"cache.key\" : key } ) as span : # Try to get from cache value = await self . redis_client . get ( key ) # Record hit/miss hit = value is not None span . set_attribute ( \"cache.hit\" , hit ) # Record in aggregated metrics if hit : await self . redis_client . incr ( \"metrics:cache:hits\" ) else : await self . redis_client . incr ( \"metrics:cache:misses\" ) return value 5. Custom Context Propagation \u00b6 from opentelemetry.context import Context , get_current from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator class ContextCarrier : \"\"\"Carries context between async tasks\"\"\" def __init__ ( self ): self . context = {} def get ( self , key , default = None ): return self . context . get ( key , default ) def set ( self , key , value ): self . context [ key ] = value def extract_context_for_background_task (): \"\"\"Extract current context to pass to background task\"\"\" carrier = ContextCarrier () propagator = TraceContextTextMapPropagator () propagator . inject ( carrier , get_current ()) return carrier async def run_in_background ( func , * args , parent_context = None , ** kwargs ): \"\"\"Run function in background with parent trace context\"\"\" if parent_context and Config . OTEL_ENABLED : propagator = TraceContextTextMapPropagator () context = propagator . extract ( parent_context ) token = context . attach () try : return await func ( * args , ** kwargs ) finally : context . detach ( token ) else : return await func ( * args , ** kwargs ) 6. External Service Integration \u00b6 6.1 Ollama API Tracing \u00b6 async def generate_ollama_response ( self , prompt , model ): \"\"\"Trace Ollama API requests\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"ollama.generate\" , attributes = { \"prompt.length\" : len ( prompt ), \"model.name\" : model } ) as span : try : # Create request url = f \" { Config . OLLAMA_BASE_URL } /api/generate\" payload = { \"prompt\" : prompt , \"model\" : model , \"stream\" : False } # Add request details to span span . set_attribute ( \"http.url\" , url ) span . set_attribute ( \"http.method\" , \"POST\" ) # Send request start_time = time . time () async with httpx . AsyncClient () as client : response = await client . post ( url , json = payload , timeout = 60.0 ) # Add response details span . set_attribute ( \"http.status_code\" , response . status_code ) span . set_attribute ( \"http.response_time\" , time . time () - start_time ) # Handle response if response . status_code == 200 : data = response . json () return data . get ( \"response\" , \"\" ) else : error_msg = f \"Ollama API error: { response . status_code } \" span . set_status ( trace . StatusCode . ERROR , error_msg ) raise Exception ( error_msg ) except Exception as e : span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) raise 6.2 Gemini API Tracing \u00b6 async def generate_gemini_response ( self , prompt , image_data = None ): \"\"\"Trace Gemini API requests\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"gemini.generate\" , attributes = { \"prompt.length\" : len ( prompt ), \"has_image\" : image_data is not None } ) as span : try : # Initialize Gemini client gemini_client = AsyncGeminiClient ( api_key = Config . GEMINI_API_KEY ) model = Config . MODEL_CONFIGS [ \"gemini-thinking\" ][ \"api_name\" ] # Add request details to span span . set_attribute ( \"model.name\" , model ) # Prepare content parts content_parts = [{ \"text\" : prompt }] if image_data : # Add image to content parts span . set_attribute ( \"image.size\" , len ( image_data )) content_parts . append ({ \"inline_data\" : { \"mime_type\" : \"image/jpeg\" , \"data\" : base64 . b64encode ( image_data ) . decode ( 'utf-8' ) } }) # Generate response start_time = time . time () response = await gemini_client . generate_content ( model = model , contents = [{ \"parts\" : content_parts }] ) # Record performance metrics duration = time . time () - start_time span . set_attribute ( \"duration_seconds\" , duration ) # Extract response text if response and response . candidates : result = response . candidates [ 0 ] . content . parts [ 0 ] . text span . set_attribute ( \"response.length\" , len ( result )) return result else : span . set_status ( trace . StatusCode . ERROR , \"Empty response from Gemini\" ) return \"No response generated\" except Exception as e : span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) raise 7. Visualization and Monitoring \u00b6 # Visualization endpoints @app . get ( \"/metrics\" ) async def metrics (): \"\"\"Expose metrics for Prometheus scraping\"\"\" if not Config . OTEL_ENABLED : return { \"error\" : \"OpenTelemetry is disabled\" } return Response ( content = generate_latest () . decode ( \"utf-8\" ), media_type = \"text/plain\" ) @app . get ( \"/traces/recent\" ) async def recent_traces (): \"\"\"Get recent traces for UI display\"\"\" # This would typically be handled by a trace visualization tool like Jaeger # This endpoint just provides basic info for quick debugging recent = await redis_client . lrange ( \"traces:recent\" , 0 , 9 ) return [ json . loads ( trace ) for trace in recent ] 8. Deployment Configuration \u00b6 For production deployments, OpenTelemetry traces are sent to a collector service that can export to various backends: # docker-compose excerpt for OpenTelemetry collector services : otel-collector : image : otel/opentelemetry-collector:0.97.0 command : [ \"--config=/etc/otel-config.yaml\" ] volumes : - ./otel-config.yaml:/etc/otel-config.yaml ports : - \"4318:4318\" # OTLP HTTP receiver - \"9464:9464\" # Prometheus exporter # Visualization with Jaeger jaeger : image : jaegertracing/all-in-one:latest ports : - \"16686:16686\" # Jaeger UI - \"14250:14250\" # Receive from collector 8.1 Collector Configuration \u00b6 # otel-config.yaml receivers : otlp : protocols : http : endpoint : 0.0.0.0:4318 processors : batch : timeout : 1s send_batch_size : 1024 exporters : jaeger : endpoint : jaeger:14250 tls : insecure : true prometheus : endpoint : 0.0.0.0:9464 namespace : opossum logging : loglevel : info service : pipelines : traces : receivers : [ otlp ] processors : [ batch ] exporters : [ jaeger , logging ] metrics : receivers : [ otlp ] processors : [ batch ] exporters : [ prometheus , logging ] The OpenTelemetry integration provides comprehensive observability for Opossum Search, enabling performance monitoring, troubleshooting, and service dependency tracking across the distributed architecture.","title":"OpenTelemetry"},{"location":"technical/opentelemetry-integration/#technical-documentation-opentelemetry-integration","text":"","title":"Technical Documentation: OpenTelemetry Integration"},{"location":"technical/opentelemetry-integration/#1-overview","text":"Opossum Search implements comprehensive observability using OpenTelemetry to monitor application performance, track request flows, and diagnose issues across its distributed architecture. This integration enables detailed visibility into system behavior, performance bottlenecks, and service dependencies.","title":"1. Overview"},{"location":"technical/opentelemetry-integration/#2-opentelemetry-configuration","text":"# OpenTelemetry Configuration OTEL_EXPORTER_OTLP_ENDPOINT = os . getenv ( \"OTEL_EXPORTER_OTLP_ENDPOINT\" , \"http://localhost:4318/v1/traces\" ) OTEL_SERVICE_NAME = os . getenv ( \"OTEL_SERVICE_NAME\" , \"opossum-search\" ) OTEL_ENABLED = os . getenv ( \"OTEL_ENABLED\" , \"True\" ) . lower () == \"true\"","title":"2. OpenTelemetry Configuration"},{"location":"technical/opentelemetry-integration/#21-initialization","text":"from opentelemetry import trace from opentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.trace.export import BatchSpanProcessor from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter from opentelemetry.sdk.resources import Resource from opentelemetry.semconv.resource import ResourceAttributes def initialize_telemetry (): \"\"\"Initialize OpenTelemetry for distributed tracing\"\"\" if not Config . OTEL_ENABLED : return # Create resource information resource = Resource . create ({ ResourceAttributes . SERVICE_NAME : Config . OTEL_SERVICE_NAME , ResourceAttributes . SERVICE_VERSION : \"1.0.0\" , \"environment\" : Config . ENV }) # Set up tracer provider provider = TracerProvider ( resource = resource ) # Create OTLP exporter otlp_exporter = OTLPSpanExporter ( endpoint = Config . OTEL_EXPORTER_OTLP_ENDPOINT ) span_processor = BatchSpanProcessor ( otlp_exporter ) provider . add_span_processor ( span_processor ) # Set global tracer provider trace . set_tracer_provider ( provider ) # Create global tracer return trace . get_tracer ( __name__ )","title":"2.1 Initialization"},{"location":"technical/opentelemetry-integration/#3-tracing-request-flow","text":"","title":"3. Tracing Request Flow"},{"location":"technical/opentelemetry-integration/#31-graphql-request-tracing","text":"from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor # Instrument FastAPI application def instrument_fastapi ( app ): \"\"\"Instrument FastAPI with OpenTelemetry\"\"\" if Config . OTEL_ENABLED : FastAPIInstrumentor . instrument_app ( app , excluded_urls = \"/health,/metrics\" , tracer_provider = trace . get_tracer_provider () )","title":"3.1 GraphQL Request Tracing"},{"location":"technical/opentelemetry-integration/#32-model-backend-tracing","text":"async def generate_response ( self , prompt , model_name ): \"\"\"Generate response with OpenTelemetry tracing\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"model.generate\" , attributes = { \"prompt.length\" : len ( prompt ), \"model.name\" : model_name } ) as span : try : # Record start time start_time = time . time () # Call model backend response = await self . _call_model_backend ( prompt , model_name ) # Calculate and record duration duration = time . time () - start_time span . set_attribute ( \"duration_seconds\" , duration ) span . set_attribute ( \"response.length\" , len ( response )) return response except Exception as e : # Record error information span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) raise","title":"3.2 Model Backend Tracing"},{"location":"technical/opentelemetry-integration/#4-custom-metrics-and-events","text":"","title":"4. Custom Metrics and Events"},{"location":"technical/opentelemetry-integration/#41-service-availability-monitoring","text":"async def check_service ( self , service_name ): \"\"\"Check service availability with metrics recording\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"service.check\" , attributes = { \"service.name\" : service_name } ) as span : # Record check attempt span . add_event ( \"check_started\" ) try : # Perform actual check is_available = await self . _perform_actual_check ( service_name ) # Record result span . set_attribute ( \"service.available\" , is_available ) span . add_event ( \"check_completed\" , { \"result\" : \"available\" if is_available else \"unavailable\" } ) return is_available except Exception as e : # Record error span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) span . add_event ( \"check_failed\" , { \"error\" : str ( e )}) return False","title":"4.1 Service Availability Monitoring"},{"location":"technical/opentelemetry-integration/#42-cache-hitmiss-monitoring","text":"async def get_cached_item ( self , key ): \"\"\"Get cached item with telemetry\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"cache.get\" , attributes = { \"cache.key\" : key } ) as span : # Try to get from cache value = await self . redis_client . get ( key ) # Record hit/miss hit = value is not None span . set_attribute ( \"cache.hit\" , hit ) # Record in aggregated metrics if hit : await self . redis_client . incr ( \"metrics:cache:hits\" ) else : await self . redis_client . incr ( \"metrics:cache:misses\" ) return value","title":"4.2 Cache Hit/Miss Monitoring"},{"location":"technical/opentelemetry-integration/#5-custom-context-propagation","text":"from opentelemetry.context import Context , get_current from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator class ContextCarrier : \"\"\"Carries context between async tasks\"\"\" def __init__ ( self ): self . context = {} def get ( self , key , default = None ): return self . context . get ( key , default ) def set ( self , key , value ): self . context [ key ] = value def extract_context_for_background_task (): \"\"\"Extract current context to pass to background task\"\"\" carrier = ContextCarrier () propagator = TraceContextTextMapPropagator () propagator . inject ( carrier , get_current ()) return carrier async def run_in_background ( func , * args , parent_context = None , ** kwargs ): \"\"\"Run function in background with parent trace context\"\"\" if parent_context and Config . OTEL_ENABLED : propagator = TraceContextTextMapPropagator () context = propagator . extract ( parent_context ) token = context . attach () try : return await func ( * args , ** kwargs ) finally : context . detach ( token ) else : return await func ( * args , ** kwargs )","title":"5. Custom Context Propagation"},{"location":"technical/opentelemetry-integration/#6-external-service-integration","text":"","title":"6. External Service Integration"},{"location":"technical/opentelemetry-integration/#61-ollama-api-tracing","text":"async def generate_ollama_response ( self , prompt , model ): \"\"\"Trace Ollama API requests\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"ollama.generate\" , attributes = { \"prompt.length\" : len ( prompt ), \"model.name\" : model } ) as span : try : # Create request url = f \" { Config . OLLAMA_BASE_URL } /api/generate\" payload = { \"prompt\" : prompt , \"model\" : model , \"stream\" : False } # Add request details to span span . set_attribute ( \"http.url\" , url ) span . set_attribute ( \"http.method\" , \"POST\" ) # Send request start_time = time . time () async with httpx . AsyncClient () as client : response = await client . post ( url , json = payload , timeout = 60.0 ) # Add response details span . set_attribute ( \"http.status_code\" , response . status_code ) span . set_attribute ( \"http.response_time\" , time . time () - start_time ) # Handle response if response . status_code == 200 : data = response . json () return data . get ( \"response\" , \"\" ) else : error_msg = f \"Ollama API error: { response . status_code } \" span . set_status ( trace . StatusCode . ERROR , error_msg ) raise Exception ( error_msg ) except Exception as e : span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) raise","title":"6.1 Ollama API Tracing"},{"location":"technical/opentelemetry-integration/#62-gemini-api-tracing","text":"async def generate_gemini_response ( self , prompt , image_data = None ): \"\"\"Trace Gemini API requests\"\"\" tracer = trace . get_tracer ( __name__ ) with tracer . start_as_current_span ( \"gemini.generate\" , attributes = { \"prompt.length\" : len ( prompt ), \"has_image\" : image_data is not None } ) as span : try : # Initialize Gemini client gemini_client = AsyncGeminiClient ( api_key = Config . GEMINI_API_KEY ) model = Config . MODEL_CONFIGS [ \"gemini-thinking\" ][ \"api_name\" ] # Add request details to span span . set_attribute ( \"model.name\" , model ) # Prepare content parts content_parts = [{ \"text\" : prompt }] if image_data : # Add image to content parts span . set_attribute ( \"image.size\" , len ( image_data )) content_parts . append ({ \"inline_data\" : { \"mime_type\" : \"image/jpeg\" , \"data\" : base64 . b64encode ( image_data ) . decode ( 'utf-8' ) } }) # Generate response start_time = time . time () response = await gemini_client . generate_content ( model = model , contents = [{ \"parts\" : content_parts }] ) # Record performance metrics duration = time . time () - start_time span . set_attribute ( \"duration_seconds\" , duration ) # Extract response text if response and response . candidates : result = response . candidates [ 0 ] . content . parts [ 0 ] . text span . set_attribute ( \"response.length\" , len ( result )) return result else : span . set_status ( trace . StatusCode . ERROR , \"Empty response from Gemini\" ) return \"No response generated\" except Exception as e : span . record_exception ( e ) span . set_status ( trace . StatusCode . ERROR , str ( e )) raise","title":"6.2 Gemini API Tracing"},{"location":"technical/opentelemetry-integration/#7-visualization-and-monitoring","text":"# Visualization endpoints @app . get ( \"/metrics\" ) async def metrics (): \"\"\"Expose metrics for Prometheus scraping\"\"\" if not Config . OTEL_ENABLED : return { \"error\" : \"OpenTelemetry is disabled\" } return Response ( content = generate_latest () . decode ( \"utf-8\" ), media_type = \"text/plain\" ) @app . get ( \"/traces/recent\" ) async def recent_traces (): \"\"\"Get recent traces for UI display\"\"\" # This would typically be handled by a trace visualization tool like Jaeger # This endpoint just provides basic info for quick debugging recent = await redis_client . lrange ( \"traces:recent\" , 0 , 9 ) return [ json . loads ( trace ) for trace in recent ]","title":"7. Visualization and Monitoring"},{"location":"technical/opentelemetry-integration/#8-deployment-configuration","text":"For production deployments, OpenTelemetry traces are sent to a collector service that can export to various backends: # docker-compose excerpt for OpenTelemetry collector services : otel-collector : image : otel/opentelemetry-collector:0.97.0 command : [ \"--config=/etc/otel-config.yaml\" ] volumes : - ./otel-config.yaml:/etc/otel-config.yaml ports : - \"4318:4318\" # OTLP HTTP receiver - \"9464:9464\" # Prometheus exporter # Visualization with Jaeger jaeger : image : jaegertracing/all-in-one:latest ports : - \"16686:16686\" # Jaeger UI - \"14250:14250\" # Receive from collector","title":"8. Deployment Configuration"},{"location":"technical/opentelemetry-integration/#81-collector-configuration","text":"# otel-config.yaml receivers : otlp : protocols : http : endpoint : 0.0.0.0:4318 processors : batch : timeout : 1s send_batch_size : 1024 exporters : jaeger : endpoint : jaeger:14250 tls : insecure : true prometheus : endpoint : 0.0.0.0:9464 namespace : opossum logging : loglevel : info service : pipelines : traces : receivers : [ otlp ] processors : [ batch ] exporters : [ jaeger , logging ] metrics : receivers : [ otlp ] processors : [ batch ] exporters : [ prometheus , logging ] The OpenTelemetry integration provides comprehensive observability for Opossum Search, enabling performance monitoring, troubleshooting, and service dependency tracking across the distributed architecture.","title":"8.1 Collector Configuration"},{"location":"technical/prompt-management/","text":"DSPy Prompt Management \u00b6 Overview \u00b6 This document explains how DSPy integrates with Opossum's existing prompt management system to provide systematic prompt optimization, evaluation, and versioning capabilities. Key Concepts \u00b6 DSPy transforms traditional prompt engineering from a manual art into a systematic engineering discipline through: Programmatic Definition : Prompts as code rather than strings Optimization Pipelines : Automated refinement of prompts Compositional Structure : Building complex prompts from simpler components Metric-Driven Evaluation : Measuring prompt effectiveness objectively Integration with Existing Prompt System \u00b6 Architecture Overview \u00b6 graph TB A[YAML Prompt Templates] --> B[Template Loader] B --> C{Integration Layer} D[DSPy Signatures] --> C E[DSPy Modules] --> C C --> F[Runtime Prompt Selection] C --> G[Dev-time Optimization] G --> H[Optimized Templates] H --> I[Template Store] I --> F F --> J[LLM Providers] classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,D,J primary; class B,C,E,F,G,H,I secondary; Integration Components \u00b6 Prompt Adapter : Bridges YAML templates and DSPy Signatures Optimization Manager : Schedules and executes optimization runs Version Control : Tracks prompt evolution and performance Template Store : Manages optimized prompt variants Transforming YAML Templates to DSPy \u00b6 Current YAML Structure \u00b6 conversation : system_prompt : > You are Opossum Search, a helpful assistant. Respond in a friendly, conversational manner. If you don't know something, admit it. user_template : > User query: {{query}} Additional context: {{context}} DSPy Transformation \u00b6 # Converting YAML templates to DSPy signatures from app.prompts.loader import get_prompt_template import dspy class ConversationSignature ( dspy . Signature ): \"\"\"Signature for conversational responses.\"\"\" query = dspy . InputField ( desc = \"User's query\" ) context = dspy . InputField ( desc = \"Additional context for the query\" ) response = dspy . OutputField ( desc = \"Assistant's helpful response\" ) def create_signature_from_yaml ( template_name ): \"\"\"Create a DSPy signature from a YAML template.\"\"\" template = get_prompt_template ( template_name ) # Extract input fields from template input_fields = extract_fields_from_template ( template ) # Create a dynamic signature class SignatureClass = type ( f \" { template_name . title () . replace ( '.' , '' ) } Signature\" , ( dspy . Signature ,), { field : dspy . InputField () for field in input_fields } | { \"response\" : dspy . OutputField ()} ) return SignatureClass Optimization Workflow \u00b6 Step 1: Define Optimization Targets \u00b6 # app/integrations/dspy/optimization.py import dspy from app.data.examples import load_examples def optimize_prompt_template ( template_name , num_examples = 20 ): \"\"\"Optimize a prompt template using DSPy.\"\"\" # Load examples for this template examples = load_examples ( template_name , limit = num_examples ) # Create baseline module signature = create_signature_from_yaml ( template_name ) baseline = dspy . Predict ( signature ) # Define optimization metric metric = dspy . Metric ( name = \"response_quality\" , fn = evaluate_response_quality ) # Create and configure teleprompter teleprompter = dspy . Teleprompter ( baseline , metric = metric , max_rounds = 5 ) # Run optimization optimized = teleprompter . optimize ( examples ) # Persist optimized template save_optimized_template ( template_name , optimized ) return optimized Step 2: Evaluation Metrics \u00b6 DSPy allows for sophisticated evaluation of prompt quality: # Evaluating prompt effectiveness def evaluate_response_quality ( example , prediction ): \"\"\"Evaluate the quality of a model response.\"\"\" # Option 1: LLM-based evaluation evaluator = dspy . Predict ( instruction = \"Evaluate the quality of this response on a scale of 1-10.\" , input_keys = [ \"query\" , \"true_response\" , \"predicted_response\" ], output_keys = [ \"score\" , \"reasoning\" ] ) eval_result = evaluator ( query = example . query , true_response = example . response , predicted_response = prediction . response ) # Option 2: Programmatic metrics relevance_score = measure_relevance ( example . query , prediction . response ) coherence_score = measure_coherence ( prediction . response ) factuality_score = measure_factuality ( prediction . response ) # Combined score score = ( float ( eval_result . score ) * 0.7 + relevance_score * 0.1 + coherence_score * 0.1 + factuality_score * 0.1 ) return score Step 3: Versioning and Storage \u00b6 # Managing prompt versions def save_optimized_template ( template_name , optimized_module , metadata = None ): \"\"\"Save an optimized template version.\"\"\" # Extract prompt from module optimized_prompt = extract_prompt_from_module ( optimized_module ) # Generate version info version_info = { \"timestamp\" : datetime . now () . isoformat (), \"base_template\" : template_name , \"performance_metrics\" : get_performance_metrics ( optimized_module ), \"metadata\" : metadata or {} } # Store in database or file system store_optimized_template ( template_name = template_name , template_content = optimized_prompt , version_info = version_info ) Runtime Integration \u00b6 Using Optimized Prompts \u00b6 # app/integrations/dspy/runtime.py from app.prompts.store import get_best_template def get_optimized_prompt ( template_name , context = None ): \"\"\"Get the best available prompt for this context.\"\"\" # Check for context-specific optimized version if context : context_hash = hash_context ( context ) context_optimized = get_template_for_context ( template_name , context_hash ) if context_optimized : return context_optimized # Fall back to best general version return get_best_template ( template_name ) def apply_prompt ( template_name , ** kwargs ): \"\"\"Apply a prompt with the given parameters.\"\"\" # Get optimized template template = get_optimized_prompt ( template_name , kwargs . get ( 'context' )) # Apply template parameters filled_prompt = template . format ( ** kwargs ) return filled_prompt Development Workflow \u00b6 DSPy prompt optimization integrates into the development workflow through: CI/CD Integration : Automated optimization runs on PR Dashboard : Visual comparison of prompt performance A/B Testing : Systematic comparison of prompt variants Usage Analytics : Tracking which prompts perform best in production # Example: CI/CD integration def ci_optimize_prompts ( changed_files ): \"\"\"Optimize prompts affected by code changes.\"\"\" affected_templates = find_affected_templates ( changed_files ) optimization_results = [] for template in affected_templates : try : optimized = optimize_prompt_template ( template ) optimization_results . append ({ \"template\" : template , \"status\" : \"success\" , \"improvement\" : calculate_improvement ( template , optimized ) }) except Exception as e : optimization_results . append ({ \"template\" : template , \"status\" : \"error\" , \"error\" : str ( e ) }) return optimization_results Benefits Over Traditional Approach \u00b6 Traditional Prompt Engineering DSPy-Enhanced Approach Manual trial and error Systematic optimization Subjective quality assessment Metric-driven evaluation Hard-coded templates Programmatic definition Ad-hoc versioning Systematic version control Context-blind templates Context-aware optimization Limited reuse Compositional building blocks Best Practices \u00b6 Start With Good Baselines : Begin with well-crafted manual prompts Diverse Examples : Collect varied examples for optimization Meaningful Metrics : Define metrics that align with user needs Targeted Optimization : Optimize for specific contexts and tasks Continuous Evaluation : Monitor prompt performance in production Version Control : Track prompt evolution and enable rollbacks Related Documentation \u00b6 Prompt Management DSPy Integration Overview DSPy Technical Implementation DSPy Usage Examples","title":"Prompt Management"},{"location":"technical/prompt-management/#dspy-prompt-management","text":"","title":"DSPy Prompt Management"},{"location":"technical/prompt-management/#overview","text":"This document explains how DSPy integrates with Opossum's existing prompt management system to provide systematic prompt optimization, evaluation, and versioning capabilities.","title":"Overview"},{"location":"technical/prompt-management/#key-concepts","text":"DSPy transforms traditional prompt engineering from a manual art into a systematic engineering discipline through: Programmatic Definition : Prompts as code rather than strings Optimization Pipelines : Automated refinement of prompts Compositional Structure : Building complex prompts from simpler components Metric-Driven Evaluation : Measuring prompt effectiveness objectively","title":"Key Concepts"},{"location":"technical/prompt-management/#integration-with-existing-prompt-system","text":"","title":"Integration with Existing Prompt System"},{"location":"technical/prompt-management/#architecture-overview","text":"graph TB A[YAML Prompt Templates] --> B[Template Loader] B --> C{Integration Layer} D[DSPy Signatures] --> C E[DSPy Modules] --> C C --> F[Runtime Prompt Selection] C --> G[Dev-time Optimization] G --> H[Optimized Templates] H --> I[Template Store] I --> F F --> J[LLM Providers] classDef primary fill:#f9f,stroke:#333,stroke-width:2px; classDef secondary fill:#bbf,stroke:#333,stroke-width:1px; class A,D,J primary; class B,C,E,F,G,H,I secondary;","title":"Architecture Overview"},{"location":"technical/prompt-management/#integration-components","text":"Prompt Adapter : Bridges YAML templates and DSPy Signatures Optimization Manager : Schedules and executes optimization runs Version Control : Tracks prompt evolution and performance Template Store : Manages optimized prompt variants","title":"Integration Components"},{"location":"technical/prompt-management/#transforming-yaml-templates-to-dspy","text":"","title":"Transforming YAML Templates to DSPy"},{"location":"technical/prompt-management/#current-yaml-structure","text":"conversation : system_prompt : > You are Opossum Search, a helpful assistant. Respond in a friendly, conversational manner. If you don't know something, admit it. user_template : > User query: {{query}} Additional context: {{context}}","title":"Current YAML Structure"},{"location":"technical/prompt-management/#dspy-transformation","text":"# Converting YAML templates to DSPy signatures from app.prompts.loader import get_prompt_template import dspy class ConversationSignature ( dspy . Signature ): \"\"\"Signature for conversational responses.\"\"\" query = dspy . InputField ( desc = \"User's query\" ) context = dspy . InputField ( desc = \"Additional context for the query\" ) response = dspy . OutputField ( desc = \"Assistant's helpful response\" ) def create_signature_from_yaml ( template_name ): \"\"\"Create a DSPy signature from a YAML template.\"\"\" template = get_prompt_template ( template_name ) # Extract input fields from template input_fields = extract_fields_from_template ( template ) # Create a dynamic signature class SignatureClass = type ( f \" { template_name . title () . replace ( '.' , '' ) } Signature\" , ( dspy . Signature ,), { field : dspy . InputField () for field in input_fields } | { \"response\" : dspy . OutputField ()} ) return SignatureClass","title":"DSPy Transformation"},{"location":"technical/prompt-management/#optimization-workflow","text":"","title":"Optimization Workflow"},{"location":"technical/prompt-management/#step-1-define-optimization-targets","text":"# app/integrations/dspy/optimization.py import dspy from app.data.examples import load_examples def optimize_prompt_template ( template_name , num_examples = 20 ): \"\"\"Optimize a prompt template using DSPy.\"\"\" # Load examples for this template examples = load_examples ( template_name , limit = num_examples ) # Create baseline module signature = create_signature_from_yaml ( template_name ) baseline = dspy . Predict ( signature ) # Define optimization metric metric = dspy . Metric ( name = \"response_quality\" , fn = evaluate_response_quality ) # Create and configure teleprompter teleprompter = dspy . Teleprompter ( baseline , metric = metric , max_rounds = 5 ) # Run optimization optimized = teleprompter . optimize ( examples ) # Persist optimized template save_optimized_template ( template_name , optimized ) return optimized","title":"Step 1: Define Optimization Targets"},{"location":"technical/prompt-management/#step-2-evaluation-metrics","text":"DSPy allows for sophisticated evaluation of prompt quality: # Evaluating prompt effectiveness def evaluate_response_quality ( example , prediction ): \"\"\"Evaluate the quality of a model response.\"\"\" # Option 1: LLM-based evaluation evaluator = dspy . Predict ( instruction = \"Evaluate the quality of this response on a scale of 1-10.\" , input_keys = [ \"query\" , \"true_response\" , \"predicted_response\" ], output_keys = [ \"score\" , \"reasoning\" ] ) eval_result = evaluator ( query = example . query , true_response = example . response , predicted_response = prediction . response ) # Option 2: Programmatic metrics relevance_score = measure_relevance ( example . query , prediction . response ) coherence_score = measure_coherence ( prediction . response ) factuality_score = measure_factuality ( prediction . response ) # Combined score score = ( float ( eval_result . score ) * 0.7 + relevance_score * 0.1 + coherence_score * 0.1 + factuality_score * 0.1 ) return score","title":"Step 2: Evaluation Metrics"},{"location":"technical/prompt-management/#step-3-versioning-and-storage","text":"# Managing prompt versions def save_optimized_template ( template_name , optimized_module , metadata = None ): \"\"\"Save an optimized template version.\"\"\" # Extract prompt from module optimized_prompt = extract_prompt_from_module ( optimized_module ) # Generate version info version_info = { \"timestamp\" : datetime . now () . isoformat (), \"base_template\" : template_name , \"performance_metrics\" : get_performance_metrics ( optimized_module ), \"metadata\" : metadata or {} } # Store in database or file system store_optimized_template ( template_name = template_name , template_content = optimized_prompt , version_info = version_info )","title":"Step 3: Versioning and Storage"},{"location":"technical/prompt-management/#runtime-integration","text":"","title":"Runtime Integration"},{"location":"technical/prompt-management/#using-optimized-prompts","text":"# app/integrations/dspy/runtime.py from app.prompts.store import get_best_template def get_optimized_prompt ( template_name , context = None ): \"\"\"Get the best available prompt for this context.\"\"\" # Check for context-specific optimized version if context : context_hash = hash_context ( context ) context_optimized = get_template_for_context ( template_name , context_hash ) if context_optimized : return context_optimized # Fall back to best general version return get_best_template ( template_name ) def apply_prompt ( template_name , ** kwargs ): \"\"\"Apply a prompt with the given parameters.\"\"\" # Get optimized template template = get_optimized_prompt ( template_name , kwargs . get ( 'context' )) # Apply template parameters filled_prompt = template . format ( ** kwargs ) return filled_prompt","title":"Using Optimized Prompts"},{"location":"technical/prompt-management/#development-workflow","text":"DSPy prompt optimization integrates into the development workflow through: CI/CD Integration : Automated optimization runs on PR Dashboard : Visual comparison of prompt performance A/B Testing : Systematic comparison of prompt variants Usage Analytics : Tracking which prompts perform best in production # Example: CI/CD integration def ci_optimize_prompts ( changed_files ): \"\"\"Optimize prompts affected by code changes.\"\"\" affected_templates = find_affected_templates ( changed_files ) optimization_results = [] for template in affected_templates : try : optimized = optimize_prompt_template ( template ) optimization_results . append ({ \"template\" : template , \"status\" : \"success\" , \"improvement\" : calculate_improvement ( template , optimized ) }) except Exception as e : optimization_results . append ({ \"template\" : template , \"status\" : \"error\" , \"error\" : str ( e ) }) return optimization_results","title":"Development Workflow"},{"location":"technical/prompt-management/#benefits-over-traditional-approach","text":"Traditional Prompt Engineering DSPy-Enhanced Approach Manual trial and error Systematic optimization Subjective quality assessment Metric-driven evaluation Hard-coded templates Programmatic definition Ad-hoc versioning Systematic version control Context-blind templates Context-aware optimization Limited reuse Compositional building blocks","title":"Benefits Over Traditional Approach"},{"location":"technical/prompt-management/#best-practices","text":"Start With Good Baselines : Begin with well-crafted manual prompts Diverse Examples : Collect varied examples for optimization Meaningful Metrics : Define metrics that align with user needs Targeted Optimization : Optimize for specific contexts and tasks Continuous Evaluation : Monitor prompt performance in production Version Control : Track prompt evolution and enable rollbacks","title":"Best Practices"},{"location":"technical/prompt-management/#related-documentation","text":"Prompt Management DSPy Integration Overview DSPy Technical Implementation DSPy Usage Examples","title":"Related Documentation"},{"location":"technical/redis-caching-architecture/","text":"Technical Documentation: Redis Caching Architecture \u00b6 1. Overview \u00b6 Opossum Search implements a sophisticated multi-level caching system using Redis to optimize performance, reduce duplicate computation, and manage distributed state across the application. This architecture supports high throughput while minimizing latency and external API usage. Related Documentation: - Technical: Hybrid Model Selection - Cache integration with model selection - Service Availability: Rate Limiting - Redis-based rate limiting implementation 2. Redis Configuration \u00b6 # Redis connection settings REDIS_HOST = os . getenv ( \"REDIS_HOST\" , \"localhost\" ) REDIS_PORT = int ( os . getenv ( \"REDIS_PORT\" , \"6379\" )) REDIS_DB = int ( os . getenv ( \"REDIS_DB\" , \"0\" )) REDIS_PASSWORD = os . getenv ( \"REDIS_PASSWORD\" , \"\" ) REDIS_TTL = int ( os . getenv ( \"REDIS_TTL\" , \"600\" )) # Default TTL: 10 minutes 2.1 Connection Pool Management \u00b6 async def get_redis_pool (): \"\"\"Get or create Redis connection pool\"\"\" global _redis_pool if _redis_pool is None : _redis_pool = aioredis . ConnectionPool ( host = Config . REDIS_HOST , port = Config . REDIS_PORT , db = Config . REDIS_DB , password = Config . REDIS_PASSWORD , max_connections = 50 , decode_responses = False ) return _redis_pool async def get_redis_client (): \"\"\"Get Redis client with connection pooling\"\"\" pool = await get_redis_pool () return aioredis . Redis ( connection_pool = pool ) 3. Cache Types and TTLs \u00b6 3.1 Response Caching \u00b6 Response caching stores LLM-generated responses to avoid redundant model calls for identical or similar queries. async def get_cached_response ( self , query , backend_name ): \"\"\"Get cached response if available\"\"\" cache_key = f \"response: { backend_name } : { hashlib . md5 ( query . encode ()) . hexdigest () } \" cached = await self . redis . get ( cache_key ) if cached : self . metrics . increment ( \"cache_hit\" , { \"backend\" : backend_name }) return json . loads ( cached ) self . metrics . increment ( \"cache_miss\" , { \"backend\" : backend_name }) return None async def cache_response ( self , query , response , backend_name ): \"\"\"Cache a model response\"\"\" cache_key = f \"response: { backend_name } : { hashlib . md5 ( query . encode ()) . hexdigest () } \" await self . redis . setex ( cache_key , Config . CACHE_TTL , # 10 minutes default json . dumps ( response ) ) TTL Strategy \u00b6 Cache Type TTL Rationale Simple responses 600s (10m) High reuse value, low volatility Complex responses 300s (5m) May become outdated faster Image processing 1800s (30m) High computation cost, stable results 3.2 Service Availability Cache \u00b6 Tracks the operational status of backend services to avoid timeouts on unavailable services. async def check_service ( self , service_name ): \"\"\"Check if a service is available with caching\"\"\" cache_key = f \"service:available: { service_name } \" # Check cache first cached = await self . redis . get ( cache_key ) if cached is not None : return cached == b \"1\" # Perform actual health check is_available = await self . _perform_check ( service_name ) # Cache result with short TTL await self . redis . setex ( cache_key , Config . AVAILABILITY_CACHE_TTL , # 30 seconds \"1\" if is_available else \"0\" ) return is_available 3.3 Model Selection Cache \u00b6 Caches model selection decisions to avoid recalculating optimal backend for similar requests. async def get_optimal_backend ( self , query_type , has_image = False ): \"\"\"Get optimal backend with caching\"\"\" # Generate cache key cache_key = f \"model:selection: { query_type } : { has_image } \" # Check cache cached = await self . redis . get ( cache_key ) if cached : return cached . decode ( 'utf-8' ) # Calculate optimal backend backend = await self . _calculate_optimal_backend ( query_type , has_image ) # Cache result await self . redis . setex ( cache_key , Config . MODEL_SELECTION_CACHE_TTL , # 60 seconds backend ) return backend 3.4 Rate Limiting \u00b6 Implements token bucket algorithm for API rate limiting. async def is_rate_limited ( self , client_id , limit_key = \"default\" ): \"\"\"Check if client is rate limited\"\"\" # Get rate limit configuration limits = Config . get_rate_limits () . get ( limit_key , [ \"100 per day\" ]) # Check each limit for limit in limits : count , period = self . _parse_limit ( limit ) # Generate Redis key rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Check current count current = await self . redis . get ( rate_key ) if current and int ( current ) >= count : return True # Rate limited return False # Not rate limited async def increment_rate_counter ( self , client_id , limit_key = \"default\" ): \"\"\"Increment rate counters for all applicable limits\"\"\" limits = Config . get_rate_limits () . get ( limit_key , [ \"100 per day\" ]) for limit in limits : count , period = self . _parse_limit ( limit ) seconds = self . _period_to_seconds ( period ) rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Increment counter and set TTL if it's new await self . redis . incr ( rate_key ) await self . redis . expire ( rate_key , seconds , nx = True ) 3.5 Performance Metrics \u00b6 Stores historical performance metrics for model selection learning. async def record_performance ( self , backend , metrics ): \"\"\"Record performance metrics for a backend\"\"\" now = int ( time . time ()) # Store individual data point await self . redis . zadd ( f \"metrics: { backend } :history\" , { json . dumps ( metrics ): now } ) # Trim to last N entries await self . redis . zremrangebyrank ( f \"metrics: { backend } :history\" , 0 , - Config . SERVICE_HISTORY_MAX_ITEMS - 1 ) # Update rolling averages for metric , value in metrics . items (): # Get current average current = await self . redis . hget ( f \"metrics: { backend } :avg\" , metric ) current = float ( current ) if current else value # Calculate new average (90% old, 10% new) new_avg = current * 0.9 + value * 0.1 # Store updated average await self . redis . hset ( f \"metrics: { backend } :avg\" , metric , new_avg ) 4. Cache Interaction Patterns \u00b6 4.1 Cache-Aside Pattern \u00b6 async def generate_response ( self , prompt , model ): \"\"\"Generate response with cache-aside pattern\"\"\" # Check cache first cache_key = f \"response: { model } : { hashlib . md5 ( prompt . encode ()) . hexdigest () } \" cached = await self . redis . get ( cache_key ) if cached : # Cache hit return json . loads ( cached ) # Cache miss - generate fresh response response = await self . _call_llm_service ( prompt , model ) # Store in cache await self . redis . setex ( cache_key , Config . CACHE_TTL , json . dumps ( response )) return response 4.2 Write-Through Pattern \u00b6 async def update_model_preference ( self , user_id , model , score ): \"\"\"Update user model preference with write-through pattern\"\"\" # Update in-memory cache self . model_preferences [ user_id ][ model ] = score # Write through to Redis await self . redis . hset ( f \"user: { user_id } :preferences\" , model , score ) 5. Monitoring and Maintenance \u00b6 5.1 Cache Hit Ratio Monitoring \u00b6 async def get_cache_metrics ( self ): \"\"\"Get cache hit/miss metrics\"\"\" hits = int ( await self . redis . get ( \"metrics:cache:hits\" ) or 0 ) misses = int ( await self . redis . get ( \"metrics:cache:misses\" ) or 0 ) total = hits + misses ratio = hits / total if total > 0 else 0 return { \"hits\" : hits , \"misses\" : misses , \"total\" : total , \"ratio\" : ratio } 5.2 Cache Invalidation \u00b6 async def invalidate_model_cache ( self , model_name = None ): \"\"\"Invalidate cache for specific model or all models\"\"\" if model_name : # Get all keys for this model keys = await self . redis . keys ( f \"response: { model_name } :*\" ) if keys : await self . redis . delete ( * keys ) else : # Get all response cache keys keys = await self . redis . keys ( \"response:*\" ) if keys : await self . redis . delete ( * keys ) 6. Redis Memory Management \u00b6 # Redis memory policy in redis.conf # maxmemory 1gb # maxmemory-policy allkeys-lru The Redis instance is configured with a memory limit and LRU eviction policy to automatically manage memory usage when limits are reached. 7. Performance Considerations \u00b6 7.1 Redis Pipeline for Batch Operations \u00b6 async def batch_update_metrics ( self , metrics_list ): \"\"\"Update multiple metrics in a single pipeline\"\"\" pipeline = self . redis . pipeline () for backend , metrics in metrics_list : for metric , value in metrics . items (): pipeline . hset ( f \"metrics: { backend } \" , metric , value ) await pipeline . execute () 7.2 Key Expiration Strategy \u00b6 Rather than running manual cleanup jobs, the system relies on Redis TTL (Time-To-Live) expirations to automatically manage cache size and freshness. Keys are assigned appropriate TTLs based on data volatility and reuse patterns. 8. Redis High Availability Configuration \u00b6 For production deployments, Redis is configured with: Redis Sentinel for automatic failover Periodic RDB snapshots for persistence Monitoring integration with system alerts This Redis caching architecture enables Opossum Search to maintain high performance while minimizing external API usage and providing resilience against backend service disruptions.","title":"Redis Caching"},{"location":"technical/redis-caching-architecture/#technical-documentation-redis-caching-architecture","text":"","title":"Technical Documentation: Redis Caching Architecture"},{"location":"technical/redis-caching-architecture/#1-overview","text":"Opossum Search implements a sophisticated multi-level caching system using Redis to optimize performance, reduce duplicate computation, and manage distributed state across the application. This architecture supports high throughput while minimizing latency and external API usage. Related Documentation: - Technical: Hybrid Model Selection - Cache integration with model selection - Service Availability: Rate Limiting - Redis-based rate limiting implementation","title":"1. Overview"},{"location":"technical/redis-caching-architecture/#2-redis-configuration","text":"# Redis connection settings REDIS_HOST = os . getenv ( \"REDIS_HOST\" , \"localhost\" ) REDIS_PORT = int ( os . getenv ( \"REDIS_PORT\" , \"6379\" )) REDIS_DB = int ( os . getenv ( \"REDIS_DB\" , \"0\" )) REDIS_PASSWORD = os . getenv ( \"REDIS_PASSWORD\" , \"\" ) REDIS_TTL = int ( os . getenv ( \"REDIS_TTL\" , \"600\" )) # Default TTL: 10 minutes","title":"2. Redis Configuration"},{"location":"technical/redis-caching-architecture/#21-connection-pool-management","text":"async def get_redis_pool (): \"\"\"Get or create Redis connection pool\"\"\" global _redis_pool if _redis_pool is None : _redis_pool = aioredis . ConnectionPool ( host = Config . REDIS_HOST , port = Config . REDIS_PORT , db = Config . REDIS_DB , password = Config . REDIS_PASSWORD , max_connections = 50 , decode_responses = False ) return _redis_pool async def get_redis_client (): \"\"\"Get Redis client with connection pooling\"\"\" pool = await get_redis_pool () return aioredis . Redis ( connection_pool = pool )","title":"2.1 Connection Pool Management"},{"location":"technical/redis-caching-architecture/#3-cache-types-and-ttls","text":"","title":"3. Cache Types and TTLs"},{"location":"technical/redis-caching-architecture/#31-response-caching","text":"Response caching stores LLM-generated responses to avoid redundant model calls for identical or similar queries. async def get_cached_response ( self , query , backend_name ): \"\"\"Get cached response if available\"\"\" cache_key = f \"response: { backend_name } : { hashlib . md5 ( query . encode ()) . hexdigest () } \" cached = await self . redis . get ( cache_key ) if cached : self . metrics . increment ( \"cache_hit\" , { \"backend\" : backend_name }) return json . loads ( cached ) self . metrics . increment ( \"cache_miss\" , { \"backend\" : backend_name }) return None async def cache_response ( self , query , response , backend_name ): \"\"\"Cache a model response\"\"\" cache_key = f \"response: { backend_name } : { hashlib . md5 ( query . encode ()) . hexdigest () } \" await self . redis . setex ( cache_key , Config . CACHE_TTL , # 10 minutes default json . dumps ( response ) )","title":"3.1 Response Caching"},{"location":"technical/redis-caching-architecture/#ttl-strategy","text":"Cache Type TTL Rationale Simple responses 600s (10m) High reuse value, low volatility Complex responses 300s (5m) May become outdated faster Image processing 1800s (30m) High computation cost, stable results","title":"TTL Strategy"},{"location":"technical/redis-caching-architecture/#32-service-availability-cache","text":"Tracks the operational status of backend services to avoid timeouts on unavailable services. async def check_service ( self , service_name ): \"\"\"Check if a service is available with caching\"\"\" cache_key = f \"service:available: { service_name } \" # Check cache first cached = await self . redis . get ( cache_key ) if cached is not None : return cached == b \"1\" # Perform actual health check is_available = await self . _perform_check ( service_name ) # Cache result with short TTL await self . redis . setex ( cache_key , Config . AVAILABILITY_CACHE_TTL , # 30 seconds \"1\" if is_available else \"0\" ) return is_available","title":"3.2 Service Availability Cache"},{"location":"technical/redis-caching-architecture/#33-model-selection-cache","text":"Caches model selection decisions to avoid recalculating optimal backend for similar requests. async def get_optimal_backend ( self , query_type , has_image = False ): \"\"\"Get optimal backend with caching\"\"\" # Generate cache key cache_key = f \"model:selection: { query_type } : { has_image } \" # Check cache cached = await self . redis . get ( cache_key ) if cached : return cached . decode ( 'utf-8' ) # Calculate optimal backend backend = await self . _calculate_optimal_backend ( query_type , has_image ) # Cache result await self . redis . setex ( cache_key , Config . MODEL_SELECTION_CACHE_TTL , # 60 seconds backend ) return backend","title":"3.3 Model Selection Cache"},{"location":"technical/redis-caching-architecture/#34-rate-limiting","text":"Implements token bucket algorithm for API rate limiting. async def is_rate_limited ( self , client_id , limit_key = \"default\" ): \"\"\"Check if client is rate limited\"\"\" # Get rate limit configuration limits = Config . get_rate_limits () . get ( limit_key , [ \"100 per day\" ]) # Check each limit for limit in limits : count , period = self . _parse_limit ( limit ) # Generate Redis key rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Check current count current = await self . redis . get ( rate_key ) if current and int ( current ) >= count : return True # Rate limited return False # Not rate limited async def increment_rate_counter ( self , client_id , limit_key = \"default\" ): \"\"\"Increment rate counters for all applicable limits\"\"\" limits = Config . get_rate_limits () . get ( limit_key , [ \"100 per day\" ]) for limit in limits : count , period = self . _parse_limit ( limit ) seconds = self . _period_to_seconds ( period ) rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Increment counter and set TTL if it's new await self . redis . incr ( rate_key ) await self . redis . expire ( rate_key , seconds , nx = True )","title":"3.4 Rate Limiting"},{"location":"technical/redis-caching-architecture/#35-performance-metrics","text":"Stores historical performance metrics for model selection learning. async def record_performance ( self , backend , metrics ): \"\"\"Record performance metrics for a backend\"\"\" now = int ( time . time ()) # Store individual data point await self . redis . zadd ( f \"metrics: { backend } :history\" , { json . dumps ( metrics ): now } ) # Trim to last N entries await self . redis . zremrangebyrank ( f \"metrics: { backend } :history\" , 0 , - Config . SERVICE_HISTORY_MAX_ITEMS - 1 ) # Update rolling averages for metric , value in metrics . items (): # Get current average current = await self . redis . hget ( f \"metrics: { backend } :avg\" , metric ) current = float ( current ) if current else value # Calculate new average (90% old, 10% new) new_avg = current * 0.9 + value * 0.1 # Store updated average await self . redis . hset ( f \"metrics: { backend } :avg\" , metric , new_avg )","title":"3.5 Performance Metrics"},{"location":"technical/redis-caching-architecture/#4-cache-interaction-patterns","text":"","title":"4. Cache Interaction Patterns"},{"location":"technical/redis-caching-architecture/#41-cache-aside-pattern","text":"async def generate_response ( self , prompt , model ): \"\"\"Generate response with cache-aside pattern\"\"\" # Check cache first cache_key = f \"response: { model } : { hashlib . md5 ( prompt . encode ()) . hexdigest () } \" cached = await self . redis . get ( cache_key ) if cached : # Cache hit return json . loads ( cached ) # Cache miss - generate fresh response response = await self . _call_llm_service ( prompt , model ) # Store in cache await self . redis . setex ( cache_key , Config . CACHE_TTL , json . dumps ( response )) return response","title":"4.1 Cache-Aside Pattern"},{"location":"technical/redis-caching-architecture/#42-write-through-pattern","text":"async def update_model_preference ( self , user_id , model , score ): \"\"\"Update user model preference with write-through pattern\"\"\" # Update in-memory cache self . model_preferences [ user_id ][ model ] = score # Write through to Redis await self . redis . hset ( f \"user: { user_id } :preferences\" , model , score )","title":"4.2 Write-Through Pattern"},{"location":"technical/redis-caching-architecture/#5-monitoring-and-maintenance","text":"","title":"5. Monitoring and Maintenance"},{"location":"technical/redis-caching-architecture/#51-cache-hit-ratio-monitoring","text":"async def get_cache_metrics ( self ): \"\"\"Get cache hit/miss metrics\"\"\" hits = int ( await self . redis . get ( \"metrics:cache:hits\" ) or 0 ) misses = int ( await self . redis . get ( \"metrics:cache:misses\" ) or 0 ) total = hits + misses ratio = hits / total if total > 0 else 0 return { \"hits\" : hits , \"misses\" : misses , \"total\" : total , \"ratio\" : ratio }","title":"5.1 Cache Hit Ratio Monitoring"},{"location":"technical/redis-caching-architecture/#52-cache-invalidation","text":"async def invalidate_model_cache ( self , model_name = None ): \"\"\"Invalidate cache for specific model or all models\"\"\" if model_name : # Get all keys for this model keys = await self . redis . keys ( f \"response: { model_name } :*\" ) if keys : await self . redis . delete ( * keys ) else : # Get all response cache keys keys = await self . redis . keys ( \"response:*\" ) if keys : await self . redis . delete ( * keys )","title":"5.2 Cache Invalidation"},{"location":"technical/redis-caching-architecture/#6-redis-memory-management","text":"# Redis memory policy in redis.conf # maxmemory 1gb # maxmemory-policy allkeys-lru The Redis instance is configured with a memory limit and LRU eviction policy to automatically manage memory usage when limits are reached.","title":"6. Redis Memory Management"},{"location":"technical/redis-caching-architecture/#7-performance-considerations","text":"","title":"7. Performance Considerations"},{"location":"technical/redis-caching-architecture/#71-redis-pipeline-for-batch-operations","text":"async def batch_update_metrics ( self , metrics_list ): \"\"\"Update multiple metrics in a single pipeline\"\"\" pipeline = self . redis . pipeline () for backend , metrics in metrics_list : for metric , value in metrics . items (): pipeline . hset ( f \"metrics: { backend } \" , metric , value ) await pipeline . execute ()","title":"7.1 Redis Pipeline for Batch Operations"},{"location":"technical/redis-caching-architecture/#72-key-expiration-strategy","text":"Rather than running manual cleanup jobs, the system relies on Redis TTL (Time-To-Live) expirations to automatically manage cache size and freshness. Keys are assigned appropriate TTLs based on data volatility and reuse patterns.","title":"7.2 Key Expiration Strategy"},{"location":"technical/redis-caching-architecture/#8-redis-high-availability-configuration","text":"For production deployments, Redis is configured with: Redis Sentinel for automatic failover Periodic RDB snapshots for persistence Monitoring integration with system alerts This Redis caching architecture enables Opossum Search to maintain high performance while minimizing external API usage and providing resilience against backend service disruptions.","title":"8. Redis High Availability Configuration"},{"location":"technical/resilience-patterns/","text":"Resilience Patterns in Opossum Search \u00b6 This document outlines the resilience patterns implemented throughout Opossum Search to ensure system stability, graceful degradation, and optimal user experience under varying conditions. Core Principles \u00b6 Our resilience strategy is built on several fundamental principles: Fail Gracefully - When failures occur, degrade functionality rather than completely failing Defense in Depth - Multiple layers of fallback mechanisms Resource Protection - Prevent cascading failures through circuit breakers and rate limiting Adaptive Selection - Intelligently choose services based on availability and capabilities Experience Preservation - Maintain core user experience even during degraded conditions Circuit Breaker Pattern \u00b6 We implement the Circuit Breaker pattern to prevent repeated calls to failing services: class CircuitBreaker : def __init__ ( self , name , failure_threshold = 3 , reset_timeout = 60 ): self . name = name self . failure_threshold = failure_threshold self . reset_timeout = reset_timeout self . failure_count = 0 self . last_failure_time = 0 self . state = \"CLOSED\" # CLOSED, OPEN, HALF_OPEN def record_success ( self ): self . failure_count = 0 if self . state != \"CLOSED\" : logger . info ( f \"Circuit breaker { self . name } closing after successful operation\" ) self . state = \"CLOSED\" def record_failure ( self ): self . failure_count += 1 self . last_failure_time = time . time () if self . state == \"CLOSED\" and self . failure_count >= self . failure_threshold : logger . warning ( f \"Circuit breaker { self . name } opening after { self . failure_count } failures\" ) self . state = \"OPEN\" elif self . state == \"HALF_OPEN\" : logger . warning ( f \"Circuit breaker { self . name } reopening after test failure\" ) self . state = \"OPEN\" def allow_request ( self ) -> bool : current_time = time . time () # If circuit is open but enough time has passed, allow a test request if self . state == \"OPEN\" and current_time - self . last_failure_time > self . reset_timeout : logger . info ( f \"Circuit breaker { self . name } switching to half-open state for testing\" ) self . state = \"HALF_OPEN\" return True # Allow requests when closed or half-open (for testing) return self . state != \"OPEN\" Circuit breakers prevent the system from making repeated calls to failing services, which: Reduces latency for end users Prevents resource exhaustion Allows services to recover Provides early failure detection Multi-Level Fallback \u00b6 Our fallback mechanisms operate at multiple levels: Primary Service Fallback Default model selection with capability matching Automatic fallback to alternative providers Emergency Fallback Predefined fallback paths when normal selection fails Transformers (local) as ultimate fallback Graceful Degradation Feature downgrading during system stress Simplified user interface elements Example: Model Selection Fallbacks \u00b6 def _emergency_fallback ( self ) -> Tuple [ str , float , str ]: \"\"\"Emergency fallback when selection process itself fails\"\"\" logger . critical ( \"Using EMERGENCY FALLBACK for model selection due to critical failure\" ) # Try each emergency fallback path for fallback in self . _emergency_fallbacks : provider = fallback [ \"provider\" ] model = fallback [ \"model\" ] # Skip if circuit breaker is fully open if provider in self . circuit_breakers and self . circuit_breakers [ provider ] . state == \"OPEN\" : continue if ( provider in self . available_backends and model in self . available_backends [ provider ]): logger . info ( f \"Emergency fallback selected: { provider } / { model } \" ) return model , 0.2 , provider # Ultimate emergency fallback - just try transformers/gemma logger . critical ( \"ALL FALLBACKS EXHAUSTED! Using transformers/gemma as final resort\" ) return \"gemma\" , 0.1 , \"transformers\" Adaptive Load Management \u00b6 We implement several techniques to manage system load: 1. Request Jittering \u00b6 During periods of system degradation, we intentionally introduce randomness in our service selection to prevent thundering herd problems: if self . _is_system_degraded () and random . random () < 0.2 : # 20% chance backup_model = self . _get_fallback_model ( model_selection [ 0 ], model_selection [ 2 ], available_backends ) if backup_model : logger . info ( f \"System degraded, applying jitter to model selection\" ) model_selection = backup_model 2. Tiered Caching \u00b6 Short-lived cache (10 seconds) for model selection results Medium-lived cache (5 minutes) for topic similarities Long-lived cache (1 hour) for SVG generations Emergency cache for critical responses 3. Feature Throttling \u00b6 Non-critical features like Easter eggs automatically throttle their activation during system degradation: def should_activate ( self , egg_type : str ) -> bool : # ...existing code... # Reduce activation chance if system is degraded if self . is_system_degraded () and egg_type not in [ \"possum_party\" , \"konami_code\" ]: threshold = min ( threshold , DEGRADED_ACTIVATION_THRESHOLD ) Experience Preservation \u00b6 Even during degraded operation, we ensure the core user experience remains functional: Feature Downgrading \u00b6 Features are downgraded rather than disabled completely: fallback_features = { \"national_opossum_day\" : { \"ui_theme\" : \"party_opossum_light\" , # Lighter version with fewer animations \"response_modifiers\" : [ \"purple_text\" ], # Only add purple text, no jokes \"animations\" : [] # No animations to reduce load }, # ...other fallbacks... } SVG Generation Resilience \u00b6 The SVG generation system combines two approaches for maximum resilience: Template-Based Generation Fast, reliable predefined templates Low resource requirements Consistent output quality AI-Powered Generation via Chat2SVG Feature-rich custom SVG generation Falls back to templates when unavailable This hybrid approach ensures users always receive visualizations, even when advanced generation is unavailable. Health Monitoring \u00b6 Our resilience systems rely on comprehensive health monitoring: Service Availability Checks Periodic health checks of all backend services Cached results to prevent excessive checking Circuit State Tracking Circuit breaker states tracked and logged Automatic recovery attempts after timeout periods Degradation Detection Consecutive failures tracked to identify degradation patterns System-wide degradation state determined by multiple provider failures Implementation Areas \u00b6 Component Resilience Patterns Model Selector Circuit breakers, adaptive selection, fallback paths, jittering SVG Generation Dual implementation (templates + Chat2SVG), tiered fallbacks Easter Eggs Activation throttling, feature downgrading, degraded modes API Endpoints Timeouts, rate limiting, caching strategies Frontend Progressive enhancement, graceful UI degradation Testing Resilience \u00b6 We test our resilience patterns through: Chaos Engineering Simulated service outages Random latency injection Resource exhaustion tests Load Testing Traffic spike simulations Concurrent request handling Long-tail latency analysis Degraded Operation Tests Functionality verification during partial outages Recovery time measurements User experience evaluation during degradation Dashboard Integration \u00b6 Our resilience patterns are visibly represented in the service status dashboard, which shows: Current availability of each service Circuit breaker states Fallback selection statistics Degradation indicators Recovery predictions Future Improvements \u00b6 Planned enhancements to our resilience patterns: Predictive Circuit Breaking Use machine learning to predict failures before they occur Preemptively route around problematic services Adaptive Resource Allocation Dynamically adjust resource allocation based on system load Prioritize critical services during degradation User-Aware Degradation Personalized degradation strategies based on user priorities Transparent communication about system status Multi-Region Resilience Geographic distribution of services Regional circuit breakers and fallbacks","title":"Core Strategies"},{"location":"technical/resilience-patterns/#resilience-patterns-in-opossum-search","text":"This document outlines the resilience patterns implemented throughout Opossum Search to ensure system stability, graceful degradation, and optimal user experience under varying conditions.","title":"Resilience Patterns in Opossum Search"},{"location":"technical/resilience-patterns/#core-principles","text":"Our resilience strategy is built on several fundamental principles: Fail Gracefully - When failures occur, degrade functionality rather than completely failing Defense in Depth - Multiple layers of fallback mechanisms Resource Protection - Prevent cascading failures through circuit breakers and rate limiting Adaptive Selection - Intelligently choose services based on availability and capabilities Experience Preservation - Maintain core user experience even during degraded conditions","title":"Core Principles"},{"location":"technical/resilience-patterns/#circuit-breaker-pattern","text":"We implement the Circuit Breaker pattern to prevent repeated calls to failing services: class CircuitBreaker : def __init__ ( self , name , failure_threshold = 3 , reset_timeout = 60 ): self . name = name self . failure_threshold = failure_threshold self . reset_timeout = reset_timeout self . failure_count = 0 self . last_failure_time = 0 self . state = \"CLOSED\" # CLOSED, OPEN, HALF_OPEN def record_success ( self ): self . failure_count = 0 if self . state != \"CLOSED\" : logger . info ( f \"Circuit breaker { self . name } closing after successful operation\" ) self . state = \"CLOSED\" def record_failure ( self ): self . failure_count += 1 self . last_failure_time = time . time () if self . state == \"CLOSED\" and self . failure_count >= self . failure_threshold : logger . warning ( f \"Circuit breaker { self . name } opening after { self . failure_count } failures\" ) self . state = \"OPEN\" elif self . state == \"HALF_OPEN\" : logger . warning ( f \"Circuit breaker { self . name } reopening after test failure\" ) self . state = \"OPEN\" def allow_request ( self ) -> bool : current_time = time . time () # If circuit is open but enough time has passed, allow a test request if self . state == \"OPEN\" and current_time - self . last_failure_time > self . reset_timeout : logger . info ( f \"Circuit breaker { self . name } switching to half-open state for testing\" ) self . state = \"HALF_OPEN\" return True # Allow requests when closed or half-open (for testing) return self . state != \"OPEN\" Circuit breakers prevent the system from making repeated calls to failing services, which: Reduces latency for end users Prevents resource exhaustion Allows services to recover Provides early failure detection","title":"Circuit Breaker Pattern"},{"location":"technical/resilience-patterns/#multi-level-fallback","text":"Our fallback mechanisms operate at multiple levels: Primary Service Fallback Default model selection with capability matching Automatic fallback to alternative providers Emergency Fallback Predefined fallback paths when normal selection fails Transformers (local) as ultimate fallback Graceful Degradation Feature downgrading during system stress Simplified user interface elements","title":"Multi-Level Fallback"},{"location":"technical/resilience-patterns/#example-model-selection-fallbacks","text":"def _emergency_fallback ( self ) -> Tuple [ str , float , str ]: \"\"\"Emergency fallback when selection process itself fails\"\"\" logger . critical ( \"Using EMERGENCY FALLBACK for model selection due to critical failure\" ) # Try each emergency fallback path for fallback in self . _emergency_fallbacks : provider = fallback [ \"provider\" ] model = fallback [ \"model\" ] # Skip if circuit breaker is fully open if provider in self . circuit_breakers and self . circuit_breakers [ provider ] . state == \"OPEN\" : continue if ( provider in self . available_backends and model in self . available_backends [ provider ]): logger . info ( f \"Emergency fallback selected: { provider } / { model } \" ) return model , 0.2 , provider # Ultimate emergency fallback - just try transformers/gemma logger . critical ( \"ALL FALLBACKS EXHAUSTED! Using transformers/gemma as final resort\" ) return \"gemma\" , 0.1 , \"transformers\"","title":"Example: Model Selection Fallbacks"},{"location":"technical/resilience-patterns/#adaptive-load-management","text":"We implement several techniques to manage system load:","title":"Adaptive Load Management"},{"location":"technical/resilience-patterns/#1-request-jittering","text":"During periods of system degradation, we intentionally introduce randomness in our service selection to prevent thundering herd problems: if self . _is_system_degraded () and random . random () < 0.2 : # 20% chance backup_model = self . _get_fallback_model ( model_selection [ 0 ], model_selection [ 2 ], available_backends ) if backup_model : logger . info ( f \"System degraded, applying jitter to model selection\" ) model_selection = backup_model","title":"1. Request Jittering"},{"location":"technical/resilience-patterns/#2-tiered-caching","text":"Short-lived cache (10 seconds) for model selection results Medium-lived cache (5 minutes) for topic similarities Long-lived cache (1 hour) for SVG generations Emergency cache for critical responses","title":"2. Tiered Caching"},{"location":"technical/resilience-patterns/#3-feature-throttling","text":"Non-critical features like Easter eggs automatically throttle their activation during system degradation: def should_activate ( self , egg_type : str ) -> bool : # ...existing code... # Reduce activation chance if system is degraded if self . is_system_degraded () and egg_type not in [ \"possum_party\" , \"konami_code\" ]: threshold = min ( threshold , DEGRADED_ACTIVATION_THRESHOLD )","title":"3. Feature Throttling"},{"location":"technical/resilience-patterns/#experience-preservation","text":"Even during degraded operation, we ensure the core user experience remains functional:","title":"Experience Preservation"},{"location":"technical/resilience-patterns/#feature-downgrading","text":"Features are downgraded rather than disabled completely: fallback_features = { \"national_opossum_day\" : { \"ui_theme\" : \"party_opossum_light\" , # Lighter version with fewer animations \"response_modifiers\" : [ \"purple_text\" ], # Only add purple text, no jokes \"animations\" : [] # No animations to reduce load }, # ...other fallbacks... }","title":"Feature Downgrading"},{"location":"technical/resilience-patterns/#svg-generation-resilience","text":"The SVG generation system combines two approaches for maximum resilience: Template-Based Generation Fast, reliable predefined templates Low resource requirements Consistent output quality AI-Powered Generation via Chat2SVG Feature-rich custom SVG generation Falls back to templates when unavailable This hybrid approach ensures users always receive visualizations, even when advanced generation is unavailable.","title":"SVG Generation Resilience"},{"location":"technical/resilience-patterns/#health-monitoring","text":"Our resilience systems rely on comprehensive health monitoring: Service Availability Checks Periodic health checks of all backend services Cached results to prevent excessive checking Circuit State Tracking Circuit breaker states tracked and logged Automatic recovery attempts after timeout periods Degradation Detection Consecutive failures tracked to identify degradation patterns System-wide degradation state determined by multiple provider failures","title":"Health Monitoring"},{"location":"technical/resilience-patterns/#implementation-areas","text":"Component Resilience Patterns Model Selector Circuit breakers, adaptive selection, fallback paths, jittering SVG Generation Dual implementation (templates + Chat2SVG), tiered fallbacks Easter Eggs Activation throttling, feature downgrading, degraded modes API Endpoints Timeouts, rate limiting, caching strategies Frontend Progressive enhancement, graceful UI degradation","title":"Implementation Areas"},{"location":"technical/resilience-patterns/#testing-resilience","text":"We test our resilience patterns through: Chaos Engineering Simulated service outages Random latency injection Resource exhaustion tests Load Testing Traffic spike simulations Concurrent request handling Long-tail latency analysis Degraded Operation Tests Functionality verification during partial outages Recovery time measurements User experience evaluation during degradation","title":"Testing Resilience"},{"location":"technical/resilience-patterns/#dashboard-integration","text":"Our resilience patterns are visibly represented in the service status dashboard, which shows: Current availability of each service Circuit breaker states Fallback selection statistics Degradation indicators Recovery predictions","title":"Dashboard Integration"},{"location":"technical/resilience-patterns/#future-improvements","text":"Planned enhancements to our resilience patterns: Predictive Circuit Breaking Use machine learning to predict failures before they occur Preemptively route around problematic services Adaptive Resource Allocation Dynamically adjust resource allocation based on system load Prioritize critical services during degradation User-Aware Degradation Personalized degradation strategies based on user priorities Transparent communication about system status Multi-Region Resilience Geographic distribution of services Regional circuit breakers and fallbacks","title":"Future Improvements"},{"location":"technical/security-model/","text":"Technical Documentation: Security Model \u00b6 Security Architecture Overview \u00b6 Opossum Search implements a comprehensive security model to protect user data, API infrastructure, and backend services. The security architecture follows these key principles: Defense in Depth : Multiple security controls at different layers Principle of Least Privilege : Minimal access rights for components Secure by Default : Conservative security defaults Fail Secure : Systems fail to a secure state Input/Output Validation : All data is validated at entry and exit points flowchart TD A[Client] --> |TLS| B[Load Balancer] B --> |TLS| C[API Gateway] C --> D{Authentication} D --> |Authenticated| E[GraphQL API] D --> |Unauthenticated| F[Error Response] E --> G[API Key Validation] E --> H[Rate Limiting] E --> I[Input Validation] I --> J[GraphQL Resolvers] J --> K[Backend Services] K --> L[Redis Cache] K --> M[Ollama Service] K --> N[Gemini API] style A fill:#A4C2F4 style B fill:#F9CB9C style D fill:#F9CB9C style F fill:#EA9999 style G fill:#B6D7A8 style H fill:#B6D7A8 style I fill:#B6D7A8 Authentication and Authorization \u00b6 Authentication Mechanisms \u00b6 # app/auth/authentication.py async def authenticate_request ( request ): \"\"\"Authenticate incoming request\"\"\" # Check for API key api_key = request . headers . get ( \"x-api-key\" ) if not api_key : api_key = request . query_params . get ( \"api_key\" ) if api_key : return await validate_api_key ( api_key ) # Check for JWT token auth_header = request . headers . get ( \"authorization\" ) if auth_header and auth_header . startswith ( \"Bearer \" ): token = auth_header . replace ( \"Bearer \" , \"\" ) return await validate_jwt_token ( token ) # No valid authentication found return None Authorization Framework \u00b6 # GraphQL auth directive implementation class AuthDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve required_role = self . args . get ( \"requires\" , \"USER\" ) async def resolve_with_auth ( obj , info , ** kwargs ): # Get user from context user = info . context . get ( \"user\" ) # Check authentication if not user : raise GraphQLError ( \"Authentication required\" ) # Check authorization if required_role == \"ADMIN\" and not user . get ( \"is_admin\" ): raise GraphQLError ( \"Admin privileges required\" ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_auth return field Permission Model \u00b6 Role Capabilities Access Control Anonymous Basic search queries only Rate limited to 10 requests/minute User Standard search, chat, image processing Rate limited to 50 requests/minute Admin All features, system management Rate limited to 100 requests/minute Data Security \u00b6 Data Classification \u00b6 Category Description Examples Protection Public Non-sensitive, publicly available Public documentation Basic validation Internal Non-sensitive, internal use Metrics, logs Access control Confidential Business sensitive API keys, user search history Encryption, access control Restricted Highly sensitive Authentication tokens Encryption, strict access Encryption Implementation \u00b6 # app/security/encryption.py from cryptography.fernet import Fernet class DataEncryption : def __init__ ( self ): self . key = self . _get_encryption_key () self . cipher = Fernet ( self . key ) def _get_encryption_key ( self ): \"\"\"Get encryption key from secure storage\"\"\" key = os . getenv ( \"ENCRYPTION_KEY\" ) if not key : raise SecurityError ( \"Encryption key not configured\" ) return key . encode () def encrypt ( self , data ): \"\"\"Encrypt sensitive data\"\"\" if isinstance ( data , str ): data = data . encode () return self . cipher . encrypt ( data ) def decrypt ( self , data ): \"\"\"Decrypt sensitive data\"\"\" return self . cipher . decrypt ( data ) . decode () Database Security \u00b6 # Redis security configuration in config.py REDIS_HOST = os . getenv ( \"REDIS_HOST\" , \"localhost\" ) REDIS_PORT = int ( os . getenv ( \"REDIS_PORT\" , \"6379\" )) REDIS_PASSWORD = os . getenv ( \"REDIS_PASSWORD\" , \"\" ) REDIS_USE_SSL = os . getenv ( \"REDIS_USE_SSL\" , \"False\" ) . lower () == \"true\" # Redis connection with security settings async def get_redis_client (): \"\"\"Get Redis client with security settings\"\"\" return aioredis . Redis ( host = Config . REDIS_HOST , port = Config . REDIS_PORT , password = Config . REDIS_PASSWORD , ssl = Config . REDIS_USE_SSL , ssl_cert_reqs = \"required\" if Config . REDIS_USE_SSL else None , ssl_ca_certs = Config . REDIS_CA_CERT if Config . REDIS_USE_SSL else None ) Data Retention Policies \u00b6 # app/policies/retention.py async def apply_retention_policies (): \"\"\"Apply data retention policies\"\"\" redis = await get_redis_client () # User search history - keep 90 days retention_days = 90 cutoff = time . time () - ( retention_days * 86400 ) # Remove expired search history user_keys = await redis . keys ( \"user:*:searches\" ) for key in user_keys : await redis . zremrangebyscore ( key , 0 , cutoff ) # Image data - keep 7 days image_retention_days = 7 image_cutoff = time . time () - ( image_retention_days * 86400 ) # Remove expired image data image_keys = await redis . keys ( \"image:data:*\" ) for key in image_keys : timestamp = await redis . hget ( key , \"timestamp\" ) if timestamp and float ( timestamp ) < image_cutoff : await redis . delete ( key ) API Security \u00b6 GraphQL Security Controls \u00b6 # GraphQL security configuration graphql_app = GraphQL ( schema , debug = Config . DEBUG , validation_rules = [ cost_analysis_validation , # Prevent resource-intensive queries depth_limit_validation , # Prevent deeply nested queries query_complexity_validation # Limit overall query complexity ], introspection = Config . GRAPHQL_INTROSPECTION_ENABLED ) Rate Limiting Implementation \u00b6 # app/security/rate_limiting.py class RateLimiter : def __init__ ( self , redis_client ): self . redis = redis_client async def is_rate_limited ( self , client_id , limit_key = \"default\" ): \"\"\"Check if client is rate limited\"\"\" # Get rate limit configuration limits = Config . RATE_LIMITS . get ( limit_key , [ \"100 per day\" ]) # Check each limit for limit in limits : count , period = self . _parse_limit ( limit ) rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Check current count current = await self . redis . get ( rate_key ) if current and int ( current ) >= count : return True # Rate limited return False # Not rate limited async def increment_rate_counter ( self , client_id , limit_key = \"default\" ): \"\"\"Increment rate counters for all applicable limits\"\"\" limits = Config . RATE_LIMITS . get ( limit_key , [ \"100 per day\" ]) for limit in limits : count , period = self . _parse_limit ( limit ) seconds = self . _period_to_seconds ( period ) rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Increment counter and set expiration await self . redis . incr ( rate_key ) await self . redis . expire ( rate_key , seconds , nx = True ) def _parse_limit ( self , limit ): \"\"\"Parse limit string like '100 per day'\"\"\" parts = limit . split ( \" per \" ) count = int ( parts [ 0 ]) period = parts [ 1 ] return count , period def _period_to_seconds ( self , period ): \"\"\"Convert period to seconds\"\"\" if period == \"second\" : return 1 elif period == \"minute\" : return 60 elif period == \"hour\" : return 3600 elif period == \"day\" : return 86400 else : return 86400 # Default to day Input Validation \u00b6 # app/validators/input_validator.py class InputValidator : @staticmethod def validate_query ( query_text ): \"\"\"Validate search query text\"\"\" if not query_text : raise ValidationError ( \"Query cannot be empty\" ) if len ( query_text ) > Config . MAX_QUERY_LENGTH : raise ValidationError ( f \"Query exceeds maximum length of { Config . MAX_QUERY_LENGTH } \" ) # Check for potentially malicious patterns if re . search ( r '<script|javascript:|data:text/html' , query_text , re . IGNORECASE ): raise ValidationError ( \"Query contains potentially unsafe content\" ) return query_text @staticmethod def sanitize_output ( text ): \"\"\"Sanitize output text\"\"\" # Remove potential XSS vectors text = re . sub ( r '<script.*?>.*?</script>' , '' , text , flags = re . DOTALL | re . IGNORECASE ) text = re . sub ( r 'javascript:' , '' , text , flags = re . IGNORECASE ) # Allow safe HTML tags if configured if Config . ALLOW_SAFE_HTML : # Use a whitelist approach allowed_tags = [ 'p' , 'br' , 'b' , 'i' , 'em' , 'strong' , 'ul' , 'ol' , 'li' , 'code' ] # Implementation would use a proper HTML sanitizer like bleach return text # Otherwise strip all HTML return re . sub ( r '<.*?>' , '' , text ) Image Processing Security \u00b6 Image Validation \u00b6 # app/security/image_validation.py def validate_image ( image_data , mime_type = None ): \"\"\"Validate image for security concerns\"\"\" # Check size limits if len ( image_data ) > Config . MAX_IMAGE_SIZE : raise SecurityError ( f \"Image exceeds maximum size of { Config . MAX_IMAGE_SIZE } bytes\" ) # Verify image format allowed_formats = [ 'image/jpeg' , 'image/png' , 'image/gif' , 'image/webp' ] if mime_type and mime_type not in allowed_formats : raise SecurityError ( f \"Image format not allowed: { mime_type } \" ) # Validate image integrity try : with wand . image . Image ( blob = image_data ) as img : # Check for suspiciously large dimensions if img . width > 10000 or img . height > 10000 : raise SecurityError ( f \"Image dimensions too large: { img . width } x { img . height } \" ) # Validate image format matches claimed format if mime_type and f \"image/ { img . format . lower () } \" != mime_type . lower (): raise SecurityError ( \"Image format doesn't match claimed format\" ) # Check for format-specific exploits if img . format . lower () == 'svg' : # SVG can contain script tags raise SecurityError ( \"SVG format not accepted for upload\" ) return True except Exception as e : raise SecurityError ( f \"Image validation failed: { str ( e ) } \" ) SVG Sanitization \u00b6 # app/security/svg_sanitization.py def sanitize_svg ( svg_markup ): \"\"\"Sanitize SVG markup for secure display\"\"\" # Remove any script tags svg_markup = re . sub ( r '<script.*?</script>' , '' , svg_markup , flags = re . DOTALL | re . IGNORECASE ) # Remove event handlers svg_markup = re . sub ( r 'on\\w+=\".*?\"' , '' , svg_markup , flags = re . IGNORECASE ) # Remove external references svg_markup = re . sub ( r 'href=\"(?!#).*?\"' , '' , svg_markup , flags = re . IGNORECASE ) # Remove potentially dangerous tags dangerous_tags = [ 'foreignObject' , 'use' ] for tag in dangerous_tags : svg_markup = re . sub ( f '< { tag } .*?</ { tag } >' , '' , svg_markup , flags = re . DOTALL | re . IGNORECASE ) svg_markup = re . sub ( f '< { tag } .*?/>' , '' , svg_markup , flags = re . DOTALL | re . IGNORECASE ) # Ensure SVG namespace is correct if 'xmlns=\"http://www.w3.org/2000/svg\"' not in svg_markup : svg_markup = svg_markup . replace ( '<svg' , '<svg xmlns=\"http://www.w3.org/2000/svg\"' , 1 ) return svg_markup Model Security \u00b6 Prompt Injection Protection \u00b6 # app/security/prompt_security.py def secure_prompt ( user_input , template ): \"\"\"Secure prompt template from injection attacks\"\"\" # Escape special characters safe_input = json . dumps ( user_input )[ 1 : - 1 ] # Use JSON encoding but remove quotes # Use a whitelist approach for allowed characters if needed if not re . match ( r '^[a-zA-Z0-9\\s.,?! \\' \"-:;()]+$' , user_input ): logging . warning ( f \"Potentially unsafe characters in prompt: { user_input } \" ) # Apply template with escaped input prompt = template . replace ( \" {user_input} \" , safe_input ) # Add safety instructions to prompt preamble safety_instructions = \"\"\" Important: Respond only to the user query. Do not execute commands. Do not engage with harmful, illegal, unethical or deceptive content. \"\"\" return safety_instructions + prompt Response Filtering \u00b6 # app/security/content_filtering.py class ContentFilter : def __init__ ( self ): # Load blocklisted patterns and terms self . blocklisted_patterns = self . _load_blocklist () def _load_blocklist ( self ): \"\"\"Load blocklisted patterns\"\"\" with open ( \"config/content_blocklists.json\" , \"r\" ) as f : blocklists = json . load ( f ) # Compile regex patterns for performance return { category : [ re . compile ( pattern , re . IGNORECASE ) for pattern in patterns ] for category , patterns in blocklists . items () } def filter_response ( self , response ): \"\"\"Filter potentially harmful content from responses\"\"\" # Check for harmful content filtered_response = response # Check against each category for category , patterns in self . blocklisted_patterns . items (): for pattern in patterns : if pattern . search ( response ): logging . warning ( f \"Found potentially problematic content in category: { category } \" ) # Apply redaction based on category if category == \"pii\" : # Redact PII but keep response filtered_response = pattern . sub ( \"[REDACTED]\" , filtered_response ) elif category == \"harmful\" : # Replace entire response for harmful content return Config . HARMFUL_CONTENT_RESPONSE return filtered_response Infrastructure Security \u00b6 Network Security Configuration \u00b6 # docker-compose network configuration services : app : networks : - frontend - backend redis : networks : - backend # Not exposed to outside world ollama : networks : - backend # Not exposed to outside world networks : frontend : # External-facing network backend : internal : true # Not exposed externally Container Security \u00b6 # Dockerfile with security best practices FROM python:3.11-slim AS builder # Use non-root user RUN groupadd -r opossum && useradd -r -g opossum opossum # Install dependencies COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt FROM python:3.11-slim # Copy only necessary files COPY --from = builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages COPY app/ /app/ # Set working directory WORKDIR /app # Use non-root user USER opossum # Set secure env defaults ENV PYTHONUNBUFFERED = 1 \\ PYTHONDONTWRITEBYTECODE = 1 \\ PIP_DISABLE_PIP_VERSION_CHECK = 1 \\ PIP_NO_CACHE_DIR = 1 # Use unbuffered output, but don't write bytecode # Healthcheck to verify application is running properly HEALTHCHECK --interval = 30s --timeout = 5s --start-period = 5s --retries = 3 \\ CMD curl -f http://localhost:8000/health || exit 1 ENTRYPOINT [ \"python\" , \"main.py\" ] Secret Management \u00b6 # Kubernetes secrets configuration apiVersion : v1 kind : Secret metadata : name : opossum-secrets type : Opaque data : gemini-api-key : <base64-encoded-key> redis-password : <base64-encoded-password> encryption-key : <base64-encoded-key> Monitoring and Incident Response \u00b6 Security Logging \u00b6 # app/monitoring/security_logging.py class SecurityLogger : def __init__ ( self ): self . logger = logging . getLogger ( \"security\" ) # Configure structured logging self . logger . setLevel ( logging . INFO ) formatter = logging . Formatter ( '{\"timestamp\": \" %(asctime)s \", \"level\": \" %(levelname)s \", ' '\"event\": \" %(message)s \", \"context\": %(context)s }' ) handler = logging . StreamHandler () handler . setFormatter ( formatter ) self . logger . addHandler ( handler ) def log_security_event ( self , event_type , details , severity = \"INFO\" ): \"\"\"Log security-related event\"\"\" context = { \"event_type\" : event_type , \"details\" : details , \"severity\" : severity } # Add to context context_json = json . dumps ( context ) # Log event self . logger . info ( f \"security_ { event_type } \" , extra = { \"context\" : context_json }) # For high-severity events, push to alerts if severity in [ \"CRITICAL\" , \"HIGH\" ]: self . _push_security_alert ( event_type , details , severity ) def _push_security_alert ( self , event_type , details , severity ): \"\"\"Push high-severity security alerts\"\"\" alert = { \"event_type\" : event_type , \"details\" : details , \"severity\" : severity , \"timestamp\" : datetime . datetime . now () . isoformat () } # Push to Redis for alerting redis_client . lpush ( \"security:alerts\" , json . dumps ( alert )) redis_client . publish ( \"security:alerts:channel\" , json . dumps ( alert )) Intrusion Detection \u00b6 # app/security/intrusion_detection.py class IntrusionDetection : def __init__ ( self , redis_client ): self . redis = redis_client self . security_logger = SecurityLogger () async def check_request_pattern ( self , request , client_ip ): \"\"\"Check for suspicious request patterns\"\"\" # Track requests per IP key = f \"security:requests: { client_ip } \" await self . redis . incr ( key ) await self . redis . expire ( key , 3600 ) # Expire after 1 hour # Get request count in last hour request_count = int ( await self . redis . get ( key ) or 0 ) # Check for rate-based attacks if request_count > Config . SECURITY_REQUEST_THRESHOLD : self . security_logger . log_security_event ( \"rate_limit_exceeded\" , { \"client_ip\" : client_ip , \"request_count\" : request_count }, \"MEDIUM\" ) # Check for path traversal attempts path = request . url . path if re . search ( r '\\.\\./' , path ) or re . search ( r 'etc/passwd' , path ): self . security_logger . log_security_event ( \"path_traversal_attempt\" , { \"client_ip\" : client_ip , \"path\" : path }, \"HIGH\" ) return True # Check for SQL injection attempts query = request . query_params . get ( \"query\" , \"\" ) if re . search ( r 'UNION\\s+SELECT|SELECT\\s+FROM' , query , re . IGNORECASE ): self . security_logger . log_security_event ( \"sql_injection_attempt\" , { \"client_ip\" : client_ip , \"query\" : query }, \"HIGH\" ) return True return False # No intrusion detected Incident Response Plan \u00b6 # app/security/incident_response.py class IncidentResponse : def __init__ ( self ): self . security_logger = SecurityLogger () async def handle_security_incident ( self , incident_type , details ): \"\"\"Handle security incident based on type and severity\"\"\" # Log the incident self . security_logger . log_security_event ( incident_type , details , details . get ( \"severity\" , \"MEDIUM\" ) ) # Take automated action based on incident type if incident_type == \"api_key_compromise\" : await self . _handle_compromised_api_key ( details ) elif incident_type == \"rate_limit_abuse\" : await self . _handle_rate_abuse ( details ) elif incident_type == \"injection_attempt\" : await self . _handle_injection_attempt ( details ) # Notify security team for high severity incidents if details . get ( \"severity\" ) in [ \"HIGH\" , \"CRITICAL\" ]: await self . _notify_security_team ( incident_type , details ) async def _handle_compromised_api_key ( self , details ): \"\"\"Handle potentially compromised API key\"\"\" # Disable the API key api_key = details . get ( \"api_key\" ) if api_key : # Mark as compromised await redis_client . set ( f \"security:compromised_key: { api_key } \" , 1 ) # Set short expiry for any active sessions await redis_client . expire ( f \"session:key: { api_key } \" , 60 ) Compliance Framework \u00b6 Compliance Controls \u00b6 Requirement Implementation Verification Data Encryption All sensitive data encrypted at rest and in transit Regular security audits Access Control Role-based access with least privilege Permission reviews Audit Logging Comprehensive logging of security events Log monitoring Data Protection Data retention policies, minimization Periodic data reviews Vulnerability Management Regular dependency updates, scanning CI/CD pipeline checks Privacy by Design \u00b6 # app/privacy/data_minimization.py class DataMinimization : @staticmethod def minimize_query_logs ( query_data ): \"\"\"Minimize data stored in query logs\"\"\" # Only store necessary fields minimized = { \"query_type\" : query_data . get ( \"query_type\" ), \"timestamp\" : query_data . get ( \"timestamp\" ), \"backend_used\" : query_data . get ( \"backend_used\" ), \"response_time\" : query_data . get ( \"response_time\" ) } # Generate non-identifying hash of original query query_text = query_data . get ( \"query_text\" , \"\" ) if query_text : minimized [ \"query_hash\" ] = hashlib . sha256 ( query_text . encode ()) . hexdigest () # Don't store PII if \"user_id\" in query_data and not Config . STORE_USER_ASSOCIATION : minimized [ \"user_id\" ] = hashlib . sha256 ( query_data [ \"user_id\" ] . encode () ) . hexdigest () return minimized Data Subject Rights \u00b6 # app/privacy/dsr.py class DataSubjectRights : def __init__ ( self , redis_client ): self . redis = redis_client async def get_user_data ( self , user_id ): \"\"\"Get all data associated with a user\"\"\" # Collect all data related to user user_data = {} # Get user profile profile = await self . redis . hgetall ( f \"user: { user_id } :profile\" ) if profile : user_data [ \"profile\" ] = profile # Get user search history searches = await self . redis . zrange ( f \"user: { user_id } :searches\" , 0 , - 1 , withscores = True ) if searches : user_data [ \"searches\" ] = [ { \"query\" : s [ 0 ] . decode (), \"timestamp\" : s [ 1 ]} for s in searches ] return user_data async def delete_user_data ( self , user_id ): \"\"\"Delete all data for a user (right to be forgotten)\"\"\" # Collect keys related to user user_keys = await self . redis . keys ( f \"user: { user_id } :*\" ) # Delete all keys if user_keys : await self . redis . delete ( * user_keys ) # Log deletion for compliance await self . redis . lpush ( \"privacy:deletion_log\" , json . dumps ({ \"user_id_hash\" : hashlib . sha256 ( user_id . encode ()) . hexdigest (), \"timestamp\" : datetime . datetime . now () . isoformat (), \"keys_deleted\" : len ( user_keys ) }) ) return { \"deleted\" : True , \"key_count\" : len ( user_keys )} The Opossum Search security model provides comprehensive protection across all layers of the application, from user authentication through data processing to infrastructure security. By implementing these security controls, the system maintains confidentiality, integrity, and availability while meeting regulatory compliance requirements.","title":"Security Model"},{"location":"technical/security-model/#technical-documentation-security-model","text":"","title":"Technical Documentation: Security Model"},{"location":"technical/security-model/#security-architecture-overview","text":"Opossum Search implements a comprehensive security model to protect user data, API infrastructure, and backend services. The security architecture follows these key principles: Defense in Depth : Multiple security controls at different layers Principle of Least Privilege : Minimal access rights for components Secure by Default : Conservative security defaults Fail Secure : Systems fail to a secure state Input/Output Validation : All data is validated at entry and exit points flowchart TD A[Client] --> |TLS| B[Load Balancer] B --> |TLS| C[API Gateway] C --> D{Authentication} D --> |Authenticated| E[GraphQL API] D --> |Unauthenticated| F[Error Response] E --> G[API Key Validation] E --> H[Rate Limiting] E --> I[Input Validation] I --> J[GraphQL Resolvers] J --> K[Backend Services] K --> L[Redis Cache] K --> M[Ollama Service] K --> N[Gemini API] style A fill:#A4C2F4 style B fill:#F9CB9C style D fill:#F9CB9C style F fill:#EA9999 style G fill:#B6D7A8 style H fill:#B6D7A8 style I fill:#B6D7A8","title":"Security Architecture Overview"},{"location":"technical/security-model/#authentication-and-authorization","text":"","title":"Authentication and Authorization"},{"location":"technical/security-model/#authentication-mechanisms","text":"# app/auth/authentication.py async def authenticate_request ( request ): \"\"\"Authenticate incoming request\"\"\" # Check for API key api_key = request . headers . get ( \"x-api-key\" ) if not api_key : api_key = request . query_params . get ( \"api_key\" ) if api_key : return await validate_api_key ( api_key ) # Check for JWT token auth_header = request . headers . get ( \"authorization\" ) if auth_header and auth_header . startswith ( \"Bearer \" ): token = auth_header . replace ( \"Bearer \" , \"\" ) return await validate_jwt_token ( token ) # No valid authentication found return None","title":"Authentication Mechanisms"},{"location":"technical/security-model/#authorization-framework","text":"# GraphQL auth directive implementation class AuthDirective ( SchemaDirectiveVisitor ): def visit_field_definition ( self , field , object_type ): original_resolver = field . resolve required_role = self . args . get ( \"requires\" , \"USER\" ) async def resolve_with_auth ( obj , info , ** kwargs ): # Get user from context user = info . context . get ( \"user\" ) # Check authentication if not user : raise GraphQLError ( \"Authentication required\" ) # Check authorization if required_role == \"ADMIN\" and not user . get ( \"is_admin\" ): raise GraphQLError ( \"Admin privileges required\" ) # Call original resolver return await original_resolver ( obj , info , ** kwargs ) field . resolve = resolve_with_auth return field","title":"Authorization Framework"},{"location":"technical/security-model/#permission-model","text":"Role Capabilities Access Control Anonymous Basic search queries only Rate limited to 10 requests/minute User Standard search, chat, image processing Rate limited to 50 requests/minute Admin All features, system management Rate limited to 100 requests/minute","title":"Permission Model"},{"location":"technical/security-model/#data-security","text":"","title":"Data Security"},{"location":"technical/security-model/#data-classification","text":"Category Description Examples Protection Public Non-sensitive, publicly available Public documentation Basic validation Internal Non-sensitive, internal use Metrics, logs Access control Confidential Business sensitive API keys, user search history Encryption, access control Restricted Highly sensitive Authentication tokens Encryption, strict access","title":"Data Classification"},{"location":"technical/security-model/#encryption-implementation","text":"# app/security/encryption.py from cryptography.fernet import Fernet class DataEncryption : def __init__ ( self ): self . key = self . _get_encryption_key () self . cipher = Fernet ( self . key ) def _get_encryption_key ( self ): \"\"\"Get encryption key from secure storage\"\"\" key = os . getenv ( \"ENCRYPTION_KEY\" ) if not key : raise SecurityError ( \"Encryption key not configured\" ) return key . encode () def encrypt ( self , data ): \"\"\"Encrypt sensitive data\"\"\" if isinstance ( data , str ): data = data . encode () return self . cipher . encrypt ( data ) def decrypt ( self , data ): \"\"\"Decrypt sensitive data\"\"\" return self . cipher . decrypt ( data ) . decode ()","title":"Encryption Implementation"},{"location":"technical/security-model/#database-security","text":"# Redis security configuration in config.py REDIS_HOST = os . getenv ( \"REDIS_HOST\" , \"localhost\" ) REDIS_PORT = int ( os . getenv ( \"REDIS_PORT\" , \"6379\" )) REDIS_PASSWORD = os . getenv ( \"REDIS_PASSWORD\" , \"\" ) REDIS_USE_SSL = os . getenv ( \"REDIS_USE_SSL\" , \"False\" ) . lower () == \"true\" # Redis connection with security settings async def get_redis_client (): \"\"\"Get Redis client with security settings\"\"\" return aioredis . Redis ( host = Config . REDIS_HOST , port = Config . REDIS_PORT , password = Config . REDIS_PASSWORD , ssl = Config . REDIS_USE_SSL , ssl_cert_reqs = \"required\" if Config . REDIS_USE_SSL else None , ssl_ca_certs = Config . REDIS_CA_CERT if Config . REDIS_USE_SSL else None )","title":"Database Security"},{"location":"technical/security-model/#data-retention-policies","text":"# app/policies/retention.py async def apply_retention_policies (): \"\"\"Apply data retention policies\"\"\" redis = await get_redis_client () # User search history - keep 90 days retention_days = 90 cutoff = time . time () - ( retention_days * 86400 ) # Remove expired search history user_keys = await redis . keys ( \"user:*:searches\" ) for key in user_keys : await redis . zremrangebyscore ( key , 0 , cutoff ) # Image data - keep 7 days image_retention_days = 7 image_cutoff = time . time () - ( image_retention_days * 86400 ) # Remove expired image data image_keys = await redis . keys ( \"image:data:*\" ) for key in image_keys : timestamp = await redis . hget ( key , \"timestamp\" ) if timestamp and float ( timestamp ) < image_cutoff : await redis . delete ( key )","title":"Data Retention Policies"},{"location":"technical/security-model/#api-security","text":"","title":"API Security"},{"location":"technical/security-model/#graphql-security-controls","text":"# GraphQL security configuration graphql_app = GraphQL ( schema , debug = Config . DEBUG , validation_rules = [ cost_analysis_validation , # Prevent resource-intensive queries depth_limit_validation , # Prevent deeply nested queries query_complexity_validation # Limit overall query complexity ], introspection = Config . GRAPHQL_INTROSPECTION_ENABLED )","title":"GraphQL Security Controls"},{"location":"technical/security-model/#rate-limiting-implementation","text":"# app/security/rate_limiting.py class RateLimiter : def __init__ ( self , redis_client ): self . redis = redis_client async def is_rate_limited ( self , client_id , limit_key = \"default\" ): \"\"\"Check if client is rate limited\"\"\" # Get rate limit configuration limits = Config . RATE_LIMITS . get ( limit_key , [ \"100 per day\" ]) # Check each limit for limit in limits : count , period = self . _parse_limit ( limit ) rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Check current count current = await self . redis . get ( rate_key ) if current and int ( current ) >= count : return True # Rate limited return False # Not rate limited async def increment_rate_counter ( self , client_id , limit_key = \"default\" ): \"\"\"Increment rate counters for all applicable limits\"\"\" limits = Config . RATE_LIMITS . get ( limit_key , [ \"100 per day\" ]) for limit in limits : count , period = self . _parse_limit ( limit ) seconds = self . _period_to_seconds ( period ) rate_key = f \"ratelimit: { client_id } : { limit_key } : { period } \" # Increment counter and set expiration await self . redis . incr ( rate_key ) await self . redis . expire ( rate_key , seconds , nx = True ) def _parse_limit ( self , limit ): \"\"\"Parse limit string like '100 per day'\"\"\" parts = limit . split ( \" per \" ) count = int ( parts [ 0 ]) period = parts [ 1 ] return count , period def _period_to_seconds ( self , period ): \"\"\"Convert period to seconds\"\"\" if period == \"second\" : return 1 elif period == \"minute\" : return 60 elif period == \"hour\" : return 3600 elif period == \"day\" : return 86400 else : return 86400 # Default to day","title":"Rate Limiting Implementation"},{"location":"technical/security-model/#input-validation","text":"# app/validators/input_validator.py class InputValidator : @staticmethod def validate_query ( query_text ): \"\"\"Validate search query text\"\"\" if not query_text : raise ValidationError ( \"Query cannot be empty\" ) if len ( query_text ) > Config . MAX_QUERY_LENGTH : raise ValidationError ( f \"Query exceeds maximum length of { Config . MAX_QUERY_LENGTH } \" ) # Check for potentially malicious patterns if re . search ( r '<script|javascript:|data:text/html' , query_text , re . IGNORECASE ): raise ValidationError ( \"Query contains potentially unsafe content\" ) return query_text @staticmethod def sanitize_output ( text ): \"\"\"Sanitize output text\"\"\" # Remove potential XSS vectors text = re . sub ( r '<script.*?>.*?</script>' , '' , text , flags = re . DOTALL | re . IGNORECASE ) text = re . sub ( r 'javascript:' , '' , text , flags = re . IGNORECASE ) # Allow safe HTML tags if configured if Config . ALLOW_SAFE_HTML : # Use a whitelist approach allowed_tags = [ 'p' , 'br' , 'b' , 'i' , 'em' , 'strong' , 'ul' , 'ol' , 'li' , 'code' ] # Implementation would use a proper HTML sanitizer like bleach return text # Otherwise strip all HTML return re . sub ( r '<.*?>' , '' , text )","title":"Input Validation"},{"location":"technical/security-model/#image-processing-security","text":"","title":"Image Processing Security"},{"location":"technical/security-model/#image-validation","text":"# app/security/image_validation.py def validate_image ( image_data , mime_type = None ): \"\"\"Validate image for security concerns\"\"\" # Check size limits if len ( image_data ) > Config . MAX_IMAGE_SIZE : raise SecurityError ( f \"Image exceeds maximum size of { Config . MAX_IMAGE_SIZE } bytes\" ) # Verify image format allowed_formats = [ 'image/jpeg' , 'image/png' , 'image/gif' , 'image/webp' ] if mime_type and mime_type not in allowed_formats : raise SecurityError ( f \"Image format not allowed: { mime_type } \" ) # Validate image integrity try : with wand . image . Image ( blob = image_data ) as img : # Check for suspiciously large dimensions if img . width > 10000 or img . height > 10000 : raise SecurityError ( f \"Image dimensions too large: { img . width } x { img . height } \" ) # Validate image format matches claimed format if mime_type and f \"image/ { img . format . lower () } \" != mime_type . lower (): raise SecurityError ( \"Image format doesn't match claimed format\" ) # Check for format-specific exploits if img . format . lower () == 'svg' : # SVG can contain script tags raise SecurityError ( \"SVG format not accepted for upload\" ) return True except Exception as e : raise SecurityError ( f \"Image validation failed: { str ( e ) } \" )","title":"Image Validation"},{"location":"technical/security-model/#svg-sanitization","text":"# app/security/svg_sanitization.py def sanitize_svg ( svg_markup ): \"\"\"Sanitize SVG markup for secure display\"\"\" # Remove any script tags svg_markup = re . sub ( r '<script.*?</script>' , '' , svg_markup , flags = re . DOTALL | re . IGNORECASE ) # Remove event handlers svg_markup = re . sub ( r 'on\\w+=\".*?\"' , '' , svg_markup , flags = re . IGNORECASE ) # Remove external references svg_markup = re . sub ( r 'href=\"(?!#).*?\"' , '' , svg_markup , flags = re . IGNORECASE ) # Remove potentially dangerous tags dangerous_tags = [ 'foreignObject' , 'use' ] for tag in dangerous_tags : svg_markup = re . sub ( f '< { tag } .*?</ { tag } >' , '' , svg_markup , flags = re . DOTALL | re . IGNORECASE ) svg_markup = re . sub ( f '< { tag } .*?/>' , '' , svg_markup , flags = re . DOTALL | re . IGNORECASE ) # Ensure SVG namespace is correct if 'xmlns=\"http://www.w3.org/2000/svg\"' not in svg_markup : svg_markup = svg_markup . replace ( '<svg' , '<svg xmlns=\"http://www.w3.org/2000/svg\"' , 1 ) return svg_markup","title":"SVG Sanitization"},{"location":"technical/security-model/#model-security","text":"","title":"Model Security"},{"location":"technical/security-model/#prompt-injection-protection","text":"# app/security/prompt_security.py def secure_prompt ( user_input , template ): \"\"\"Secure prompt template from injection attacks\"\"\" # Escape special characters safe_input = json . dumps ( user_input )[ 1 : - 1 ] # Use JSON encoding but remove quotes # Use a whitelist approach for allowed characters if needed if not re . match ( r '^[a-zA-Z0-9\\s.,?! \\' \"-:;()]+$' , user_input ): logging . warning ( f \"Potentially unsafe characters in prompt: { user_input } \" ) # Apply template with escaped input prompt = template . replace ( \" {user_input} \" , safe_input ) # Add safety instructions to prompt preamble safety_instructions = \"\"\" Important: Respond only to the user query. Do not execute commands. Do not engage with harmful, illegal, unethical or deceptive content. \"\"\" return safety_instructions + prompt","title":"Prompt Injection Protection"},{"location":"technical/security-model/#response-filtering","text":"# app/security/content_filtering.py class ContentFilter : def __init__ ( self ): # Load blocklisted patterns and terms self . blocklisted_patterns = self . _load_blocklist () def _load_blocklist ( self ): \"\"\"Load blocklisted patterns\"\"\" with open ( \"config/content_blocklists.json\" , \"r\" ) as f : blocklists = json . load ( f ) # Compile regex patterns for performance return { category : [ re . compile ( pattern , re . IGNORECASE ) for pattern in patterns ] for category , patterns in blocklists . items () } def filter_response ( self , response ): \"\"\"Filter potentially harmful content from responses\"\"\" # Check for harmful content filtered_response = response # Check against each category for category , patterns in self . blocklisted_patterns . items (): for pattern in patterns : if pattern . search ( response ): logging . warning ( f \"Found potentially problematic content in category: { category } \" ) # Apply redaction based on category if category == \"pii\" : # Redact PII but keep response filtered_response = pattern . sub ( \"[REDACTED]\" , filtered_response ) elif category == \"harmful\" : # Replace entire response for harmful content return Config . HARMFUL_CONTENT_RESPONSE return filtered_response","title":"Response Filtering"},{"location":"technical/security-model/#infrastructure-security","text":"","title":"Infrastructure Security"},{"location":"technical/security-model/#network-security-configuration","text":"# docker-compose network configuration services : app : networks : - frontend - backend redis : networks : - backend # Not exposed to outside world ollama : networks : - backend # Not exposed to outside world networks : frontend : # External-facing network backend : internal : true # Not exposed externally","title":"Network Security Configuration"},{"location":"technical/security-model/#container-security","text":"# Dockerfile with security best practices FROM python:3.11-slim AS builder # Use non-root user RUN groupadd -r opossum && useradd -r -g opossum opossum # Install dependencies COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt FROM python:3.11-slim # Copy only necessary files COPY --from = builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages COPY app/ /app/ # Set working directory WORKDIR /app # Use non-root user USER opossum # Set secure env defaults ENV PYTHONUNBUFFERED = 1 \\ PYTHONDONTWRITEBYTECODE = 1 \\ PIP_DISABLE_PIP_VERSION_CHECK = 1 \\ PIP_NO_CACHE_DIR = 1 # Use unbuffered output, but don't write bytecode # Healthcheck to verify application is running properly HEALTHCHECK --interval = 30s --timeout = 5s --start-period = 5s --retries = 3 \\ CMD curl -f http://localhost:8000/health || exit 1 ENTRYPOINT [ \"python\" , \"main.py\" ]","title":"Container Security"},{"location":"technical/security-model/#secret-management","text":"# Kubernetes secrets configuration apiVersion : v1 kind : Secret metadata : name : opossum-secrets type : Opaque data : gemini-api-key : <base64-encoded-key> redis-password : <base64-encoded-password> encryption-key : <base64-encoded-key>","title":"Secret Management"},{"location":"technical/security-model/#monitoring-and-incident-response","text":"","title":"Monitoring and Incident Response"},{"location":"technical/security-model/#security-logging","text":"# app/monitoring/security_logging.py class SecurityLogger : def __init__ ( self ): self . logger = logging . getLogger ( \"security\" ) # Configure structured logging self . logger . setLevel ( logging . INFO ) formatter = logging . Formatter ( '{\"timestamp\": \" %(asctime)s \", \"level\": \" %(levelname)s \", ' '\"event\": \" %(message)s \", \"context\": %(context)s }' ) handler = logging . StreamHandler () handler . setFormatter ( formatter ) self . logger . addHandler ( handler ) def log_security_event ( self , event_type , details , severity = \"INFO\" ): \"\"\"Log security-related event\"\"\" context = { \"event_type\" : event_type , \"details\" : details , \"severity\" : severity } # Add to context context_json = json . dumps ( context ) # Log event self . logger . info ( f \"security_ { event_type } \" , extra = { \"context\" : context_json }) # For high-severity events, push to alerts if severity in [ \"CRITICAL\" , \"HIGH\" ]: self . _push_security_alert ( event_type , details , severity ) def _push_security_alert ( self , event_type , details , severity ): \"\"\"Push high-severity security alerts\"\"\" alert = { \"event_type\" : event_type , \"details\" : details , \"severity\" : severity , \"timestamp\" : datetime . datetime . now () . isoformat () } # Push to Redis for alerting redis_client . lpush ( \"security:alerts\" , json . dumps ( alert )) redis_client . publish ( \"security:alerts:channel\" , json . dumps ( alert ))","title":"Security Logging"},{"location":"technical/security-model/#intrusion-detection","text":"# app/security/intrusion_detection.py class IntrusionDetection : def __init__ ( self , redis_client ): self . redis = redis_client self . security_logger = SecurityLogger () async def check_request_pattern ( self , request , client_ip ): \"\"\"Check for suspicious request patterns\"\"\" # Track requests per IP key = f \"security:requests: { client_ip } \" await self . redis . incr ( key ) await self . redis . expire ( key , 3600 ) # Expire after 1 hour # Get request count in last hour request_count = int ( await self . redis . get ( key ) or 0 ) # Check for rate-based attacks if request_count > Config . SECURITY_REQUEST_THRESHOLD : self . security_logger . log_security_event ( \"rate_limit_exceeded\" , { \"client_ip\" : client_ip , \"request_count\" : request_count }, \"MEDIUM\" ) # Check for path traversal attempts path = request . url . path if re . search ( r '\\.\\./' , path ) or re . search ( r 'etc/passwd' , path ): self . security_logger . log_security_event ( \"path_traversal_attempt\" , { \"client_ip\" : client_ip , \"path\" : path }, \"HIGH\" ) return True # Check for SQL injection attempts query = request . query_params . get ( \"query\" , \"\" ) if re . search ( r 'UNION\\s+SELECT|SELECT\\s+FROM' , query , re . IGNORECASE ): self . security_logger . log_security_event ( \"sql_injection_attempt\" , { \"client_ip\" : client_ip , \"query\" : query }, \"HIGH\" ) return True return False # No intrusion detected","title":"Intrusion Detection"},{"location":"technical/security-model/#incident-response-plan","text":"# app/security/incident_response.py class IncidentResponse : def __init__ ( self ): self . security_logger = SecurityLogger () async def handle_security_incident ( self , incident_type , details ): \"\"\"Handle security incident based on type and severity\"\"\" # Log the incident self . security_logger . log_security_event ( incident_type , details , details . get ( \"severity\" , \"MEDIUM\" ) ) # Take automated action based on incident type if incident_type == \"api_key_compromise\" : await self . _handle_compromised_api_key ( details ) elif incident_type == \"rate_limit_abuse\" : await self . _handle_rate_abuse ( details ) elif incident_type == \"injection_attempt\" : await self . _handle_injection_attempt ( details ) # Notify security team for high severity incidents if details . get ( \"severity\" ) in [ \"HIGH\" , \"CRITICAL\" ]: await self . _notify_security_team ( incident_type , details ) async def _handle_compromised_api_key ( self , details ): \"\"\"Handle potentially compromised API key\"\"\" # Disable the API key api_key = details . get ( \"api_key\" ) if api_key : # Mark as compromised await redis_client . set ( f \"security:compromised_key: { api_key } \" , 1 ) # Set short expiry for any active sessions await redis_client . expire ( f \"session:key: { api_key } \" , 60 )","title":"Incident Response Plan"},{"location":"technical/security-model/#compliance-framework","text":"","title":"Compliance Framework"},{"location":"technical/security-model/#compliance-controls","text":"Requirement Implementation Verification Data Encryption All sensitive data encrypted at rest and in transit Regular security audits Access Control Role-based access with least privilege Permission reviews Audit Logging Comprehensive logging of security events Log monitoring Data Protection Data retention policies, minimization Periodic data reviews Vulnerability Management Regular dependency updates, scanning CI/CD pipeline checks","title":"Compliance Controls"},{"location":"technical/security-model/#privacy-by-design","text":"# app/privacy/data_minimization.py class DataMinimization : @staticmethod def minimize_query_logs ( query_data ): \"\"\"Minimize data stored in query logs\"\"\" # Only store necessary fields minimized = { \"query_type\" : query_data . get ( \"query_type\" ), \"timestamp\" : query_data . get ( \"timestamp\" ), \"backend_used\" : query_data . get ( \"backend_used\" ), \"response_time\" : query_data . get ( \"response_time\" ) } # Generate non-identifying hash of original query query_text = query_data . get ( \"query_text\" , \"\" ) if query_text : minimized [ \"query_hash\" ] = hashlib . sha256 ( query_text . encode ()) . hexdigest () # Don't store PII if \"user_id\" in query_data and not Config . STORE_USER_ASSOCIATION : minimized [ \"user_id\" ] = hashlib . sha256 ( query_data [ \"user_id\" ] . encode () ) . hexdigest () return minimized","title":"Privacy by Design"},{"location":"technical/security-model/#data-subject-rights","text":"# app/privacy/dsr.py class DataSubjectRights : def __init__ ( self , redis_client ): self . redis = redis_client async def get_user_data ( self , user_id ): \"\"\"Get all data associated with a user\"\"\" # Collect all data related to user user_data = {} # Get user profile profile = await self . redis . hgetall ( f \"user: { user_id } :profile\" ) if profile : user_data [ \"profile\" ] = profile # Get user search history searches = await self . redis . zrange ( f \"user: { user_id } :searches\" , 0 , - 1 , withscores = True ) if searches : user_data [ \"searches\" ] = [ { \"query\" : s [ 0 ] . decode (), \"timestamp\" : s [ 1 ]} for s in searches ] return user_data async def delete_user_data ( self , user_id ): \"\"\"Delete all data for a user (right to be forgotten)\"\"\" # Collect keys related to user user_keys = await self . redis . keys ( f \"user: { user_id } :*\" ) # Delete all keys if user_keys : await self . redis . delete ( * user_keys ) # Log deletion for compliance await self . redis . lpush ( \"privacy:deletion_log\" , json . dumps ({ \"user_id_hash\" : hashlib . sha256 ( user_id . encode ()) . hexdigest (), \"timestamp\" : datetime . datetime . now () . isoformat (), \"keys_deleted\" : len ( user_keys ) }) ) return { \"deleted\" : True , \"key_count\" : len ( user_keys )} The Opossum Search security model provides comprehensive protection across all layers of the application, from user authentication through data processing to infrastructure security. By implementing these security controls, the system maintains confidentiality, integrity, and availability while meeting regulatory compliance requirements.","title":"Data Subject Rights"},{"location":"technical/svg-markup/","text":"Technical Documentation: SVG Markup Generation \u00b6 Overview: LLM-Driven SVG Generation \u00b6 The Opossum Search platform uses LLMs to generate SVG markup directly for creating dynamic visualizations: User provides a natural language request (e.g., \"Show me a pie chart of opossum diet\") LLM generates SVG markup (XML-based vector graphics) Server validates and returns the SVG markup Browser renders the SVG natively Security Analysis \u00b6 Why SVG Markup Generation is Secure \u00b6 Declarative vs. Procedural SVG is declarative markup , not executable code Describes what to display, not how to execute operations No ability to access system resources or perform operations Browser Rendering SVG is rendered by the client browser, not executed on the server Server only passes the validated markup Follows same security model as other web content (HTML/CSS) Validation Capabilities SVG can be validated against known schemas Potential for sanitizing any scripting elements (like embedded JavaScript) def sanitize_svg ( svg_markup ): # Remove any script tags or event handlers sanitized = re . sub ( r '<script.*?</script>' , '' , svg_markup , flags = re . DOTALL ) sanitized = re . sub ( r 'on\\w+=\".*?\"' , '' , sanitized ) return sanitized Comparison with Script-Based Drawing \u00b6 Aspect SVG Markup Generation Script-Based Drawing Execution Model Rendered by browser Executed on server Security Risk Low (similar to HTML) Very High (RCE risk) Performance Lightweight for server Resource-intensive Scalability High Low Browser Support Native in all modern browsers Requires conversion to images Implementation Advantages \u00b6 Vector Graphics Benefits Infinite scaling without quality loss Smaller file sizes than raster images Accessibility features (screen readers can access text elements) Animation Capabilities Native animation via <animate> tags Interactive elements possible No need for video processing Integration with Web Technologies Direct CSS styling JavaScript interactivity when needed Compatible with all web frameworks Example Implementation \u00b6 async def generate_svg_visualization ( self , query ): \"\"\"Generate SVG markup based on user query\"\"\" # Prompt design for SVG generation prompt = f \"\"\" Generate SVG markup for a visualization showing { query } . Use only standard SVG elements (<svg>, <rect>, <circle>, <path>, etc.) Ensure the viewBox is appropriate and all elements are properly structured. Do not include any <script> elements or event handlers. \"\"\" # Get SVG from LLM svg_markup = await self . llm_client . generate_content ( prompt ) # Validate and sanitize sanitized_svg = self . sanitize_svg ( svg_markup ) # Return to client return { \"content_type\" : \"image/svg+xml\" , \"data\" : sanitized_svg } Related Research and Implementations \u00b6 Academic and Industry Efforts \u00b6 Recent research has explored the intersection of LLMs and vector graphics generation, notably: Chat2SVG (2023): Research project demonstrating direct SVG generation from natural language prompts using large language models SVGCode (Microsoft): Converting raster images to SVG using machine learning techniques Nougat (Meta): Academic paper rendering model focused on mathematical notation and diagrams Distinctive Aspects of Opossum's Implementation \u00b6 While building on these foundational concepts, our implementation is distinctive in several key ways: Search Integration Context-aware visualizations that incorporate search history and domain knowledge Visualization as an extension of information retrieval rather than standalone generation Hybrid Model Architecture Intelligent model selection based on query complexity and visualization requirements Fallback mechanisms that ensure generation resilience in production environments Multiple model support (Gemini, Ollama-based models, local transformers) Specialized Search Domain Optimizations Domain-specific templates and knowledge integration for our subject area Consistency with textual search results for a unified experience Conclusion \u00b6 SVG markup generation represents a secure, efficient approach for dynamic visualization in Opossum Search. By having LLMs generate SVG directly rather than executable code, we maintain strong security boundaries while enabling rich visual content. This approach aligns with web standards and modern best practices for content generation and rendering.","title":"SVG Markup"},{"location":"technical/svg-markup/#technical-documentation-svg-markup-generation","text":"","title":"Technical Documentation: SVG Markup Generation"},{"location":"technical/svg-markup/#overview-llm-driven-svg-generation","text":"The Opossum Search platform uses LLMs to generate SVG markup directly for creating dynamic visualizations: User provides a natural language request (e.g., \"Show me a pie chart of opossum diet\") LLM generates SVG markup (XML-based vector graphics) Server validates and returns the SVG markup Browser renders the SVG natively","title":"Overview: LLM-Driven SVG Generation"},{"location":"technical/svg-markup/#security-analysis","text":"","title":"Security Analysis"},{"location":"technical/svg-markup/#why-svg-markup-generation-is-secure","text":"Declarative vs. Procedural SVG is declarative markup , not executable code Describes what to display, not how to execute operations No ability to access system resources or perform operations Browser Rendering SVG is rendered by the client browser, not executed on the server Server only passes the validated markup Follows same security model as other web content (HTML/CSS) Validation Capabilities SVG can be validated against known schemas Potential for sanitizing any scripting elements (like embedded JavaScript) def sanitize_svg ( svg_markup ): # Remove any script tags or event handlers sanitized = re . sub ( r '<script.*?</script>' , '' , svg_markup , flags = re . DOTALL ) sanitized = re . sub ( r 'on\\w+=\".*?\"' , '' , sanitized ) return sanitized","title":"Why SVG Markup Generation is Secure"},{"location":"technical/svg-markup/#comparison-with-script-based-drawing","text":"Aspect SVG Markup Generation Script-Based Drawing Execution Model Rendered by browser Executed on server Security Risk Low (similar to HTML) Very High (RCE risk) Performance Lightweight for server Resource-intensive Scalability High Low Browser Support Native in all modern browsers Requires conversion to images","title":"Comparison with Script-Based Drawing"},{"location":"technical/svg-markup/#implementation-advantages","text":"Vector Graphics Benefits Infinite scaling without quality loss Smaller file sizes than raster images Accessibility features (screen readers can access text elements) Animation Capabilities Native animation via <animate> tags Interactive elements possible No need for video processing Integration with Web Technologies Direct CSS styling JavaScript interactivity when needed Compatible with all web frameworks","title":"Implementation Advantages"},{"location":"technical/svg-markup/#example-implementation","text":"async def generate_svg_visualization ( self , query ): \"\"\"Generate SVG markup based on user query\"\"\" # Prompt design for SVG generation prompt = f \"\"\" Generate SVG markup for a visualization showing { query } . Use only standard SVG elements (<svg>, <rect>, <circle>, <path>, etc.) Ensure the viewBox is appropriate and all elements are properly structured. Do not include any <script> elements or event handlers. \"\"\" # Get SVG from LLM svg_markup = await self . llm_client . generate_content ( prompt ) # Validate and sanitize sanitized_svg = self . sanitize_svg ( svg_markup ) # Return to client return { \"content_type\" : \"image/svg+xml\" , \"data\" : sanitized_svg }","title":"Example Implementation"},{"location":"technical/svg-markup/#related-research-and-implementations","text":"","title":"Related Research and Implementations"},{"location":"technical/svg-markup/#academic-and-industry-efforts","text":"Recent research has explored the intersection of LLMs and vector graphics generation, notably: Chat2SVG (2023): Research project demonstrating direct SVG generation from natural language prompts using large language models SVGCode (Microsoft): Converting raster images to SVG using machine learning techniques Nougat (Meta): Academic paper rendering model focused on mathematical notation and diagrams","title":"Academic and Industry Efforts"},{"location":"technical/svg-markup/#distinctive-aspects-of-opossums-implementation","text":"While building on these foundational concepts, our implementation is distinctive in several key ways: Search Integration Context-aware visualizations that incorporate search history and domain knowledge Visualization as an extension of information retrieval rather than standalone generation Hybrid Model Architecture Intelligent model selection based on query complexity and visualization requirements Fallback mechanisms that ensure generation resilience in production environments Multiple model support (Gemini, Ollama-based models, local transformers) Specialized Search Domain Optimizations Domain-specific templates and knowledge integration for our subject area Consistency with textual search results for a unified experience","title":"Distinctive Aspects of Opossum's Implementation"},{"location":"technical/svg-markup/#conclusion","text":"SVG markup generation represents a secure, efficient approach for dynamic visualization in Opossum Search. By having LLMs generate SVG directly rather than executable code, we maintain strong security boundaries while enabling rich visual content. This approach aligns with web standards and modern best practices for content generation and rendering.","title":"Conclusion"},{"location":"technical/system-architecture-overview/","text":"Diagrams and Visuals \u00b6 System Architecture Diagram \u00b6 try #mermaid-svg-FxL1E0DKPqjTQdKE{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-FxL1E0DKPqjTQdKE .error-icon{fill:#552222;}#mermaid-svg-FxL1E0DKPqjTQdKE .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-FxL1E0DKPqjTQdKE .marker{fill:#666;stroke:#666;}#mermaid-svg-FxL1E0DKPqjTQdKE .marker.cross{stroke:#666;}#mermaid-svg-FxL1E0DKPqjTQdKE svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-FxL1E0DKPqjTQdKE .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster-label text{fill:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster-label span{color:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE .label text,#mermaid-svg-FxL1E0DKPqjTQdKE span{fill:#000000;color:#000000;}#mermaid-svg-FxL1E0DKPqjTQdKE .node rect,#mermaid-svg-FxL1E0DKPqjTQdKE .node circle,#mermaid-svg-FxL1E0DKPqjTQdKE .node ellipse,#mermaid-svg-FxL1E0DKPqjTQdKE .node polygon,#mermaid-svg-FxL1E0DKPqjTQdKE .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-FxL1E0DKPqjTQdKE .node .label{text-align:center;}#mermaid-svg-FxL1E0DKPqjTQdKE .node.clickable{cursor:pointer;}#mermaid-svg-FxL1E0DKPqjTQdKE .arrowheadPath{fill:#333333;}#mermaid-svg-FxL1E0DKPqjTQdKE .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-FxL1E0DKPqjTQdKE .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-FxL1E0DKPqjTQdKE .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-FxL1E0DKPqjTQdKE .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster text{fill:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster span{color:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-FxL1E0DKPqjTQdKE:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-FxL1E0DKPqjTQdKE flowchart-v2{fill:apa;} Client Application Service Router Gemini API (External) Ollama (Local API) Transformers (Local Lib) Availability Monitoring System Service Monitoring Flowchart \u00b6 try #mermaid-svg-JPTzStuAU7T3u43Y{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-JPTzStuAU7T3u43Y .error-icon{fill:#552222;}#mermaid-svg-JPTzStuAU7T3u43Y .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-JPTzStuAU7T3u43Y .marker{fill:#666;stroke:#666;}#mermaid-svg-JPTzStuAU7T3u43Y .marker.cross{stroke:#666;}#mermaid-svg-JPTzStuAU7T3u43Y svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup text{fill:#999;stroke:none;font-size:10px;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup text{fill:#000000;stroke:none;font-size:10px;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup .state-title{font-weight:bolder;fill:black;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup rect{fill:#eee;stroke:#999;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup line{stroke:#666;stroke-width:1;}#mermaid-svg-JPTzStuAU7T3u43Y .transition{stroke:#666;stroke-width:1;fill:none;}#mermaid-svg-JPTzStuAU7T3u43Y .stateGroup .composit{fill:#ffffff;border-bottom:1px;}#mermaid-svg-JPTzStuAU7T3u43Y .stateGroup .alt-composit{fill:#e0e0e0;border-bottom:1px;}#mermaid-svg-JPTzStuAU7T3u43Y .state-note{stroke:hsl(60,100%,23.3333333333%);fill:#ffa;}#mermaid-svg-JPTzStuAU7T3u43Y .state-note text{fill:black;stroke:none;font-size:10px;}#mermaid-svg-JPTzStuAU7T3u43Y .stateLabel .box{stroke:none;stroke-width:0;fill:#eee;opacity:0.5;}#mermaid-svg-JPTzStuAU7T3u43Y .edgeLabel .label rect{fill:hsl(-160,0%,93.3333333333%);opacity:0.5;}#mermaid-svg-JPTzStuAU7T3u43Y .edgeLabel .label text{fill:rgb(17.0000000001,17.0000000001,17.0000000001);}#mermaid-svg-JPTzStuAU7T3u43Y .label div .edgeLabel{color:rgb(17.0000000001,17.0000000001,17.0000000001);}#mermaid-svg-JPTzStuAU7T3u43Y .stateLabel text{fill:black;font-size:10px;font-weight:bold;}#mermaid-svg-JPTzStuAU7T3u43Y .node circle.state-start{fill:#666;stroke:black;}#mermaid-svg-JPTzStuAU7T3u43Y .node circle.state-end{fill:hsl(0,0%,83.3333333333%);stroke:#ffffff;stroke-width:1.5;}#mermaid-svg-JPTzStuAU7T3u43Y .end-state-inner{fill:#ffffff;stroke-width:1.5;}#mermaid-svg-JPTzStuAU7T3u43Y .node rect{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-JPTzStuAU7T3u43Y #statediagram-barbEnd{fill:#666;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster rect{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-JPTzStuAU7T3u43Y .cluster-label,#mermaid-svg-JPTzStuAU7T3u43Y .nodeLabel{color:#000000;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster rect.outer{rx:5px;ry:5px;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state .divider{stroke:#999;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state .title-state{rx:5px;ry:5px;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster.statediagram-cluster .inner{fill:#ffffff;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster.statediagram-cluster-alt .inner{fill:#e0e0e0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster .inner{rx:0;ry:0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state rect.basic{rx:5px;ry:5px;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state rect.divider{stroke-dasharray:10,10;fill:hsl(210,66.6666666667%,95%);}#mermaid-svg-JPTzStuAU7T3u43Y .note-edge{stroke-dasharray:5;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note rect{fill:#ffa;stroke:hsl(60,100%,23.3333333333%);stroke-width:1px;rx:0;ry:0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note rect{fill:#ffa;stroke:hsl(60,100%,23.3333333333%);stroke-width:1px;rx:0;ry:0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note text{fill:#333;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note .nodeLabel{color:#333;}#mermaid-svg-JPTzStuAU7T3u43Y #dependencyStart,#mermaid-svg-JPTzStuAU7T3u43Y #dependencyEnd{fill:#666;stroke:#666;stroke-width:1;}#mermaid-svg-JPTzStuAU7T3u43Y:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-JPTzStuAU7T3u43Y stateDiagram{fill:apa;} Application Started Time Elapses Interval Reached Update Cache Evaluate Status Status Changed? If Changed Alerts Logged If Unchanged Application Ends Startup Idle CheckInterval CheckService CacheUpdated StatusChanged Logging Failover Process Diagram \u00b6 try #mermaid-svg-NGVEvpt2Atxyqkwy{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-NGVEvpt2Atxyqkwy .error-icon{fill:#552222;}#mermaid-svg-NGVEvpt2Atxyqkwy .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-NGVEvpt2Atxyqkwy .marker{fill:#666;stroke:#666;}#mermaid-svg-NGVEvpt2Atxyqkwy .marker.cross{stroke:#666;}#mermaid-svg-NGVEvpt2Atxyqkwy svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-NGVEvpt2Atxyqkwy .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster-label text{fill:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster-label span{color:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy .label text,#mermaid-svg-NGVEvpt2Atxyqkwy span{fill:#000000;color:#000000;}#mermaid-svg-NGVEvpt2Atxyqkwy .node rect,#mermaid-svg-NGVEvpt2Atxyqkwy .node circle,#mermaid-svg-NGVEvpt2Atxyqkwy .node ellipse,#mermaid-svg-NGVEvpt2Atxyqkwy .node polygon,#mermaid-svg-NGVEvpt2Atxyqkwy .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-NGVEvpt2Atxyqkwy .node .label{text-align:center;}#mermaid-svg-NGVEvpt2Atxyqkwy .node.clickable{cursor:pointer;}#mermaid-svg-NGVEvpt2Atxyqkwy .arrowheadPath{fill:#333333;}#mermaid-svg-NGVEvpt2Atxyqkwy .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-NGVEvpt2Atxyqkwy .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-NGVEvpt2Atxyqkwy .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-NGVEvpt2Atxyqkwy .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster text{fill:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster span{color:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-NGVEvpt2Atxyqkwy:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-NGVEvpt2Atxyqkwy flowchart-v2{fill:apa;} Yes No No No Yes Request Received Service Available? Service Rate-Limited? Try Next Service Process with Primary Service Any Service Available? Return Error Use Client Fallback Process with Available Service Rate Limit Monitoring Diagram \u00b6 try #mermaid-svg-SE6CYeVV0VzfhS6c{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-SE6CYeVV0VzfhS6c .error-icon{fill:#552222;}#mermaid-svg-SE6CYeVV0VzfhS6c .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-SE6CYeVV0VzfhS6c .marker{fill:#666;stroke:#666;}#mermaid-svg-SE6CYeVV0VzfhS6c .marker.cross{stroke:#666;}#mermaid-svg-SE6CYeVV0VzfhS6c svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-SE6CYeVV0VzfhS6c .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster-label text{fill:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster-label span{color:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c .label text,#mermaid-svg-SE6CYeVV0VzfhS6c span{fill:#000000;color:#000000;}#mermaid-svg-SE6CYeVV0VzfhS6c .node rect,#mermaid-svg-SE6CYeVV0VzfhS6c .node circle,#mermaid-svg-SE6CYeVV0VzfhS6c .node ellipse,#mermaid-svg-SE6CYeVV0VzfhS6c .node polygon,#mermaid-svg-SE6CYeVV0VzfhS6c .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-SE6CYeVV0VzfhS6c .node .label{text-align:center;}#mermaid-svg-SE6CYeVV0VzfhS6c .node.clickable{cursor:pointer;}#mermaid-svg-SE6CYeVV0VzfhS6c .arrowheadPath{fill:#333333;}#mermaid-svg-SE6CYeVV0VzfhS6c .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-SE6CYeVV0VzfhS6c .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-SE6CYeVV0VzfhS6c .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-SE6CYeVV0VzfhS6c .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster text{fill:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster span{color:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-SE6CYeVV0VzfhS6c:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-SE6CYeVV0VzfhS6c flowchart-v2{fill:apa;} Yes Yes No Track API Request Reset Period Elapsed? Reset Counters Increment Counters Approaching Limit? Log Warning Prepare Fallback Continue Normal Operation Recovery Detection Process \u00b6 try #mermaid-svg-ZWE087sJgkjkL2K1{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-ZWE087sJgkjkL2K1 .error-icon{fill:#552222;}#mermaid-svg-ZWE087sJgkjkL2K1 .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-ZWE087sJgkjkL2K1 .marker{fill:#666;stroke:#666;}#mermaid-svg-ZWE087sJgkjkL2K1 .marker.cross{stroke:#666;}#mermaid-svg-ZWE087sJgkjkL2K1 svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-ZWE087sJgkjkL2K1 .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster-label text{fill:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster-label span{color:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 .label text,#mermaid-svg-ZWE087sJgkjkL2K1 span{fill:#000000;color:#000000;}#mermaid-svg-ZWE087sJgkjkL2K1 .node rect,#mermaid-svg-ZWE087sJgkjkL2K1 .node circle,#mermaid-svg-ZWE087sJgkjkL2K1 .node ellipse,#mermaid-svg-ZWE087sJgkjkL2K1 .node polygon,#mermaid-svg-ZWE087sJgkjkL2K1 .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-ZWE087sJgkjkL2K1 .node .label{text-align:center;}#mermaid-svg-ZWE087sJgkjkL2K1 .node.clickable{cursor:pointer;}#mermaid-svg-ZWE087sJgkjkL2K1 .arrowheadPath{fill:#333333;}#mermaid-svg-ZWE087sJgkjkL2K1 .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-ZWE087sJgkjkL2K1 .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-ZWE087sJgkjkL2K1 .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-ZWE087sJgkjkL2K1 .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster text{fill:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster span{color:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-ZWE087sJgkjkL2K1:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-ZWE087sJgkjkL2K1 flowchart-v2{fill:apa;} No Yes Service Unavailable Periodic Health Check Service Recovered? Continue Using Fallback Log Recovery Update Status Resume Using Preferred Service User Experience Flow \u00b6 try #mermaid-svg-VNG44VsYtDnaxTfG{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-VNG44VsYtDnaxTfG .error-icon{fill:#552222;}#mermaid-svg-VNG44VsYtDnaxTfG .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-VNG44VsYtDnaxTfG .marker{fill:#666;stroke:#666;}#mermaid-svg-VNG44VsYtDnaxTfG .marker.cross{stroke:#666;}#mermaid-svg-VNG44VsYtDnaxTfG svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-VNG44VsYtDnaxTfG .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster-label text{fill:#333;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster-label span{color:#333;}#mermaid-svg-VNG44VsYtDnaxTfG .label text,#mermaid-svg-VNG44VsYtDnaxTfG span{fill:#000000;color:#000000;}#mermaid-svg-VNG44VsYtDnaxTfG .node rect,#mermaid-svg-VNG44VsYtDnaxTfG .node circle,#mermaid-svg-VNG44VsYtDnaxTfG .node ellipse,#mermaid-svg-VNG44VsYtDnaxTfG .node polygon,#mermaid-svg-VNG44VsYtDnaxTfG .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-VNG44VsYtDnaxTfG .node .label{text-align:center;}#mermaid-svg-VNG44VsYtDnaxTfG .node.clickable{cursor:pointer;}#mermaid-svg-VNG44VsYtDnaxTfG .arrowheadPath{fill:#333333;}#mermaid-svg-VNG44VsYtDnaxTfG .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-VNG44VsYtDnaxTfG .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-VNG44VsYtDnaxTfG .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-VNG44VsYtDnaxTfG .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster text{fill:#333;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster span{color:#333;}#mermaid-svg-VNG44VsYtDnaxTfG div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-VNG44VsYtDnaxTfG:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-VNG44VsYtDnaxTfG flowchart-v2{fill:apa;} No Yes Yes No User Submits Query Frontend Processes Input API Request to Server Server Available? Client-side Fallback Activated Show Error Message with Limited Mode Service Using Fallback? Notify User of Service Limitations Normal Response Processing","title":"System Overview"},{"location":"technical/system-architecture-overview/#diagrams-and-visuals","text":"","title":"Diagrams and Visuals"},{"location":"technical/system-architecture-overview/#system-architecture-diagram","text":"try #mermaid-svg-FxL1E0DKPqjTQdKE{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-FxL1E0DKPqjTQdKE .error-icon{fill:#552222;}#mermaid-svg-FxL1E0DKPqjTQdKE .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-FxL1E0DKPqjTQdKE .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-FxL1E0DKPqjTQdKE .marker{fill:#666;stroke:#666;}#mermaid-svg-FxL1E0DKPqjTQdKE .marker.cross{stroke:#666;}#mermaid-svg-FxL1E0DKPqjTQdKE svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-FxL1E0DKPqjTQdKE .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster-label text{fill:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster-label span{color:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE .label text,#mermaid-svg-FxL1E0DKPqjTQdKE span{fill:#000000;color:#000000;}#mermaid-svg-FxL1E0DKPqjTQdKE .node rect,#mermaid-svg-FxL1E0DKPqjTQdKE .node circle,#mermaid-svg-FxL1E0DKPqjTQdKE .node ellipse,#mermaid-svg-FxL1E0DKPqjTQdKE .node polygon,#mermaid-svg-FxL1E0DKPqjTQdKE .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-FxL1E0DKPqjTQdKE .node .label{text-align:center;}#mermaid-svg-FxL1E0DKPqjTQdKE .node.clickable{cursor:pointer;}#mermaid-svg-FxL1E0DKPqjTQdKE .arrowheadPath{fill:#333333;}#mermaid-svg-FxL1E0DKPqjTQdKE .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-FxL1E0DKPqjTQdKE .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-FxL1E0DKPqjTQdKE .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-FxL1E0DKPqjTQdKE .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster text{fill:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE .cluster span{color:#333;}#mermaid-svg-FxL1E0DKPqjTQdKE div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-FxL1E0DKPqjTQdKE:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-FxL1E0DKPqjTQdKE flowchart-v2{fill:apa;} Client Application Service Router Gemini API (External) Ollama (Local API) Transformers (Local Lib) Availability Monitoring System","title":"System Architecture Diagram"},{"location":"technical/system-architecture-overview/#service-monitoring-flowchart","text":"try #mermaid-svg-JPTzStuAU7T3u43Y{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-JPTzStuAU7T3u43Y .error-icon{fill:#552222;}#mermaid-svg-JPTzStuAU7T3u43Y .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-JPTzStuAU7T3u43Y .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-JPTzStuAU7T3u43Y .marker{fill:#666;stroke:#666;}#mermaid-svg-JPTzStuAU7T3u43Y .marker.cross{stroke:#666;}#mermaid-svg-JPTzStuAU7T3u43Y svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup text{fill:#999;stroke:none;font-size:10px;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup text{fill:#000000;stroke:none;font-size:10px;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup .state-title{font-weight:bolder;fill:black;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup rect{fill:#eee;stroke:#999;}#mermaid-svg-JPTzStuAU7T3u43Y g.stateGroup line{stroke:#666;stroke-width:1;}#mermaid-svg-JPTzStuAU7T3u43Y .transition{stroke:#666;stroke-width:1;fill:none;}#mermaid-svg-JPTzStuAU7T3u43Y .stateGroup .composit{fill:#ffffff;border-bottom:1px;}#mermaid-svg-JPTzStuAU7T3u43Y .stateGroup .alt-composit{fill:#e0e0e0;border-bottom:1px;}#mermaid-svg-JPTzStuAU7T3u43Y .state-note{stroke:hsl(60,100%,23.3333333333%);fill:#ffa;}#mermaid-svg-JPTzStuAU7T3u43Y .state-note text{fill:black;stroke:none;font-size:10px;}#mermaid-svg-JPTzStuAU7T3u43Y .stateLabel .box{stroke:none;stroke-width:0;fill:#eee;opacity:0.5;}#mermaid-svg-JPTzStuAU7T3u43Y .edgeLabel .label rect{fill:hsl(-160,0%,93.3333333333%);opacity:0.5;}#mermaid-svg-JPTzStuAU7T3u43Y .edgeLabel .label text{fill:rgb(17.0000000001,17.0000000001,17.0000000001);}#mermaid-svg-JPTzStuAU7T3u43Y .label div .edgeLabel{color:rgb(17.0000000001,17.0000000001,17.0000000001);}#mermaid-svg-JPTzStuAU7T3u43Y .stateLabel text{fill:black;font-size:10px;font-weight:bold;}#mermaid-svg-JPTzStuAU7T3u43Y .node circle.state-start{fill:#666;stroke:black;}#mermaid-svg-JPTzStuAU7T3u43Y .node circle.state-end{fill:hsl(0,0%,83.3333333333%);stroke:#ffffff;stroke-width:1.5;}#mermaid-svg-JPTzStuAU7T3u43Y .end-state-inner{fill:#ffffff;stroke-width:1.5;}#mermaid-svg-JPTzStuAU7T3u43Y .node rect{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-JPTzStuAU7T3u43Y #statediagram-barbEnd{fill:#666;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster rect{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-JPTzStuAU7T3u43Y .cluster-label,#mermaid-svg-JPTzStuAU7T3u43Y .nodeLabel{color:#000000;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster rect.outer{rx:5px;ry:5px;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state .divider{stroke:#999;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state .title-state{rx:5px;ry:5px;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster.statediagram-cluster .inner{fill:#ffffff;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster.statediagram-cluster-alt .inner{fill:#e0e0e0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-cluster .inner{rx:0;ry:0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state rect.basic{rx:5px;ry:5px;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-state rect.divider{stroke-dasharray:10,10;fill:hsl(210,66.6666666667%,95%);}#mermaid-svg-JPTzStuAU7T3u43Y .note-edge{stroke-dasharray:5;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note rect{fill:#ffa;stroke:hsl(60,100%,23.3333333333%);stroke-width:1px;rx:0;ry:0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note rect{fill:#ffa;stroke:hsl(60,100%,23.3333333333%);stroke-width:1px;rx:0;ry:0;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note text{fill:#333;}#mermaid-svg-JPTzStuAU7T3u43Y .statediagram-note .nodeLabel{color:#333;}#mermaid-svg-JPTzStuAU7T3u43Y #dependencyStart,#mermaid-svg-JPTzStuAU7T3u43Y #dependencyEnd{fill:#666;stroke:#666;stroke-width:1;}#mermaid-svg-JPTzStuAU7T3u43Y:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-JPTzStuAU7T3u43Y stateDiagram{fill:apa;} Application Started Time Elapses Interval Reached Update Cache Evaluate Status Status Changed? If Changed Alerts Logged If Unchanged Application Ends Startup Idle CheckInterval CheckService CacheUpdated StatusChanged Logging","title":"Service Monitoring Flowchart"},{"location":"technical/system-architecture-overview/#failover-process-diagram","text":"try #mermaid-svg-NGVEvpt2Atxyqkwy{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-NGVEvpt2Atxyqkwy .error-icon{fill:#552222;}#mermaid-svg-NGVEvpt2Atxyqkwy .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-NGVEvpt2Atxyqkwy .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-NGVEvpt2Atxyqkwy .marker{fill:#666;stroke:#666;}#mermaid-svg-NGVEvpt2Atxyqkwy .marker.cross{stroke:#666;}#mermaid-svg-NGVEvpt2Atxyqkwy svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-NGVEvpt2Atxyqkwy .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster-label text{fill:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster-label span{color:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy .label text,#mermaid-svg-NGVEvpt2Atxyqkwy span{fill:#000000;color:#000000;}#mermaid-svg-NGVEvpt2Atxyqkwy .node rect,#mermaid-svg-NGVEvpt2Atxyqkwy .node circle,#mermaid-svg-NGVEvpt2Atxyqkwy .node ellipse,#mermaid-svg-NGVEvpt2Atxyqkwy .node polygon,#mermaid-svg-NGVEvpt2Atxyqkwy .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-NGVEvpt2Atxyqkwy .node .label{text-align:center;}#mermaid-svg-NGVEvpt2Atxyqkwy .node.clickable{cursor:pointer;}#mermaid-svg-NGVEvpt2Atxyqkwy .arrowheadPath{fill:#333333;}#mermaid-svg-NGVEvpt2Atxyqkwy .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-NGVEvpt2Atxyqkwy .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-NGVEvpt2Atxyqkwy .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-NGVEvpt2Atxyqkwy .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster text{fill:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy .cluster span{color:#333;}#mermaid-svg-NGVEvpt2Atxyqkwy div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-NGVEvpt2Atxyqkwy:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-NGVEvpt2Atxyqkwy flowchart-v2{fill:apa;} Yes No No No Yes Request Received Service Available? Service Rate-Limited? Try Next Service Process with Primary Service Any Service Available? Return Error Use Client Fallback Process with Available Service","title":"Failover Process Diagram"},{"location":"technical/system-architecture-overview/#rate-limit-monitoring-diagram","text":"try #mermaid-svg-SE6CYeVV0VzfhS6c{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-SE6CYeVV0VzfhS6c .error-icon{fill:#552222;}#mermaid-svg-SE6CYeVV0VzfhS6c .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-SE6CYeVV0VzfhS6c .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-SE6CYeVV0VzfhS6c .marker{fill:#666;stroke:#666;}#mermaid-svg-SE6CYeVV0VzfhS6c .marker.cross{stroke:#666;}#mermaid-svg-SE6CYeVV0VzfhS6c svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-SE6CYeVV0VzfhS6c .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster-label text{fill:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster-label span{color:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c .label text,#mermaid-svg-SE6CYeVV0VzfhS6c span{fill:#000000;color:#000000;}#mermaid-svg-SE6CYeVV0VzfhS6c .node rect,#mermaid-svg-SE6CYeVV0VzfhS6c .node circle,#mermaid-svg-SE6CYeVV0VzfhS6c .node ellipse,#mermaid-svg-SE6CYeVV0VzfhS6c .node polygon,#mermaid-svg-SE6CYeVV0VzfhS6c .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-SE6CYeVV0VzfhS6c .node .label{text-align:center;}#mermaid-svg-SE6CYeVV0VzfhS6c .node.clickable{cursor:pointer;}#mermaid-svg-SE6CYeVV0VzfhS6c .arrowheadPath{fill:#333333;}#mermaid-svg-SE6CYeVV0VzfhS6c .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-SE6CYeVV0VzfhS6c .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-SE6CYeVV0VzfhS6c .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-SE6CYeVV0VzfhS6c .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster text{fill:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c .cluster span{color:#333;}#mermaid-svg-SE6CYeVV0VzfhS6c div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-SE6CYeVV0VzfhS6c:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-SE6CYeVV0VzfhS6c flowchart-v2{fill:apa;} Yes Yes No Track API Request Reset Period Elapsed? Reset Counters Increment Counters Approaching Limit? Log Warning Prepare Fallback Continue Normal Operation","title":"Rate Limit Monitoring Diagram"},{"location":"technical/system-architecture-overview/#recovery-detection-process","text":"try #mermaid-svg-ZWE087sJgkjkL2K1{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-ZWE087sJgkjkL2K1 .error-icon{fill:#552222;}#mermaid-svg-ZWE087sJgkjkL2K1 .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-ZWE087sJgkjkL2K1 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-ZWE087sJgkjkL2K1 .marker{fill:#666;stroke:#666;}#mermaid-svg-ZWE087sJgkjkL2K1 .marker.cross{stroke:#666;}#mermaid-svg-ZWE087sJgkjkL2K1 svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-ZWE087sJgkjkL2K1 .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster-label text{fill:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster-label span{color:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 .label text,#mermaid-svg-ZWE087sJgkjkL2K1 span{fill:#000000;color:#000000;}#mermaid-svg-ZWE087sJgkjkL2K1 .node rect,#mermaid-svg-ZWE087sJgkjkL2K1 .node circle,#mermaid-svg-ZWE087sJgkjkL2K1 .node ellipse,#mermaid-svg-ZWE087sJgkjkL2K1 .node polygon,#mermaid-svg-ZWE087sJgkjkL2K1 .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-ZWE087sJgkjkL2K1 .node .label{text-align:center;}#mermaid-svg-ZWE087sJgkjkL2K1 .node.clickable{cursor:pointer;}#mermaid-svg-ZWE087sJgkjkL2K1 .arrowheadPath{fill:#333333;}#mermaid-svg-ZWE087sJgkjkL2K1 .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-ZWE087sJgkjkL2K1 .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-ZWE087sJgkjkL2K1 .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-ZWE087sJgkjkL2K1 .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster text{fill:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 .cluster span{color:#333;}#mermaid-svg-ZWE087sJgkjkL2K1 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-ZWE087sJgkjkL2K1:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-ZWE087sJgkjkL2K1 flowchart-v2{fill:apa;} No Yes Service Unavailable Periodic Health Check Service Recovered? Continue Using Fallback Log Recovery Update Status Resume Using Preferred Service","title":"Recovery Detection Process"},{"location":"technical/system-architecture-overview/#user-experience-flow","text":"try #mermaid-svg-VNG44VsYtDnaxTfG{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#000000;}#mermaid-svg-VNG44VsYtDnaxTfG .error-icon{fill:#552222;}#mermaid-svg-VNG44VsYtDnaxTfG .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-VNG44VsYtDnaxTfG .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-VNG44VsYtDnaxTfG .marker{fill:#666;stroke:#666;}#mermaid-svg-VNG44VsYtDnaxTfG .marker.cross{stroke:#666;}#mermaid-svg-VNG44VsYtDnaxTfG svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-VNG44VsYtDnaxTfG .label{font-family:\"trebuchet ms\",verdana,arial,sans-serif;color:#000000;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster-label text{fill:#333;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster-label span{color:#333;}#mermaid-svg-VNG44VsYtDnaxTfG .label text,#mermaid-svg-VNG44VsYtDnaxTfG span{fill:#000000;color:#000000;}#mermaid-svg-VNG44VsYtDnaxTfG .node rect,#mermaid-svg-VNG44VsYtDnaxTfG .node circle,#mermaid-svg-VNG44VsYtDnaxTfG .node ellipse,#mermaid-svg-VNG44VsYtDnaxTfG .node polygon,#mermaid-svg-VNG44VsYtDnaxTfG .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-svg-VNG44VsYtDnaxTfG .node .label{text-align:center;}#mermaid-svg-VNG44VsYtDnaxTfG .node.clickable{cursor:pointer;}#mermaid-svg-VNG44VsYtDnaxTfG .arrowheadPath{fill:#333333;}#mermaid-svg-VNG44VsYtDnaxTfG .edgePath .path{stroke:#666;stroke-width:1.5px;}#mermaid-svg-VNG44VsYtDnaxTfG .flowchart-link{stroke:#666;fill:none;}#mermaid-svg-VNG44VsYtDnaxTfG .edgeLabel{background-color:white;text-align:center;}#mermaid-svg-VNG44VsYtDnaxTfG .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster rect{fill:hsl(210,66.6666666667%,95%);stroke:#26a;stroke-width:1px;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster text{fill:#333;}#mermaid-svg-VNG44VsYtDnaxTfG .cluster span{color:#333;}#mermaid-svg-VNG44VsYtDnaxTfG div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:12px;background:hsl(-160,0%,93.3333333333%);border:1px solid #26a;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-VNG44VsYtDnaxTfG:root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}#mermaid-svg-VNG44VsYtDnaxTfG flowchart-v2{fill:apa;} No Yes Yes No User Submits Query Frontend Processes Input API Request to Server Server Available? Client-side Fallback Activated Show Error Message with Limited Mode Service Using Fallback? Notify User of Service Limitations Normal Response Processing","title":"User Experience Flow"},{"location":"tutorials/dspy/advanced-reasoning/","text":"Advanced Reasoning with DSPy \u00b6 Having covered the basics of DSPy modules in Getting Started and how to optimize them in Optimizing Prompts , we now turn to more complex reasoning tasks. DSPy provides powerful modules that enable language models (LMs) to perform multi-step reasoning, use tools, and tackle problems that require more than a simple input-output transformation. Within Opossum Search, these advanced techniques can enhance the system's ability to answer complex queries, compare concepts, synthesize information, and potentially interact with other internal tools or knowledge bases. Why Advanced Reasoning in Opossum? \u00b6 Deeper Understanding: Answer \"why\" and \"how\" questions that require step-by-step thinking, aligning with Opossum's goal of providing accurate, detailed information. Comparative Analysis: Enable comparisons between concepts (e.g., \"Compare opossums to raccoons,\" a query type mentioned in # docs\\technical\\bot-user-simulation.md ). Tool Use & Grounding: Allow the LM to potentially use internal Opossum tools (like specific knowledge retrieval functions, or even aspects of the Capability Matrix ) to ground its responses or perform calculations. This relates to future goals like RAG (# docs\\about\\roadmap.md ). Complex Task Decomposition: Break down complex user requests into manageable sub-problems that the LM can solve sequentially. Key DSPy Modules for Reasoning \u00b6 dspy.ChainOfThought(Signature) : Concept: Explicitly prompts the LM to \"think step-by-step\" to reach the final answer. It adds a rationale field to the signature internally. Use Case: Useful for arithmetic, symbolic reasoning, or any task where breaking down the problem helps improve accuracy. Opossum could use this for explaining complex biological facts or comparing different aspects of opossum behavior. dspy.ReAct(Signature, tools=[Tool]) : Concept: Implements the ReAct (Reasoning + Acting) framework. The LM can iteratively use provided tools (which are just Python functions with descriptions) to gather information or perform actions, reasoning about the next step based on tool outputs. Use Case: Powerful for questions requiring external knowledge lookup (beyond the LM's training data) or calculations. In Opossum, tools could potentially wrap: A specific database lookup for opossum facts (potentially using data like opossum_dataset_converted.json ). A function to check the Service Availability status. A calculator for simple math mentioned in queries. Even a simplified interface to the Backend Selection logic to explain why a certain model might be chosen. dspy.MultiChainComparison(Signature, num_comparisons=N) : Concept: Generates multiple ChainOfThought responses ( N times) and then synthesizes them into a final answer. Use Case: Improves robustness and explores different reasoning paths for complex questions where a single chain might be insufficient or prone to errors. Example: Chain of Thought for Comparison \u00b6 Let's imagine using ChainOfThought to handle a comparative query, drawing on general knowledge. import dspy # Assume dspy.settings.configure(lm=...) has been called class CompareAnimals ( dspy . Signature ): \"\"\"Compare two animals based on specified criteria.\"\"\" animal_1 = dspy . InputField ( desc = \"The first animal.\" ) animal_2 = dspy . InputField ( desc = \"The second animal.\" ) criteria = dspy . InputField ( desc = \"Aspects to compare (e.g., diet, habitat, lifespan).\" ) comparison = dspy . OutputField ( desc = \"A detailed comparison addressing the criteria.\" ) # Use ChainOfThought with the signature compare_step_by_step = dspy . ChainOfThought ( CompareAnimals ) # Run the module animal1 = \"Opossum\" animal2 = \"Raccoon\" comparison_criteria = \"diet and typical lifespan\" result = compare_step_by_step ( animal_1 = animal1 , animal_2 = animal2 , criteria = comparison_criteria ) print ( f \"Comparing { animal1 } and { animal2 } based on { comparison_criteria } : \\n \" ) # The rationale is generated internally by ChainOfThought but not typically part of the final output field # To see it, you might inspect the prompt history: dspy.settings.lm.inspect_history(n=1) print ( f \"Comparison Result: \\n { result . comparison } \" ) Example: ReAct for Tool Use \u00b6 Imagine we want the LM to answer questions using specific facts that might be stored in our opossum_dataset_converted.json or a similar knowledge source. We can create a hypothetical DSPy Tool that allows the ReAct agent to query this data. import dspy # --- Hypothetical Tool Definition --- class OpossumFactTool ( dspy . Tool ): name = \"opossum_fact_lookup\" input_variable = \"query\" output_variable = \"fact\" description = \"Looks up specific facts about opossums (e.g., 'gestation period', 'number of teeth') from the Opossum knowledge base.\" def __call__ ( self , query : str ) -> str : query = query . lower () # Example lookup logic (replace with actual JSON parsing/search) if \"gestation period\" in query : # Hypothetical lookup result return \"The gestation period for the Virginia opossum is very short, typically 12-13 days.\" elif \"number of teeth\" in query : # Hypothetical lookup result return \"Opossums have 50 teeth, more than any other North American mammal.\" else : # Fallback if fact not found in our data return f \"Fact not found in Opossum knowledge base for ' { query } '.\" # --- Signature for ReAct --- class QuestionWithFacts ( dspy . Signature ): \"\"\"Answer questions, potentially using external tools to find specific facts.\"\"\" question = dspy . InputField () answer = dspy . OutputField ( desc = \"A comprehensive answer incorporating retrieved facts if necessary.\" ) # --- Instantiate ReAct Module --- # Provide the tool instance to the ReAct module agent = dspy . ReAct ( QuestionWithFacts , tools = [ OpossumFactTool ()]) # --- Run the Agent --- user_question = \"How many teeth does an opossum have, and how long is its gestation period?\" result = agent ( question = user_question ) print ( f \"Question: { user_question } \" ) print ( f \"Answer: { result . answer } \" ) # Inspect history to see the Thought/Action/Observation steps # dspy.settings.lm.inspect_history(n=1) Combining and Optimizing Reasoning Modules \u00b6 Pipelines: You can chain these advanced modules together just like simpler ones (see Building Pipelines ). For example, the output of a ChainOfThought module could feed into a Predict module for summarization. Optimization: Teleprompters like BootstrapFewShot can optimize reasoning modules too. The training data would need to include examples demonstrating the desired reasoning process (potentially including intermediate steps or tool usage if optimizing ReAct ). Metrics would evaluate the final output's correctness and potentially the quality of the reasoning steps. Integration in Opossum Search \u00b6 Complex Query Handling: Use ChainOfThought or ReAct when the Request Analyzer detects complex reasoning or comparative queries (# docs\\model-integration\\backend-selection.md ). Fact Grounding: Implement tools for ReAct that access Opossum's curated knowledge (like opossum_dataset_converted.json ) or even external APIs, ensuring responses are grounded in reliable data. Explanation Generation: Use ChainOfThought to generate explanations for why Opossum provided a certain answer or took a specific action (e.g., explaining fallback logic, # docs\\technical\\resilience-patterns.md ). Structured Data Extraction: While not strictly reasoning, DSPy modules can be prompted to extract structured information (e.g., parameters for SVG Generation ) as part of a reasoning process. Conclusion \u00b6 DSPy's advanced reasoning modules like ChainOfThought and ReAct provide powerful ways to build more sophisticated and capable LM applications. By incorporating these techniques, Opossum Search can move beyond simple Q&A to handle more complex user needs, provide deeper insights, and potentially leverage its internal knowledge and tools more effectively. Remember that these modules can also be optimized using DSPy's teleprompters, ensuring both capability and efficiency.","title":"Advanced Reasoning"},{"location":"tutorials/dspy/advanced-reasoning/#advanced-reasoning-with-dspy","text":"Having covered the basics of DSPy modules in Getting Started and how to optimize them in Optimizing Prompts , we now turn to more complex reasoning tasks. DSPy provides powerful modules that enable language models (LMs) to perform multi-step reasoning, use tools, and tackle problems that require more than a simple input-output transformation. Within Opossum Search, these advanced techniques can enhance the system's ability to answer complex queries, compare concepts, synthesize information, and potentially interact with other internal tools or knowledge bases.","title":"Advanced Reasoning with DSPy"},{"location":"tutorials/dspy/advanced-reasoning/#why-advanced-reasoning-in-opossum","text":"Deeper Understanding: Answer \"why\" and \"how\" questions that require step-by-step thinking, aligning with Opossum's goal of providing accurate, detailed information. Comparative Analysis: Enable comparisons between concepts (e.g., \"Compare opossums to raccoons,\" a query type mentioned in # docs\\technical\\bot-user-simulation.md ). Tool Use & Grounding: Allow the LM to potentially use internal Opossum tools (like specific knowledge retrieval functions, or even aspects of the Capability Matrix ) to ground its responses or perform calculations. This relates to future goals like RAG (# docs\\about\\roadmap.md ). Complex Task Decomposition: Break down complex user requests into manageable sub-problems that the LM can solve sequentially.","title":"Why Advanced Reasoning in Opossum?"},{"location":"tutorials/dspy/advanced-reasoning/#key-dspy-modules-for-reasoning","text":"dspy.ChainOfThought(Signature) : Concept: Explicitly prompts the LM to \"think step-by-step\" to reach the final answer. It adds a rationale field to the signature internally. Use Case: Useful for arithmetic, symbolic reasoning, or any task where breaking down the problem helps improve accuracy. Opossum could use this for explaining complex biological facts or comparing different aspects of opossum behavior. dspy.ReAct(Signature, tools=[Tool]) : Concept: Implements the ReAct (Reasoning + Acting) framework. The LM can iteratively use provided tools (which are just Python functions with descriptions) to gather information or perform actions, reasoning about the next step based on tool outputs. Use Case: Powerful for questions requiring external knowledge lookup (beyond the LM's training data) or calculations. In Opossum, tools could potentially wrap: A specific database lookup for opossum facts (potentially using data like opossum_dataset_converted.json ). A function to check the Service Availability status. A calculator for simple math mentioned in queries. Even a simplified interface to the Backend Selection logic to explain why a certain model might be chosen. dspy.MultiChainComparison(Signature, num_comparisons=N) : Concept: Generates multiple ChainOfThought responses ( N times) and then synthesizes them into a final answer. Use Case: Improves robustness and explores different reasoning paths for complex questions where a single chain might be insufficient or prone to errors.","title":"Key DSPy Modules for Reasoning"},{"location":"tutorials/dspy/advanced-reasoning/#example-chain-of-thought-for-comparison","text":"Let's imagine using ChainOfThought to handle a comparative query, drawing on general knowledge. import dspy # Assume dspy.settings.configure(lm=...) has been called class CompareAnimals ( dspy . Signature ): \"\"\"Compare two animals based on specified criteria.\"\"\" animal_1 = dspy . InputField ( desc = \"The first animal.\" ) animal_2 = dspy . InputField ( desc = \"The second animal.\" ) criteria = dspy . InputField ( desc = \"Aspects to compare (e.g., diet, habitat, lifespan).\" ) comparison = dspy . OutputField ( desc = \"A detailed comparison addressing the criteria.\" ) # Use ChainOfThought with the signature compare_step_by_step = dspy . ChainOfThought ( CompareAnimals ) # Run the module animal1 = \"Opossum\" animal2 = \"Raccoon\" comparison_criteria = \"diet and typical lifespan\" result = compare_step_by_step ( animal_1 = animal1 , animal_2 = animal2 , criteria = comparison_criteria ) print ( f \"Comparing { animal1 } and { animal2 } based on { comparison_criteria } : \\n \" ) # The rationale is generated internally by ChainOfThought but not typically part of the final output field # To see it, you might inspect the prompt history: dspy.settings.lm.inspect_history(n=1) print ( f \"Comparison Result: \\n { result . comparison } \" )","title":"Example: Chain of Thought for Comparison"},{"location":"tutorials/dspy/advanced-reasoning/#example-react-for-tool-use","text":"Imagine we want the LM to answer questions using specific facts that might be stored in our opossum_dataset_converted.json or a similar knowledge source. We can create a hypothetical DSPy Tool that allows the ReAct agent to query this data. import dspy # --- Hypothetical Tool Definition --- class OpossumFactTool ( dspy . Tool ): name = \"opossum_fact_lookup\" input_variable = \"query\" output_variable = \"fact\" description = \"Looks up specific facts about opossums (e.g., 'gestation period', 'number of teeth') from the Opossum knowledge base.\" def __call__ ( self , query : str ) -> str : query = query . lower () # Example lookup logic (replace with actual JSON parsing/search) if \"gestation period\" in query : # Hypothetical lookup result return \"The gestation period for the Virginia opossum is very short, typically 12-13 days.\" elif \"number of teeth\" in query : # Hypothetical lookup result return \"Opossums have 50 teeth, more than any other North American mammal.\" else : # Fallback if fact not found in our data return f \"Fact not found in Opossum knowledge base for ' { query } '.\" # --- Signature for ReAct --- class QuestionWithFacts ( dspy . Signature ): \"\"\"Answer questions, potentially using external tools to find specific facts.\"\"\" question = dspy . InputField () answer = dspy . OutputField ( desc = \"A comprehensive answer incorporating retrieved facts if necessary.\" ) # --- Instantiate ReAct Module --- # Provide the tool instance to the ReAct module agent = dspy . ReAct ( QuestionWithFacts , tools = [ OpossumFactTool ()]) # --- Run the Agent --- user_question = \"How many teeth does an opossum have, and how long is its gestation period?\" result = agent ( question = user_question ) print ( f \"Question: { user_question } \" ) print ( f \"Answer: { result . answer } \" ) # Inspect history to see the Thought/Action/Observation steps # dspy.settings.lm.inspect_history(n=1)","title":"Example: ReAct for Tool Use"},{"location":"tutorials/dspy/advanced-reasoning/#combining-and-optimizing-reasoning-modules","text":"Pipelines: You can chain these advanced modules together just like simpler ones (see Building Pipelines ). For example, the output of a ChainOfThought module could feed into a Predict module for summarization. Optimization: Teleprompters like BootstrapFewShot can optimize reasoning modules too. The training data would need to include examples demonstrating the desired reasoning process (potentially including intermediate steps or tool usage if optimizing ReAct ). Metrics would evaluate the final output's correctness and potentially the quality of the reasoning steps.","title":"Combining and Optimizing Reasoning Modules"},{"location":"tutorials/dspy/advanced-reasoning/#integration-in-opossum-search","text":"Complex Query Handling: Use ChainOfThought or ReAct when the Request Analyzer detects complex reasoning or comparative queries (# docs\\model-integration\\backend-selection.md ). Fact Grounding: Implement tools for ReAct that access Opossum's curated knowledge (like opossum_dataset_converted.json ) or even external APIs, ensuring responses are grounded in reliable data. Explanation Generation: Use ChainOfThought to generate explanations for why Opossum provided a certain answer or took a specific action (e.g., explaining fallback logic, # docs\\technical\\resilience-patterns.md ). Structured Data Extraction: While not strictly reasoning, DSPy modules can be prompted to extract structured information (e.g., parameters for SVG Generation ) as part of a reasoning process.","title":"Integration in Opossum Search"},{"location":"tutorials/dspy/advanced-reasoning/#conclusion","text":"DSPy's advanced reasoning modules like ChainOfThought and ReAct provide powerful ways to build more sophisticated and capable LM applications. By incorporating these techniques, Opossum Search can move beyond simple Q&A to handle more complex user needs, provide deeper insights, and potentially leverage its internal knowledge and tools more effectively. Remember that these modules can also be optimized using DSPy's teleprompters, ensuring both capability and efficiency.","title":"Conclusion"},{"location":"tutorials/dspy/building-pipelines/","text":"Building Pipelines with DSPy \u00b6 In the previous tutorials, we explored basic DSPy concepts , optimizing prompts , and advanced reasoning techniques . Now let's examine how to chain these components into cohesive pipelines - one of DSPy's most powerful features for complex language processing tasks. Why Build Pipelines in Opossum? \u00b6 Decomposition: Break down complex language tasks into smaller, more manageable steps that are easier to develop, debug, and maintain. Specialized Processing: Apply different DSPy modules (possibly with different LM backends) to handle specific sub-tasks they're best suited for. Controlled Information Flow: Explicitly define how information moves through your application, reducing \"lost context\" problems. Resilience Enhancement: Align with Opossum's resilience patterns by allowing fallbacks or alternative paths if a component fails. Progressive Refinement: Gradually improve response quality through multiple processing stages. Pipeline Basics \u00b6 DSPy pipelines are built using the dspy.Module class. By inheriting from this class, you can define your own multi-step modules that orchestrate calls to other DSPy modules. The general pattern looks like this: class MyPipeline ( dspy . Module ): def __init__ ( self , param1 , param2 , ... ): super () . __init__ () # Initialize sub-modules self . step1 = dspy . Predict ( Step1Signature ) self . step2 = dspy . ChainOfThought ( Step2Signature ) # ... more sub-modules def forward ( self , input1 , input2 , ... ): # Run first step step1_result = self . step1 ( input_field1 = input1 , input_field2 = input2 ) # Use step1's output as input for step2 step2_result = self . step2 ( input_field_for_step2 = step1_result . output_field_from_step1 ) # ... more steps # Return final result return step2_result # or create and return a dspy.Prediction object This structure provides a clean separation of concerns while maintaining a clear information flow. Example: Multi-Stage Fact-Based Q&A Pipeline \u00b6 Let's build a pipeline using the OpossumFactTool from the Advanced Reasoning tutorial to create a more sophisticated Q&A system: import dspy from app.tools.fact_checker import OpossumFactTool # Our fact lookup tool # 1. Define signatures for each stage class QuestionAnalysis ( dspy . Signature ): \"\"\"Analyze a question to identify key concepts for lookup.\"\"\" question = dspy . InputField () key_concepts = dspy . OutputField ( desc = \"List of 2-3 key concepts to look up, separated by semicolons\" ) class FactRetrieval ( dspy . Signature ): \"\"\"Retrieve facts based on key concepts.\"\"\" key_concepts = dspy . InputField () retrieved_facts = dspy . OutputField ( desc = \"Facts retrieved from knowledge base\" ) class AnswerGeneration ( dspy . Signature ): \"\"\"Generate a comprehensive answer using question and retrieved facts.\"\"\" question = dspy . InputField () retrieved_facts = dspy . InputField () answer = dspy . OutputField ( desc = \"A comprehensive, factual answer to the question.\" ) # 2. Build the pipeline class OposumQAPipeline ( dspy . Module ): def __init__ ( self ): super () . __init__ () # Initialize individual modules self . question_analyzer = dspy . ChainOfThought ( QuestionAnalysis ) # Create the fact tool self . fact_tool = OpossumFactTool ( threshold = 0.25 , max_results = 3 , search_method = \"auto\" ) # Final answer generation with reasoning self . answer_generator = dspy . ChainOfThought ( AnswerGeneration ) def forward ( self , question ): # Step 1: Analyze the question to extract key concepts analysis = self . question_analyzer ( question = question ) key_concepts = analysis . key_concepts # Step 2: Retrieve facts for each key concept retrieved_facts = [] for concept in key_concepts . split ( ';' ): if concept . strip (): fact = self . fact_tool ( concept . strip ()) if not fact . startswith ( \"No facts found\" ): retrieved_facts . append ( fact ) # Combine all retrieved facts combined_facts = \" \\n\\n \" . join ( retrieved_facts ) if retrieved_facts else \"No relevant facts found.\" # Step 3: Generate the final answer result = self . answer_generator ( question = question , retrieved_facts = combined_facts ) return dspy . Prediction ( answer = result . answer ) # 3. Use the pipeline # Assuming dspy.settings.configure(lm=...) has been called qa_pipeline = OposumQAPipeline () response = qa_pipeline ( question = \"What adaptations help opossums survive in urban environments?\" ) print ( f \"Answer: { response . answer } \" ) This pipeline: 1. Analyzes the question to extract key concepts 2. Retrieves facts for each concept using our specialized tool 3. Synthesizes the information into a coherent answer Branching and Conditional Logic \u00b6 Pipelines can incorporate branching logic to handle different types of requests or implement fallback strategies: class SmartOposumPipeline ( dspy . Module ): def __init__ ( self ): super () . __init__ () self . classifier = dspy . Predict ( QueryClassifier ) self . factual_pipeline = OposumQAPipeline () self . comparison_pipeline = ComparisonPipeline () # Hypothetical comparison module self . default_responder = dspy . ChainOfThought ( DefaultResponse ) def forward ( self , user_query ): # Classify the query type classification = self . classifier ( query = user_query ) query_type = classification . query_type # Route to appropriate pipeline based on classification if query_type == \"factual\" : return self . factual_pipeline ( question = user_query ) elif query_type == \"comparison\" : return self . comparison_pipeline ( question = user_query ) else : # Handle other query types return self . default_responder ( question = user_query ) This pattern aligns well with Opossum's model selection architecture, where different query types might be routed to different processing pipelines. Pipeline Optimization \u00b6 Entire pipelines can be optimized just like individual modules. This is one of DSPy's most powerful features, as it allows end-to-end optimization of complex workflows. import dspy from dspy.teleprompt import BootstrapFewShot # Define a pipeline my_pipeline = OposumQAPipeline () # Define a metric for the entire pipeline def pipeline_quality_metric ( gold , pred , trace = None ): # Evaluate end-to-end performance # gold.answer is the ground truth # pred.answer is the pipeline's prediction return quality_score ( gold . answer , pred . answer ) # Create an optimizer teleprompter = BootstrapFewShot ( metric = pipeline_quality_metric ) # Optimize the pipeline end-to-end optimized_pipeline = teleprompter . compile ( student = my_pipeline , trainset = train_examples , valset = validation_examples ) # The optimized pipeline can then be saved and used in production optimized_pipeline . save ( \"./models/optimized_pipeline.dspy\" ) Persistent Pipelines \u00b6 Pipelines can be saved and loaded, preserving their optimized state: # Save an optimized pipeline optimized_pipeline . save ( \"./models/qa_pipeline_v1.dspy\" ) # Load the pipeline later from dspy.utils import load_module loaded_pipeline = load_module ( \"./models/qa_pipeline_v1.dspy\" ) # Use the loaded pipeline result = loaded_pipeline ( question = \"What do opossums eat?\" ) This enables a workflow where pipelines are optimized offline and then deployed to production, aligning with Opossum's design for API integration . Integrating with Opossum's Architecture \u00b6 Within Opossum Search, DSPy pipelines can serve various roles: Query Processing Pipeline: Handle user queries with multiple steps (understanding, retrieval, generation) Content Filtering: Implement content safety checks and post-processing Model Fallback Chain: Implement the fallback logic between different LM providers SVG Generation: Process natural language descriptions into structured SVG parameters Example integration point with Opossum's infrastructure: from app.core.config import get_settings from app.core.llm_clients import get_lm_client from app.tools.fact_checker import OpossumFactTool import dspy class OpossumSearchHandler : def __init__ ( self ): # Configure DSPy with Opossum's LM client settings = get_settings () lm_client = get_lm_client ( settings . preferred_provider ) dspy . settings . configure ( lm = lm_client ) # Load the pre-optimized pipeline self . pipeline = dspy . utils . load_module ( settings . dspy_pipeline_path ) async def process_query ( self , user_query , conversation_context = None ): try : # Process through the DSPy pipeline result = self . pipeline ( question = user_query , context = conversation_context ) return { \"response\" : result . answer , \"success\" : True } except Exception as e : # Implement fallback logic logger . error ( f \"Pipeline error: { e } \" ) return { \"response\" : \"I'm sorry, I encountered an issue processing your request.\" , \"success\" : False } Monitoring and Observability \u00b6 Integrate DSPy pipeline tracing with Opossum's existing OpenTelemetry integration : from app.telemetry import get_tracer from opentelemetry import trace # Create a custom DSPy module that adds telemetry class TracedModule ( dspy . Module ): def __init__ ( self , module , name ): super () . __init__ () self . module = module self . name = name self . tracer = get_tracer ( f \"dspy. { name } \" ) def forward ( self , ** kwargs ): with self . tracer . start_as_current_span ( self . name ) as span : # Add request metadata to span span . set_attribute ( \"input_keys\" , str ( list ( kwargs . keys ()))) # Process through the underlying module start_time = time . time () result = self . module ( ** kwargs ) duration = time . time () - start_time # Add response metadata to span span . set_attribute ( \"duration_seconds\" , duration ) return result Conclusion \u00b6 DSPy pipelines provide a powerful framework for orchestrating complex language processing tasks within Opossum Search. By decomposing tasks into well-defined modules with clear interfaces, you can build maintainable, resilient, and optimizable systems that leverage the best capabilities of language models. As Opossum continues to evolve, these pipeline patterns can help implement increasingly sophisticated features while maintaining code quality and performance. The ability to optimize entire pipelines end-to-end is particularly valuable for maximizing quality while controlling costs. For more advanced usage, explore the DSPy documentation and consider how these patterns can be combined with Opossum's existing architecture for system resilience and observability .","title":"Building Pipelines"},{"location":"tutorials/dspy/building-pipelines/#building-pipelines-with-dspy","text":"In the previous tutorials, we explored basic DSPy concepts , optimizing prompts , and advanced reasoning techniques . Now let's examine how to chain these components into cohesive pipelines - one of DSPy's most powerful features for complex language processing tasks.","title":"Building Pipelines with DSPy"},{"location":"tutorials/dspy/building-pipelines/#why-build-pipelines-in-opossum","text":"Decomposition: Break down complex language tasks into smaller, more manageable steps that are easier to develop, debug, and maintain. Specialized Processing: Apply different DSPy modules (possibly with different LM backends) to handle specific sub-tasks they're best suited for. Controlled Information Flow: Explicitly define how information moves through your application, reducing \"lost context\" problems. Resilience Enhancement: Align with Opossum's resilience patterns by allowing fallbacks or alternative paths if a component fails. Progressive Refinement: Gradually improve response quality through multiple processing stages.","title":"Why Build Pipelines in Opossum?"},{"location":"tutorials/dspy/building-pipelines/#pipeline-basics","text":"DSPy pipelines are built using the dspy.Module class. By inheriting from this class, you can define your own multi-step modules that orchestrate calls to other DSPy modules. The general pattern looks like this: class MyPipeline ( dspy . Module ): def __init__ ( self , param1 , param2 , ... ): super () . __init__ () # Initialize sub-modules self . step1 = dspy . Predict ( Step1Signature ) self . step2 = dspy . ChainOfThought ( Step2Signature ) # ... more sub-modules def forward ( self , input1 , input2 , ... ): # Run first step step1_result = self . step1 ( input_field1 = input1 , input_field2 = input2 ) # Use step1's output as input for step2 step2_result = self . step2 ( input_field_for_step2 = step1_result . output_field_from_step1 ) # ... more steps # Return final result return step2_result # or create and return a dspy.Prediction object This structure provides a clean separation of concerns while maintaining a clear information flow.","title":"Pipeline Basics"},{"location":"tutorials/dspy/building-pipelines/#example-multi-stage-fact-based-qa-pipeline","text":"Let's build a pipeline using the OpossumFactTool from the Advanced Reasoning tutorial to create a more sophisticated Q&A system: import dspy from app.tools.fact_checker import OpossumFactTool # Our fact lookup tool # 1. Define signatures for each stage class QuestionAnalysis ( dspy . Signature ): \"\"\"Analyze a question to identify key concepts for lookup.\"\"\" question = dspy . InputField () key_concepts = dspy . OutputField ( desc = \"List of 2-3 key concepts to look up, separated by semicolons\" ) class FactRetrieval ( dspy . Signature ): \"\"\"Retrieve facts based on key concepts.\"\"\" key_concepts = dspy . InputField () retrieved_facts = dspy . OutputField ( desc = \"Facts retrieved from knowledge base\" ) class AnswerGeneration ( dspy . Signature ): \"\"\"Generate a comprehensive answer using question and retrieved facts.\"\"\" question = dspy . InputField () retrieved_facts = dspy . InputField () answer = dspy . OutputField ( desc = \"A comprehensive, factual answer to the question.\" ) # 2. Build the pipeline class OposumQAPipeline ( dspy . Module ): def __init__ ( self ): super () . __init__ () # Initialize individual modules self . question_analyzer = dspy . ChainOfThought ( QuestionAnalysis ) # Create the fact tool self . fact_tool = OpossumFactTool ( threshold = 0.25 , max_results = 3 , search_method = \"auto\" ) # Final answer generation with reasoning self . answer_generator = dspy . ChainOfThought ( AnswerGeneration ) def forward ( self , question ): # Step 1: Analyze the question to extract key concepts analysis = self . question_analyzer ( question = question ) key_concepts = analysis . key_concepts # Step 2: Retrieve facts for each key concept retrieved_facts = [] for concept in key_concepts . split ( ';' ): if concept . strip (): fact = self . fact_tool ( concept . strip ()) if not fact . startswith ( \"No facts found\" ): retrieved_facts . append ( fact ) # Combine all retrieved facts combined_facts = \" \\n\\n \" . join ( retrieved_facts ) if retrieved_facts else \"No relevant facts found.\" # Step 3: Generate the final answer result = self . answer_generator ( question = question , retrieved_facts = combined_facts ) return dspy . Prediction ( answer = result . answer ) # 3. Use the pipeline # Assuming dspy.settings.configure(lm=...) has been called qa_pipeline = OposumQAPipeline () response = qa_pipeline ( question = \"What adaptations help opossums survive in urban environments?\" ) print ( f \"Answer: { response . answer } \" ) This pipeline: 1. Analyzes the question to extract key concepts 2. Retrieves facts for each concept using our specialized tool 3. Synthesizes the information into a coherent answer","title":"Example: Multi-Stage Fact-Based Q&amp;A Pipeline"},{"location":"tutorials/dspy/building-pipelines/#branching-and-conditional-logic","text":"Pipelines can incorporate branching logic to handle different types of requests or implement fallback strategies: class SmartOposumPipeline ( dspy . Module ): def __init__ ( self ): super () . __init__ () self . classifier = dspy . Predict ( QueryClassifier ) self . factual_pipeline = OposumQAPipeline () self . comparison_pipeline = ComparisonPipeline () # Hypothetical comparison module self . default_responder = dspy . ChainOfThought ( DefaultResponse ) def forward ( self , user_query ): # Classify the query type classification = self . classifier ( query = user_query ) query_type = classification . query_type # Route to appropriate pipeline based on classification if query_type == \"factual\" : return self . factual_pipeline ( question = user_query ) elif query_type == \"comparison\" : return self . comparison_pipeline ( question = user_query ) else : # Handle other query types return self . default_responder ( question = user_query ) This pattern aligns well with Opossum's model selection architecture, where different query types might be routed to different processing pipelines.","title":"Branching and Conditional Logic"},{"location":"tutorials/dspy/building-pipelines/#pipeline-optimization","text":"Entire pipelines can be optimized just like individual modules. This is one of DSPy's most powerful features, as it allows end-to-end optimization of complex workflows. import dspy from dspy.teleprompt import BootstrapFewShot # Define a pipeline my_pipeline = OposumQAPipeline () # Define a metric for the entire pipeline def pipeline_quality_metric ( gold , pred , trace = None ): # Evaluate end-to-end performance # gold.answer is the ground truth # pred.answer is the pipeline's prediction return quality_score ( gold . answer , pred . answer ) # Create an optimizer teleprompter = BootstrapFewShot ( metric = pipeline_quality_metric ) # Optimize the pipeline end-to-end optimized_pipeline = teleprompter . compile ( student = my_pipeline , trainset = train_examples , valset = validation_examples ) # The optimized pipeline can then be saved and used in production optimized_pipeline . save ( \"./models/optimized_pipeline.dspy\" )","title":"Pipeline Optimization"},{"location":"tutorials/dspy/building-pipelines/#persistent-pipelines","text":"Pipelines can be saved and loaded, preserving their optimized state: # Save an optimized pipeline optimized_pipeline . save ( \"./models/qa_pipeline_v1.dspy\" ) # Load the pipeline later from dspy.utils import load_module loaded_pipeline = load_module ( \"./models/qa_pipeline_v1.dspy\" ) # Use the loaded pipeline result = loaded_pipeline ( question = \"What do opossums eat?\" ) This enables a workflow where pipelines are optimized offline and then deployed to production, aligning with Opossum's design for API integration .","title":"Persistent Pipelines"},{"location":"tutorials/dspy/building-pipelines/#integrating-with-opossums-architecture","text":"Within Opossum Search, DSPy pipelines can serve various roles: Query Processing Pipeline: Handle user queries with multiple steps (understanding, retrieval, generation) Content Filtering: Implement content safety checks and post-processing Model Fallback Chain: Implement the fallback logic between different LM providers SVG Generation: Process natural language descriptions into structured SVG parameters Example integration point with Opossum's infrastructure: from app.core.config import get_settings from app.core.llm_clients import get_lm_client from app.tools.fact_checker import OpossumFactTool import dspy class OpossumSearchHandler : def __init__ ( self ): # Configure DSPy with Opossum's LM client settings = get_settings () lm_client = get_lm_client ( settings . preferred_provider ) dspy . settings . configure ( lm = lm_client ) # Load the pre-optimized pipeline self . pipeline = dspy . utils . load_module ( settings . dspy_pipeline_path ) async def process_query ( self , user_query , conversation_context = None ): try : # Process through the DSPy pipeline result = self . pipeline ( question = user_query , context = conversation_context ) return { \"response\" : result . answer , \"success\" : True } except Exception as e : # Implement fallback logic logger . error ( f \"Pipeline error: { e } \" ) return { \"response\" : \"I'm sorry, I encountered an issue processing your request.\" , \"success\" : False }","title":"Integrating with Opossum's Architecture"},{"location":"tutorials/dspy/building-pipelines/#monitoring-and-observability","text":"Integrate DSPy pipeline tracing with Opossum's existing OpenTelemetry integration : from app.telemetry import get_tracer from opentelemetry import trace # Create a custom DSPy module that adds telemetry class TracedModule ( dspy . Module ): def __init__ ( self , module , name ): super () . __init__ () self . module = module self . name = name self . tracer = get_tracer ( f \"dspy. { name } \" ) def forward ( self , ** kwargs ): with self . tracer . start_as_current_span ( self . name ) as span : # Add request metadata to span span . set_attribute ( \"input_keys\" , str ( list ( kwargs . keys ()))) # Process through the underlying module start_time = time . time () result = self . module ( ** kwargs ) duration = time . time () - start_time # Add response metadata to span span . set_attribute ( \"duration_seconds\" , duration ) return result","title":"Monitoring and Observability"},{"location":"tutorials/dspy/building-pipelines/#conclusion","text":"DSPy pipelines provide a powerful framework for orchestrating complex language processing tasks within Opossum Search. By decomposing tasks into well-defined modules with clear interfaces, you can build maintainable, resilient, and optimizable systems that leverage the best capabilities of language models. As Opossum continues to evolve, these pipeline patterns can help implement increasingly sophisticated features while maintaining code quality and performance. The ability to optimize entire pipelines end-to-end is particularly valuable for maximizing quality while controlling costs. For more advanced usage, explore the DSPy documentation and consider how these patterns can be combined with Opossum's existing architecture for system resilience and observability .","title":"Conclusion"},{"location":"tutorials/dspy/getting-started/","text":"Getting Started with DSPy \u00b6 Welcome to the DSPy tutorial for Opossum Search! DSPy is a framework for algorithmically optimizing language model (LM) prompts and weights, especially when LMs are used in pipelines. Within Opossum Search, DSPy helps us build more robust, efficient, and adaptable interactions with the underlying language models like Gemini , Ollama , and Transformers . Why DSPy in Opossum Search? \u00b6 Systematic Prompt Engineering: Move beyond manual prompt tweaking to algorithmic optimization. Modular Pipelines: Construct complex LM workflows (e.g., for multi-step reasoning or specialized response generation) using composable modules. Adaptability: Optimize prompts and module interactions based on performance metrics, making the system more resilient to changes in underlying models or data. Integration with Existing Infrastructure: DSPy can work alongside Opossum's existing hybrid model selection and resilience patterns . Core Concepts \u00b6 Signatures: Define the input/output behavior of an LM task (e.g., Question -> Answer ). They specify the fields your LM module should expect and produce. Modules: Building blocks of DSPy programs (e.g., dspy.Predict , dspy.ChainOfThought , dspy.ReAct ). Modules take Signatures and handle the interaction with the LM. Language Models (LMs): DSPy interacts with LMs. In Opossum, this could be configured to use Gemini, Ollama, or local Transformers based on availability and configuration ( docs/model-integration/configuration.md ). Optimizers (Teleprompters): Algorithms that tune the prompts and/or weights within your DSPy modules based on a defined metric and training data (more in the Optimizing Prompts tutorial). Prerequisites \u00b6 Ensure your Opossum Search development environment is set up according to the DevOps Guide . DSPy should be included in the project's Python dependencies ( requirements.txt or similar). Familiarity with Opossum's configuration for LM providers ( app/config.py ). Basic Usage Example \u00b6 Let's configure a simple DSPy interaction using one of Opossum's configured LMs. # Example: Assumes LM configuration is loaded elsewhere # (e.g., from Opossum's config module) import dspy from app.core.llm_clients import get_configured_lm # Hypothetical function # 1. Configure the Language Model (using Opossum's setup) # This might involve selecting Gemini, Ollama, etc. based on Opossum's logic # For simplicity, let's assume we get a configured dspy LM client try : # Attempt to get a high-capability model like Gemini first lm_client = get_configured_lm ( prefer_service = \"gemini\" ) except Exception : # Fallback if preferred service fails (using Opossum's resilience) lm_client = get_configured_lm ( prefer_service = \"ollama\" ) dspy . settings . configure ( lm = lm_client ) # 2. Define a Signature class BasicQA ( dspy . Signature ): \"\"\"Answer questions based on context.\"\"\" context = dspy . InputField ( desc = \"May contain relevant facts.\" ) question = dspy . InputField () answer = dspy . OutputField ( desc = \"Often short, concise answer.\" ) # 3. Use a Module generate_answer = dspy . Predict ( BasicQA ) # 4. Run the module context_data = \"Opossums are marsupials native to the Americas. They are known for playing dead.\" question_to_ask = \"What defense mechanism are opossums known for?\" prediction = generate_answer ( context = context_data , question = question_to_ask ) print ( f \"Question: { question_to_ask } \" ) print ( f \"Predicted Answer: { prediction . answer } \" ) (Note: The get_configured_lm function is hypothetical and represents how Opossum's existing LM client management and selection logic would integrate with DSPy's dspy.settings.configure .) Integration Points in Opossum \u00b6 LM Configuration: DSPy needs to be configured to use the LM instances managed by Opossum (Gemini, Ollama, Transformers). This involves wrapping or adapting Opossum's clients for DSPy compatibility. Pipeline Construction: DSPy modules can be used to define specific parts of the request processing pipeline, potentially replacing or augmenting existing logic in app/pipelines or similar areas. Prompt Management: DSPy's optimization capabilities can enhance Opossum's prompt management strategies . Next Steps \u00b6 Learn how to automatically improve prompts in Optimizing Prompts with DSPy . Discover how to chain modules together in Building Pipelines with DSPy . Explore more complex techniques in Advanced Reasoning with DSPy .","title":"Getting Started with DSPy"},{"location":"tutorials/dspy/getting-started/#getting-started-with-dspy","text":"Welcome to the DSPy tutorial for Opossum Search! DSPy is a framework for algorithmically optimizing language model (LM) prompts and weights, especially when LMs are used in pipelines. Within Opossum Search, DSPy helps us build more robust, efficient, and adaptable interactions with the underlying language models like Gemini , Ollama , and Transformers .","title":"Getting Started with DSPy"},{"location":"tutorials/dspy/getting-started/#why-dspy-in-opossum-search","text":"Systematic Prompt Engineering: Move beyond manual prompt tweaking to algorithmic optimization. Modular Pipelines: Construct complex LM workflows (e.g., for multi-step reasoning or specialized response generation) using composable modules. Adaptability: Optimize prompts and module interactions based on performance metrics, making the system more resilient to changes in underlying models or data. Integration with Existing Infrastructure: DSPy can work alongside Opossum's existing hybrid model selection and resilience patterns .","title":"Why DSPy in Opossum Search?"},{"location":"tutorials/dspy/getting-started/#core-concepts","text":"Signatures: Define the input/output behavior of an LM task (e.g., Question -> Answer ). They specify the fields your LM module should expect and produce. Modules: Building blocks of DSPy programs (e.g., dspy.Predict , dspy.ChainOfThought , dspy.ReAct ). Modules take Signatures and handle the interaction with the LM. Language Models (LMs): DSPy interacts with LMs. In Opossum, this could be configured to use Gemini, Ollama, or local Transformers based on availability and configuration ( docs/model-integration/configuration.md ). Optimizers (Teleprompters): Algorithms that tune the prompts and/or weights within your DSPy modules based on a defined metric and training data (more in the Optimizing Prompts tutorial).","title":"Core Concepts"},{"location":"tutorials/dspy/getting-started/#prerequisites","text":"Ensure your Opossum Search development environment is set up according to the DevOps Guide . DSPy should be included in the project's Python dependencies ( requirements.txt or similar). Familiarity with Opossum's configuration for LM providers ( app/config.py ).","title":"Prerequisites"},{"location":"tutorials/dspy/getting-started/#basic-usage-example","text":"Let's configure a simple DSPy interaction using one of Opossum's configured LMs. # Example: Assumes LM configuration is loaded elsewhere # (e.g., from Opossum's config module) import dspy from app.core.llm_clients import get_configured_lm # Hypothetical function # 1. Configure the Language Model (using Opossum's setup) # This might involve selecting Gemini, Ollama, etc. based on Opossum's logic # For simplicity, let's assume we get a configured dspy LM client try : # Attempt to get a high-capability model like Gemini first lm_client = get_configured_lm ( prefer_service = \"gemini\" ) except Exception : # Fallback if preferred service fails (using Opossum's resilience) lm_client = get_configured_lm ( prefer_service = \"ollama\" ) dspy . settings . configure ( lm = lm_client ) # 2. Define a Signature class BasicQA ( dspy . Signature ): \"\"\"Answer questions based on context.\"\"\" context = dspy . InputField ( desc = \"May contain relevant facts.\" ) question = dspy . InputField () answer = dspy . OutputField ( desc = \"Often short, concise answer.\" ) # 3. Use a Module generate_answer = dspy . Predict ( BasicQA ) # 4. Run the module context_data = \"Opossums are marsupials native to the Americas. They are known for playing dead.\" question_to_ask = \"What defense mechanism are opossums known for?\" prediction = generate_answer ( context = context_data , question = question_to_ask ) print ( f \"Question: { question_to_ask } \" ) print ( f \"Predicted Answer: { prediction . answer } \" ) (Note: The get_configured_lm function is hypothetical and represents how Opossum's existing LM client management and selection logic would integrate with DSPy's dspy.settings.configure .)","title":"Basic Usage Example"},{"location":"tutorials/dspy/getting-started/#integration-points-in-opossum","text":"LM Configuration: DSPy needs to be configured to use the LM instances managed by Opossum (Gemini, Ollama, Transformers). This involves wrapping or adapting Opossum's clients for DSPy compatibility. Pipeline Construction: DSPy modules can be used to define specific parts of the request processing pipeline, potentially replacing or augmenting existing logic in app/pipelines or similar areas. Prompt Management: DSPy's optimization capabilities can enhance Opossum's prompt management strategies .","title":"Integration Points in Opossum"},{"location":"tutorials/dspy/getting-started/#next-steps","text":"Learn how to automatically improve prompts in Optimizing Prompts with DSPy . Discover how to chain modules together in Building Pipelines with DSPy . Explore more complex techniques in Advanced Reasoning with DSPy .","title":"Next Steps"},{"location":"tutorials/dspy/optimizing-prompts/","text":"Optimizing Prompts with DSPy \u00b6 In the Getting Started guide, we saw how to define DSPy modules and signatures to interact with language models (LMs) like Gemini or Ollama within Opossum Search. Now, let's explore one of DSPy's most powerful features: optimizing the prompts used by these modules automatically. Manually crafting the perfect prompt (\"prompt engineering\") can be time-consuming and brittle. DSPy replaces this manual effort with optimizers (called \"Teleprompters\") that algorithmically search for effective prompts based on your specific needs and data. Why Optimize Prompts in Opossum? \u00b6 Improved Accuracy & Relevance: Generate prompts that lead to more accurate and relevant responses from the underlying LMs for search queries or conversational tasks, aligning with Opossum's goal of providing scientifically accurate facts. Enhanced Efficiency: Discover prompts that achieve the desired outcome with fewer tokens or faster response times, contributing to Optimization Strategies . Adaptability: Automatically adjust prompts when underlying models change or when new data reveals weaknesses in existing prompts. Consistency: Ensure more consistent behavior across different types of inputs. Constraint Adherence: Optimize prompts to better follow specific constraints defined within Opossum (e.g., response format, tone, adherence to evaluation_criteria found in datasets like opossum_dataset_converted.json ). Core Concepts for Optimization \u00b6 Training Data: Optimizers need examples to learn from. This typically consists of input/output pairs that demonstrate the desired behavior. For Opossum, this data could come from: Curated examples like those in opossum_dataset_converted.json , mapping user_input to expected_response . Logged interactions (potentially filtered or annotated). Synthetic data generated for specific scenarios. Data used for evaluating response generation quality. Metric: You need a way to tell the optimizer what \"good\" looks like. A metric is a function that scores the output of your DSPy module against the desired output in your training data. Examples: Exact match accuracy on a specific field (e.g., main_fact ). F1 score for information extraction. LLM-based evaluation (using another LM to judge quality based on criteria like accuracy or completeness from the dataset). Custom metrics specific to Opossum's goals (e.g., checking if an SVG response is valid, measuring adherence to safety guidelines, validating against evaluation_criteria ). Teleprompters (Optimizers): These are the DSPy algorithms that perform the optimization. They take your un-optimized module(s), training data, and a metric, then experiment with different prompts (and potentially demonstrations) to create an optimized version of your module. Common teleprompters include: BootstrapFewShot : Generates demonstrations (examples) to include in the prompt context. Effective for few-shot learning. MIPRO (Multi-stage Instruction Prompt Refinement) : Iteratively refines the instruction part of the prompt using Bayesian Optimization. Good for complex instructions. SignatureOptimizer : Optimizes the textual descriptions within your signature (e.g., InputField descriptions). Others like BootstrapFewShotWithRandomSearch . The Optimization Process (Compilation) \u00b6 Optimizing in DSPy is often referred to as \"compiling\" your program. The general steps are: Prepare Data: Load your data (e.g., from opossum_dataset_converted.json ) and format it into a list of dspy.Example objects. Each example should map the input fields (like user_input ) to the desired ground truth output fields (like main_fact or a structured version of expected_response ). Define Metric: Write a Python function metric(gold, prediction, trace?) -> score where gold is the dspy.Example (ground truth) and prediction is the output from your module. The score is typically a boolean or a float (higher is better). Instantiate Teleprompter: Choose an optimizer and configure it with your metric and any specific parameters (e.g., teleprompter = dspy.BootstrapFewShot(metric=your_metric, max_bootstrapped_demos=4) ). Compile: Call the teleprompter's compile method, passing your student module (the one to be optimized), training data, and potentially validation data ( optimizer.compile(student=your_module, trainset=your_training_data, valset=your_validation_data) ). This returns a new , optimized module instance. Evaluate (Optional but Recommended): Test the compiled module on a separate test dataset (not used during training or validation) to get an unbiased estimate of its performance. Optimization Example using Opossum Dataset \u00b6 Let's adapt the Q&A example, imagining we load data from opossum_dataset_converted.json . import dspy import json from dspy.teleprompt import BootstrapFewShot from dspy.evaluate.evaluate import Evaluate # Assume dspy.settings.configure(lm=...) has been called using Opossum's LM config # --- Data Loading and Preparation --- def load_opossum_data ( filepath = \"c:/Users/beb/PycharmProjects/Opossum/opossum_dataset_converted.json\" ): \"\"\"Loads data and converts it to dspy.Example format.\"\"\" with open ( filepath , 'r' , encoding = 'utf-8' ) as f : data = json . load ( f ) examples = [] for item in data . get ( \"prompts\" , []): prompt_data = item . get ( \"prompt\" , {}) user_input = prompt_data . get ( \"user_input\" ) expected_response = prompt_data . get ( \"expected_response\" , {}) main_fact = expected_response . get ( \"main_fact\" ) # Add other fields as needed for your signature/metric if user_input and main_fact : # Create a dspy.Example. Input field is 'question', output field is 'answer'. # We map 'user_input' to 'question' and 'main_fact' to 'answer'. example = dspy . Example ( question = user_input , answer = main_fact ) . with_inputs ( \"question\" ) examples . append ( example ) return examples # Load and split data (simple split for demonstration) all_examples = load_opossum_data () split_point = int ( len ( all_examples ) * 0.7 ) trainset = all_examples [: split_point ] valset = all_examples [ split_point :] # Use a separate validation set for compilation # --- Signature Definition --- class OpossumQA ( dspy . Signature ): \"\"\"Answer questions about opossums based on general knowledge.\"\"\" question = dspy . InputField ( desc = \"A question about opossums.\" ) answer = dspy . OutputField ( desc = \"A concise factual answer, focusing on the main point.\" ) # --- Metric Definition --- def validate_opossum_answer ( gold , pred , trace = None ): \"\"\"Checks if the predicted answer is contained within the gold answer (case-insensitive).\"\"\" # A simple metric; more sophisticated ones (e.g., using evaluation_criteria) are possible gold_answer = gold . answer . lower () pred_answer = pred . answer . lower () return pred_answer in gold_answer or gold_answer in pred_answer # Allow for slight variations # --- Teleprompter Instantiation --- # Configure BootstrapFewShot: Use validation set, metric, and specify max demos teleprompter = BootstrapFewShot ( metric = validate_opossum_answer , max_bootstrapped_demos = 4 , # Number of examples to find for the prompt ) # --- Compilation --- # Define the student module we want to optimize uncompiled_qa = dspy . Predict ( OpossumQA ) # Compile the module using the training set and validation set # The teleprompter uses the valset to guide the search for good prompts/demos compiled_qa = teleprompter . compile ( student = uncompiled_qa , trainset = trainset , valset = valset ) # --- Evaluation (Optional) --- # Set up the evaluator evaluate = Evaluate ( devset = valset , # Evaluate on the validation set (or better, a separate test set) metric = validate_opossum_answer , num_threads = 4 , display_progress = True , display_table = 5 ) # Evaluate the compiled module score = evaluate ( compiled_qa ) print ( f \"Evaluation score after compilation: { score } \" ) # --- Using the Optimized Module --- question_to_ask = \"Do opossums carry rabies?\" prediction = compiled_qa ( question = question_to_ask ) print ( f \" \\n Question: { question_to_ask } \" ) print ( f \"Optimized Predicted Answer: { prediction . answer } \" ) # Inspect the final prompt used (including generated demonstrations) # dspy.settings.lm.inspect_history(n=1) Choosing Metrics and Teleprompters \u00b6 Metrics: Start simple (e.g., exact match, keyword check). If needed, create more complex metrics, potentially using another LM ( dspy.Suggest ) to evaluate based on criteria like accuracy, completeness, and relevance defined in your dataset ( evaluation_criteria ). Align the metric closely with Opossum's specific goals for the task. Teleprompters: BootstrapFewShot is often a good starting point, especially if providing good examples in the prompt is effective. MIPRO or SignatureOptimizer might be better if the core instruction needs refinement rather than just examples. Experimentation is key. Try different teleprompters and parameters on your validation set. Integration in Opossum Search \u00b6 Offline Process: Compilation is computationally intensive and typically done offline as part of a model/prompt refinement workflow, not during live user requests. Workflow: Collect and prepare training/validation/test data (e.g., from opossum_dataset_converted.json , logs). Define DSPy Signatures and Modules for the target task within Opossum (e.g., a specific step in Pipeline Optimization ). Define appropriate metrics aligned with Opossum's quality requirements (accuracy, adherence to constraints). Run the DSPy compilation process using configured LMs ( app/core/llm_clients.py ). Evaluate the compiled module thoroughly on a held-out test set. Store the state of the optimized module (DSPy provides methods like save() / load() ). This saves the learned prompts/demonstrations. Load the compiled module into the Opossum application (e.g., within app/pipelines or relevant service) for use during runtime. This ensures production uses the optimized prompts. Next Steps \u00b6 Explore how to combine multiple optimized modules in Building Pipelines with DSPy . Dive into more complex reasoning patterns in Advanced Reasoning with DSPy . Consult the official DSPy documentation for details on different teleprompters and advanced metric definition.","title":"Optimizing Prompts"},{"location":"tutorials/dspy/optimizing-prompts/#optimizing-prompts-with-dspy","text":"In the Getting Started guide, we saw how to define DSPy modules and signatures to interact with language models (LMs) like Gemini or Ollama within Opossum Search. Now, let's explore one of DSPy's most powerful features: optimizing the prompts used by these modules automatically. Manually crafting the perfect prompt (\"prompt engineering\") can be time-consuming and brittle. DSPy replaces this manual effort with optimizers (called \"Teleprompters\") that algorithmically search for effective prompts based on your specific needs and data.","title":"Optimizing Prompts with DSPy"},{"location":"tutorials/dspy/optimizing-prompts/#why-optimize-prompts-in-opossum","text":"Improved Accuracy & Relevance: Generate prompts that lead to more accurate and relevant responses from the underlying LMs for search queries or conversational tasks, aligning with Opossum's goal of providing scientifically accurate facts. Enhanced Efficiency: Discover prompts that achieve the desired outcome with fewer tokens or faster response times, contributing to Optimization Strategies . Adaptability: Automatically adjust prompts when underlying models change or when new data reveals weaknesses in existing prompts. Consistency: Ensure more consistent behavior across different types of inputs. Constraint Adherence: Optimize prompts to better follow specific constraints defined within Opossum (e.g., response format, tone, adherence to evaluation_criteria found in datasets like opossum_dataset_converted.json ).","title":"Why Optimize Prompts in Opossum?"},{"location":"tutorials/dspy/optimizing-prompts/#core-concepts-for-optimization","text":"Training Data: Optimizers need examples to learn from. This typically consists of input/output pairs that demonstrate the desired behavior. For Opossum, this data could come from: Curated examples like those in opossum_dataset_converted.json , mapping user_input to expected_response . Logged interactions (potentially filtered or annotated). Synthetic data generated for specific scenarios. Data used for evaluating response generation quality. Metric: You need a way to tell the optimizer what \"good\" looks like. A metric is a function that scores the output of your DSPy module against the desired output in your training data. Examples: Exact match accuracy on a specific field (e.g., main_fact ). F1 score for information extraction. LLM-based evaluation (using another LM to judge quality based on criteria like accuracy or completeness from the dataset). Custom metrics specific to Opossum's goals (e.g., checking if an SVG response is valid, measuring adherence to safety guidelines, validating against evaluation_criteria ). Teleprompters (Optimizers): These are the DSPy algorithms that perform the optimization. They take your un-optimized module(s), training data, and a metric, then experiment with different prompts (and potentially demonstrations) to create an optimized version of your module. Common teleprompters include: BootstrapFewShot : Generates demonstrations (examples) to include in the prompt context. Effective for few-shot learning. MIPRO (Multi-stage Instruction Prompt Refinement) : Iteratively refines the instruction part of the prompt using Bayesian Optimization. Good for complex instructions. SignatureOptimizer : Optimizes the textual descriptions within your signature (e.g., InputField descriptions). Others like BootstrapFewShotWithRandomSearch .","title":"Core Concepts for Optimization"},{"location":"tutorials/dspy/optimizing-prompts/#the-optimization-process-compilation","text":"Optimizing in DSPy is often referred to as \"compiling\" your program. The general steps are: Prepare Data: Load your data (e.g., from opossum_dataset_converted.json ) and format it into a list of dspy.Example objects. Each example should map the input fields (like user_input ) to the desired ground truth output fields (like main_fact or a structured version of expected_response ). Define Metric: Write a Python function metric(gold, prediction, trace?) -> score where gold is the dspy.Example (ground truth) and prediction is the output from your module. The score is typically a boolean or a float (higher is better). Instantiate Teleprompter: Choose an optimizer and configure it with your metric and any specific parameters (e.g., teleprompter = dspy.BootstrapFewShot(metric=your_metric, max_bootstrapped_demos=4) ). Compile: Call the teleprompter's compile method, passing your student module (the one to be optimized), training data, and potentially validation data ( optimizer.compile(student=your_module, trainset=your_training_data, valset=your_validation_data) ). This returns a new , optimized module instance. Evaluate (Optional but Recommended): Test the compiled module on a separate test dataset (not used during training or validation) to get an unbiased estimate of its performance.","title":"The Optimization Process (Compilation)"},{"location":"tutorials/dspy/optimizing-prompts/#optimization-example-using-opossum-dataset","text":"Let's adapt the Q&A example, imagining we load data from opossum_dataset_converted.json . import dspy import json from dspy.teleprompt import BootstrapFewShot from dspy.evaluate.evaluate import Evaluate # Assume dspy.settings.configure(lm=...) has been called using Opossum's LM config # --- Data Loading and Preparation --- def load_opossum_data ( filepath = \"c:/Users/beb/PycharmProjects/Opossum/opossum_dataset_converted.json\" ): \"\"\"Loads data and converts it to dspy.Example format.\"\"\" with open ( filepath , 'r' , encoding = 'utf-8' ) as f : data = json . load ( f ) examples = [] for item in data . get ( \"prompts\" , []): prompt_data = item . get ( \"prompt\" , {}) user_input = prompt_data . get ( \"user_input\" ) expected_response = prompt_data . get ( \"expected_response\" , {}) main_fact = expected_response . get ( \"main_fact\" ) # Add other fields as needed for your signature/metric if user_input and main_fact : # Create a dspy.Example. Input field is 'question', output field is 'answer'. # We map 'user_input' to 'question' and 'main_fact' to 'answer'. example = dspy . Example ( question = user_input , answer = main_fact ) . with_inputs ( \"question\" ) examples . append ( example ) return examples # Load and split data (simple split for demonstration) all_examples = load_opossum_data () split_point = int ( len ( all_examples ) * 0.7 ) trainset = all_examples [: split_point ] valset = all_examples [ split_point :] # Use a separate validation set for compilation # --- Signature Definition --- class OpossumQA ( dspy . Signature ): \"\"\"Answer questions about opossums based on general knowledge.\"\"\" question = dspy . InputField ( desc = \"A question about opossums.\" ) answer = dspy . OutputField ( desc = \"A concise factual answer, focusing on the main point.\" ) # --- Metric Definition --- def validate_opossum_answer ( gold , pred , trace = None ): \"\"\"Checks if the predicted answer is contained within the gold answer (case-insensitive).\"\"\" # A simple metric; more sophisticated ones (e.g., using evaluation_criteria) are possible gold_answer = gold . answer . lower () pred_answer = pred . answer . lower () return pred_answer in gold_answer or gold_answer in pred_answer # Allow for slight variations # --- Teleprompter Instantiation --- # Configure BootstrapFewShot: Use validation set, metric, and specify max demos teleprompter = BootstrapFewShot ( metric = validate_opossum_answer , max_bootstrapped_demos = 4 , # Number of examples to find for the prompt ) # --- Compilation --- # Define the student module we want to optimize uncompiled_qa = dspy . Predict ( OpossumQA ) # Compile the module using the training set and validation set # The teleprompter uses the valset to guide the search for good prompts/demos compiled_qa = teleprompter . compile ( student = uncompiled_qa , trainset = trainset , valset = valset ) # --- Evaluation (Optional) --- # Set up the evaluator evaluate = Evaluate ( devset = valset , # Evaluate on the validation set (or better, a separate test set) metric = validate_opossum_answer , num_threads = 4 , display_progress = True , display_table = 5 ) # Evaluate the compiled module score = evaluate ( compiled_qa ) print ( f \"Evaluation score after compilation: { score } \" ) # --- Using the Optimized Module --- question_to_ask = \"Do opossums carry rabies?\" prediction = compiled_qa ( question = question_to_ask ) print ( f \" \\n Question: { question_to_ask } \" ) print ( f \"Optimized Predicted Answer: { prediction . answer } \" ) # Inspect the final prompt used (including generated demonstrations) # dspy.settings.lm.inspect_history(n=1)","title":"Optimization Example using Opossum Dataset"},{"location":"tutorials/dspy/optimizing-prompts/#choosing-metrics-and-teleprompters","text":"Metrics: Start simple (e.g., exact match, keyword check). If needed, create more complex metrics, potentially using another LM ( dspy.Suggest ) to evaluate based on criteria like accuracy, completeness, and relevance defined in your dataset ( evaluation_criteria ). Align the metric closely with Opossum's specific goals for the task. Teleprompters: BootstrapFewShot is often a good starting point, especially if providing good examples in the prompt is effective. MIPRO or SignatureOptimizer might be better if the core instruction needs refinement rather than just examples. Experimentation is key. Try different teleprompters and parameters on your validation set.","title":"Choosing Metrics and Teleprompters"},{"location":"tutorials/dspy/optimizing-prompts/#integration-in-opossum-search","text":"Offline Process: Compilation is computationally intensive and typically done offline as part of a model/prompt refinement workflow, not during live user requests. Workflow: Collect and prepare training/validation/test data (e.g., from opossum_dataset_converted.json , logs). Define DSPy Signatures and Modules for the target task within Opossum (e.g., a specific step in Pipeline Optimization ). Define appropriate metrics aligned with Opossum's quality requirements (accuracy, adherence to constraints). Run the DSPy compilation process using configured LMs ( app/core/llm_clients.py ). Evaluate the compiled module thoroughly on a held-out test set. Store the state of the optimized module (DSPy provides methods like save() / load() ). This saves the learned prompts/demonstrations. Load the compiled module into the Opossum application (e.g., within app/pipelines or relevant service) for use during runtime. This ensures production uses the optimized prompts.","title":"Integration in Opossum Search"},{"location":"tutorials/dspy/optimizing-prompts/#next-steps","text":"Explore how to combine multiple optimized modules in Building Pipelines with DSPy . Dive into more complex reasoning patterns in Advanced Reasoning with DSPy . Consult the official DSPy documentation for details on different teleprompters and advanced metric definition.","title":"Next Steps"}]}